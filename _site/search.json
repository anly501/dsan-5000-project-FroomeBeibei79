[
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data Section",
    "section": "",
    "text": "This is a link to data in GitHub Click here"
  },
  {
    "objectID": "Data.html#driver-side",
    "href": "Data.html#driver-side",
    "title": "Data Section",
    "section": "Driver Side",
    "text": "Driver Side\n\nGender\nGender is a significant factor that has an impact on driver behavior. When it comes to fatal injury odds in passenger vehicle accidents, “the fatal injury odds for females were lower than males.” (p21)\n\n\nYoung driver\nAnother publication focus on young driver affect driver behavior analysis, as statistics state that “The rate of drivers involved in fatal traffic crashes per 100,000 licensed drivers for young female drivers was 25.51 in 2021. For young male drivers in 2021 the involvement rate was 60.28, more than twice that of young female drivers.” (U.S. Department of Transportation, National Highway Traffic Safety Administration. (2022). Traffic Safety Facts 2021 Data (Report No. 813492))"
  },
  {
    "objectID": "Data Cleaning.html",
    "href": "Data Cleaning.html",
    "title": "Data Cleaning Code",
    "section": "",
    "text": "This is code section\n\nR Data\nThis will be a link to another page where you can see the code of how to clean the dataset in R\n\n\nPython Data\nThis will be a link to another page where you can see the code of how to clean the dataset in python"
  },
  {
    "objectID": "Data Cleaning R.html",
    "href": "Data Cleaning R.html",
    "title": "Xingrui's Project",
    "section": "",
    "text": "Below is the code to clean the gender dataset\n\nlibrary(readxl)\nlibrary(tidyverse)\ngender &lt;- read_excel(\"data/dl220.xls\")\ngender &lt;- gender[-(1:2),]\ncolnames(gender) &lt;- gender[1,]\ngender &lt;- gender[-1,]\ngender &lt;- gender[1:(nrow(gender) -4),]\ncolnames(gender)[1] &lt;- \"Year\"\ngender &lt;- gender[-1,]\ncolnames1 &lt;- paste0(colnames(gender), unlist(gender[1,]))\ncolnames(gender) &lt;- colnames1\ngender &lt;- gender[-1,]\n\nnames &lt;- which(duplicated(colnames(gender)))\nfor (col_index in names){\n  colnames(gender)[col_index] &lt;- paste0(colnames(gender)[col_index], \"_\", col_index)\n}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#it-will-show-some-detailed-information-regrading-different-manufactures",
    "href": "about.html#it-will-show-some-detailed-information-regrading-different-manufactures",
    "title": "About",
    "section": "it will show some detailed information regrading different manufactures",
    "text": "it will show some detailed information regrading different manufactures\n\nMore will coming soon…\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some sample data\nx = np.linspace(0, 2 * np.pi, 100)  # Create an array of values from 0 to 2*pi\ny = np.sin(x)  # Compute the sine of each value\n\n# Create a plot\nplt.figure(figsize=(8, 4))  # Set the figure size\nplt.plot(x, y, label='sin(x)', color='blue', linestyle='-', linewidth=2)  # Create the plot\n\n# Add labels and a legend\nplt.xlabel('x')  # X-axis label\nplt.ylabel('y')  # Y-axis label\nplt.title('Sine Function')  # Title of the plot\nplt.legend()\n\n# Show the plot\nplt.grid(True)  # Add grid lines\nplt.show()"
  },
  {
    "objectID": "NB.html",
    "href": "NB.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a probabilistic machine learning algorithm based on Bayes’ theorem, which is used for classification tasks. In its essence, it involves using probability to make predictions.\n\n\nNaive Bayes relies on Bayes’ theorem, which describes the probability of an event based on prior knowledge of related conditions. In the context of classification, it’s used to find the probability that a given instance belongs to a particular category based on its features.\n\n\n\nIt calculates the probability of each class given the input features, and the class with the highest probability is considered as the output.\nAssumption of Independence: The “Naive” part of Naive Bayes comes from the assumption that all the features are independent of each other given the class label. While this is a strong and often unrealistic assumption, in practice, Naive Bayes classifiers perform surprisingly well even when this condition is not met.\n\n\n\nThe main objective of Naive Bayes classification is to quickly and efficiently categorize new instances into predefined classes based on the statistical properties of the features of the training data.\n\n\n\n\nDue to its simplicity and the fact that it doesn’t require complex iterative optimization, it’s very fast and efficient, especially for high-dimensional datasets.\n\n\n\nIt can handle missing values by ignoring the missing features during computation.\n\n\n\nIt provides a good baseline model for classification tasks, offering a point of comparison for more complex algorithms.\n\n\n\n\n\n\nAssumes that the features follow a normal distribution. It’s useful when dealing with continuous data.\n\n\n\nUseful for discrete counts, such as word counts in text classification.\n\n\n\n\nSimilar to Multinomial Naive Bayes, but it’s specifically designed for binary/boolean features.\n\n\n\n\n\nUse when your features are continuous and you can assume a Gaussian distribution.\n\n\n\nIdeal for text classification problems where you have counts of word occurrences.\n\n\n\nUse when you’re dealing with binary or boolean features."
  },
  {
    "objectID": "NB.html#bayes-theorem-foundation",
    "href": "NB.html#bayes-theorem-foundation",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes relies on Bayes’ theorem, which describes the probability of an event based on prior knowledge of related conditions. In the context of classification, it’s used to find the probability that a given instance belongs to a particular category based on its features."
  },
  {
    "objectID": "NB.html#probabilistic-nature",
    "href": "NB.html#probabilistic-nature",
    "title": "Naïve Bayes",
    "section": "",
    "text": "It calculates the probability of each class given the input features, and the class with the highest probability is considered as the output.\nAssumption of Independence: The “Naive” part of Naive Bayes comes from the assumption that all the features are independent of each other given the class label. While this is a strong and often unrealistic assumption, in practice, Naive Bayes classifiers perform surprisingly well even when this condition is not met."
  },
  {
    "objectID": "NB.html#objectives",
    "href": "NB.html#objectives",
    "title": "Naïve Bayes",
    "section": "",
    "text": "The main objective of Naive Bayes classification is to quickly and efficiently categorize new instances into predefined classes based on the statistical properties of the features of the training data.\n\n\n\n\nDue to its simplicity and the fact that it doesn’t require complex iterative optimization, it’s very fast and efficient, especially for high-dimensional datasets.\n\n\n\nIt can handle missing values by ignoring the missing features during computation.\n\n\n\nIt provides a good baseline model for classification tasks, offering a point of comparison for more complex algorithms.\n\n\n\n\n\n\nAssumes that the features follow a normal distribution. It’s useful when dealing with continuous data.\n\n\n\nUseful for discrete counts, such as word counts in text classification.\n\n\n\n\nSimilar to Multinomial Naive Bayes, but it’s specifically designed for binary/boolean features.\n\n\n\n\n\nUse when your features are continuous and you can assume a Gaussian distribution.\n\n\n\nIdeal for text classification problems where you have counts of word occurrences.\n\n\n\nUse when you’re dealing with binary or boolean features."
  },
  {
    "objectID": "NB.html#record-data-of-naïve-bayes",
    "href": "NB.html#record-data-of-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "Record Data of Naïve Bayes",
    "text": "Record Data of Naïve Bayes\nFor the record data, we will focus on fatalties of accidents. We choose to predict the time for EMS took on the road. The time gap vary based on various conditions, we will use this time difference as our target variable for the Naive Bayes classification.\nHere in this dataset, there’s no specifc column that record the time EMS took on road, but there are columns that record when the EMS received the notification and the time EMS arrived at the scene. We will use this as the time gap, also our target variable.\n\n\n\ntime gap\n\n\nFor feature selection, there are many possible factors that affect the time EMS took on the road. In addition, there are many dynamic factors that could possibly affect the time as well, and as of current time, there is no way to record every single one of them. What we can do right now is to record those data that might affect the traffic flow, and applied those as features.\nThe result is shown below: \nThe confusion matrix is shown below: \n\nThe result\nPrecision for class 0 (EMS took 10 minutes or less): 79%\nRecall for class 0: 31%\nF1-score for class 0: 45%\nPrecision for class 1 (EMS took more than 10 minutes): 35%\nRecall for class 1: 82%\nF1-score for class 1: 49%\nFrom the confusion matrix, The high number of false positives (802) relative to true negatives (346) indicates that the model is overly pessimistic about the EMS response time, often predicting delays where there are none. The model has a better true positive rate, with 893 correctly predicted delays, but this comes at the cost of a high false-positive rate. The false-negative count (196) is lower than the false positives, which suggests that when the model predicts a quick response, it is somewhat more likely to be correct. However, in emergency response situations, even a small number of false negatives can be critical.\nour model is trying to predict whether an emergency medical service (EMS) will take more than 10 minutes to arrive at the scene of an accident.\nThis accuracy tells us what portion of the total predictions made by the model were correct. Our model has an accuracy of approximately 47.06%, which means that about 47 out of every 100 predictions it makes about EMS arrival times are correct. It’s not very high, so the model is not very reliable in its current state.\nPrecision tells us how often the model is correct when it predicts a certain event. For instance, when our model predicts that the EMS will take more than 10 minutes to arrive, it is correct 35% of the time. Conversely, when it predicts that EMS will take 10 minutes or less, it is correct 79% of the time. High precision for a category means that when the model predicts that category, it’s usually right.\nThe F1-score is 49%, and for predictions of 10 minutes or less, it’s 45%. This suggests that the model is slightly better at predicting longer arrival times than shorter ones, but it still isn’t highly accurate in either case.\nGiven the accuracy and the confusion matrix, we see that the model has an accuracy of approximately 47.06% on the test set, which is not very high and is close to random guessing. This could indicate that the model is underfitting. It is too simplistic and not capturing the underlying patterns in the data well enough to make accurate predictions on either the training or the test set.\nA PRC Curve for evaluating the performance of a classifier \nA ROC Curve for evaluating the performance of classification models"
  },
  {
    "objectID": "NB.html#text-data-of-naïve-bayes",
    "href": "NB.html#text-data-of-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "Text Data of Naïve Bayes",
    "text": "Text Data of Naïve Bayes\nThe text data was extracted using News API, and I also precleaned to make sure it does not have noise for future use \n\nThe result\nThe confusion matrix for text is shown below: \nThe Multinomial Naive Bayes classifier has an overall accuracy of 50% on the test set. The classification report and confusion matrix give us a more detailed insight into the performance for each sentiment class:\n\nNegative Sentiment: The model has a high precision of 100% but a very low recall of 20%, indicating that while the predictions made as negative are all correct, the model fails to identify most of the negative instances\nNeutral Sentiment: The model fails to correctly identify any neutral sentiments, as indicated by both precision and recall being 0%.\nPositive Sentiment: The model has a precision of 47% with a recall of 100%, suggesting that while it identifies all positive instances, it also incorrectly labels some non-positive instances as positive.\n\nThe confusion matrix visualization shows the distribution of predictions across the actual sentiments. We can see that all negative and neutral sentiments are predominantly classified as positive, which is a sign of bias towards the positive class in the model’s predictions.\n\nEvaluation Metrics\nThe accuracy metric alone is not sufficient to assess the performance of the Naive Bayes classifier. Precision, recall, and F1-score provide a more comprehensive evaluation. The precision tells us the accuracy of the positive predictions made, recall gives us a measure of the model’s ability to find all the positive instances, and the F1-score is a harmonic mean of precision and recall.\n\n\nOverfitting and Underfitting\nThe model does not appear to be overfitting, as overfitting would typically present as a high accuracy on the training set but poor performance on the test set. However, the model might be underfitting since it is overly generalized, leading to poor performance across all metrics.\n\n\nModel Performance\nThe model’s performance is not ideal, with a low F1-score for negative and neutral classes and a moderate F1-score for the positive class. This suggests that while the model can predict positive sentiments relatively well, it struggles to distinguish between negative and neutral sentiments.\n\n\n\nprc text\n\n\n\n\n\nresult text\n\n\nThe visualizations confirm the earlier discussions about the model’s performance and suggest potential avenues for improvement, such as addressing the class imbalance or exploring more sophisticated models and features.​"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Xingrui Huo, with GU net ID: xh231\nThis is a webpage that introduces me.\n\n\n\n\nGeorgetown University, Master of Science, Data Science & Analytics\nGeoorgwetown is an excellent unviersity that sits in Georgetown, Washington, D.C.\n\n\n\nI spent my undergraduate life at Rutgers, the State University of New Jersey. RU Rah Rah!\n\n\n\nBefore my undergrad, I stayed back in China for 18 years, born and raised up in the same place."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "",
    "text": "Georgetown University, Master of Science, Data Science & Analytics\nGeoorgwetown is an excellent unviersity that sits in Georgetown, Washington, D.C.\n\n\n\nI spent my undergraduate life at Rutgers, the State University of New Jersey. RU Rah Rah!\n\n\n\nBefore my undergrad, I stayed back in China for 18 years, born and raised up in the same place."
  },
  {
    "objectID": "index.html#sports",
    "href": "index.html#sports",
    "title": "About Me",
    "section": "Sports",
    "text": "Sports\nI love to bike, and watch Formula 1 during the weekend. Sometimes I also play the simulator on PC just to have fun, and experience the Formula 1 in a more intuitive way"
  },
  {
    "objectID": "index.html#things-in-life",
    "href": "index.html#things-in-life",
    "title": "About Me",
    "section": "Things in Life",
    "text": "Things in Life\nI like cars, A LOT. I love everything from the design of the car, the engine, transmission of the car, to the difference on configration of the car due to the different law and environmental requirments in different countries. I also have a very strong interest on analyzing anything data-related to the car. I have done a project before, which focuses on analyzing lap time driven by myself in game F1 22. Now I have switched tracks to collect data from real-world, and to see if the data can inspire me in some ways!"
  },
  {
    "objectID": "index.html#foods",
    "href": "index.html#foods",
    "title": "About Me",
    "section": "Foods!",
    "text": "Foods!\nI love trying new foods in every places around the world. I also very much engaged with trying to re-make the same food at home. Ramen, steak, burrito, everything! Below are something I have tried to cook at home, and it was very good at the end!\n\n\n\nRamen\nDumpling\nZongzi\n\n\n\nPizza\nBarbecue\nSteak"
  },
  {
    "objectID": "index.html#this-is-a-hilarious-quote-from-jeremy-clarkson",
    "href": "index.html#this-is-a-hilarious-quote-from-jeremy-clarkson",
    "title": "About Me",
    "section": "This is a hilarious quote from Jeremy Clarkson",
    "text": "This is a hilarious quote from Jeremy Clarkson\n\nSpeed has never killed anyone, suddenly becoming stationary… That’s what gets you – Jeremy Clarkson\n\n\nSome Requirements\nThis is an inline math equation:  \\(x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\).\nThe area of a circle with radius (r) is given by the formula:\n\\[\n[A = \\pi r^2]\n\\]\nWhere:\n\n(A) is the area of the circle.\n(pi) is approximately 3.14159.\n(r) is the radius of the circle.\n\n\n\n\n\ngraph LR;\n  A[A4, A5] --&gt; B(S4, S5);\n  B --&gt; C(RS4, RS5);\n\n\n\n\n\n\nThis approach enables you to use the same bibliography with different citation styles without having to change anything about your document or the bibliography itself apart from the bibliography style when your paper is finally compiled for print. [@fenn_managing_2006]"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "About Me",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne same engine have slightly different models based on differnt vehicles, country law’s requirments, and marketing needs↩︎"
  },
  {
    "objectID": "Data Gathering.html",
    "href": "Data Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "The recod data I collected are mainly from NHTSA, the National Highway Traffic Safety Administration. The NHTSA have publications that contains many data that is quite useful for this research. However, some data are available in differnt kinds besides csv. So it will take quite a time to clean\n\n\nNow the raw data looks like this,  It’s quite messy on the top. So, I will need to clean the column names to make it tidy.\nAfter cleaning process, the data now looks like this, \nNote this is not enough, I still need to further tidy the column names to make it better\n\n\n\nThe data available about young driver information is mainly from NHTSA, and it’s in PDF format, so I will need to first download the data, scrape them into csv file, and then do more analysis.\n\n\n\n\nThere are also some text data available that I use API to pull them from the webiste. I will further clean them and also put them onto the page. \nSo right now I will need to sort the text out, like find out the publisher of the paper, the title, the description, etc."
  },
  {
    "objectID": "Data Gathering.html#record-data",
    "href": "Data Gathering.html#record-data",
    "title": "Data Gathering",
    "section": "",
    "text": "The recod data I collected are mainly from NHTSA, the National Highway Traffic Safety Administration. The NHTSA have publications that contains many data that is quite useful for this research. However, some data are available in differnt kinds besides csv. So it will take quite a time to clean\n\n\nNow the raw data looks like this,  It’s quite messy on the top. So, I will need to clean the column names to make it tidy.\nAfter cleaning process, the data now looks like this, \nNote this is not enough, I still need to further tidy the column names to make it better\n\n\n\nThe data available about young driver information is mainly from NHTSA, and it’s in PDF format, so I will need to first download the data, scrape them into csv file, and then do more analysis."
  },
  {
    "objectID": "Data Gathering.html#text-data",
    "href": "Data Gathering.html#text-data",
    "title": "Data Gathering",
    "section": "",
    "text": "There are also some text data available that I use API to pull them from the webiste. I will further clean them and also put them onto the page. \nSo right now I will need to sort the text out, like find out the publisher of the paper, the title, the description, etc."
  },
  {
    "objectID": "Data Cleaning Python.html",
    "href": "Data Cleaning Python.html",
    "title": "Data cleaning in text data",
    "section": "",
    "text": "First I use NewsAPI to get text data from NewsAPI on different key words in terms of driver behavior analysis\nThe key words I used are: driving behavior, distracted driving, driver risk assessment, driver profilling"
  },
  {
    "objectID": "Data Cleaning Python.html#record-data",
    "href": "Data Cleaning Python.html#record-data",
    "title": "Data cleaning in text data",
    "section": "Record Data",
    "text": "Record Data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n\n\n\ndata = pd.read_csv('Data/FARS2021NationalCSV/accident.csv', encoding='ISO-8859-1')\n\n# Define a function to convert hours and minutes into minutes since the start of the day\ndef convert_to_minutes(hour_col, min_col):\n    return hour_col * 60 + min_col\n\n# Convert notification time and arrival time into minutes\ndata['NOT_MINUTES'] = convert_to_minutes(data['NOT_HOUR'], data['NOT_MIN'])\ndata['ARR_MINUTES'] = convert_to_minutes(data['ARR_HOUR'], data['ARR_MIN'])\n\n# Calculate the time gap\ndata['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']\n\n# Handling cases where the time difference is negative due to crossing midnight\n# Assuming that EMS response times will be within a 24-hour period\ndata['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x &lt; 0 else 0))\n\n# Create the binary target variable\ndata['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] &gt; 10).astype(int)\n\n# Display the new columns\ndata[['NOT_HOUR', 'NOT_MIN', 'ARR_HOUR', 'ARR_MIN', 'TIME_DIFF', 'EMS_MORE_THAN_10_MIN']].head()\n\nvalid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]\n\n# Redefine the feature selection without the specified columns\nrevised_features = [\n    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', \n    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'\n]\n\n\nselected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')\n\n# Define the target variable\ny_revised = valid_data['EMS_MORE_THAN_10_MIN']\n\n# Split the revised data into training and testing sets\nX_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(\n    selected_features_revised, y_revised, test_size=0.2, random_state=0)\n\nmodel_revised = GaussianNB()\n\n# Train the revised model\nmodel_revised.fit(X_train_revised, y_train_revised)\n\n# Predict on revised test data\ny_pred_revised = model_revised.predict(X_test_revised)\n\ncm_revised = confusion_matrix(y_test_revised, y_pred_revised)\n\naccuracy_revised = accuracy_score(y_test_revised, y_pred_revised)\nreport_revised = classification_report(y_test_revised, y_pred_revised)\n\nprint(accuracy_revised)\nprint(report_revised)\n\n# Plotting the confusion matrix for the revised model\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_revised, annot=True, fmt='d', cmap='Blues', xticklabels=['&lt;=10 min', '&gt;10 min'], yticklabels=['&lt;=10 min', '&gt;10 min'])\nplt.title('Confusion Matrix for Revised EMS Arrival Time Prediction')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\n\n# Load the dataset\npdf_path = ('Data/FARS2021NationalCSV/accident.csv')\ndata = pd.read_csv(pdf_path, encoding='ISO-8859-1')\n\n# Preprocess the data as before\ndata['NOT_MINUTES'] = data['NOT_HOUR'] * 60 + data['NOT_MIN']\ndata['ARR_MINUTES'] = data['ARR_HOUR'] * 60 + data['ARR_MIN']\ndata['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']\ndata['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x &lt; 0 else 0))\ndata['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] &gt; 10).astype(int)\n\n# Remove invalid records (where NOT_HOUR or ARR_HOUR is 99)\nvalid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]\n\n# Redefine the feature selection without the specified columns\nrevised_features = [\n    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', \n    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'\n]\n\n# Prepare the feature matrix and target vector\nselected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')\ny_revised = valid_data['EMS_MORE_THAN_10_MIN']\n\n# Encode categorical variables\nlabel_encoders = {}\nfor column in selected_features_revised.select_dtypes(include=['object']).columns:\n    label_encoders[column] = LabelEncoder()\n    selected_features_revised[column] = label_encoders[column].fit_transform(selected_features_revised[column])\n\n# Split the dataset into train and test sets\nX_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(\n    selected_features_revised, y_revised, test_size=0.2, random_state=0)\n\n# Initialize and train the Gaussian Naive Bayes model\nmodel_revised = GaussianNB()\nmodel_revised.fit(X_train_revised, y_train_revised)\n\n# Predict probabilities for the test set\ny_scores_revised = model_revised.predict_proba(X_test_revised)[:, 1]\n\n# Compute precision-recall pairs for different probability thresholds\nprecision_revised, recall_revised, thresholds_revised = precision_recall_curve(y_test_revised, y_scores_revised)\n\n# Plot the Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall_revised, precision_revised, marker='.', label='Revised Naive Bayes')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Revised Model')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nfpr, tpr, roc_thresholds = roc_curve(y_test_revised, y_scores_revised)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "Data Cleaning Python.html#text-data",
    "href": "Data Cleaning Python.html#text-data",
    "title": "Data cleaning in text data",
    "section": "Text Data",
    "text": "Text Data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nfile_path = (\"Data/labeled_articles_sentiment.csv\")\ndata = pd.read_csv(file_path)\n\ndata['text'] = data['title'] + ' ' + data['description']\n\n# Encode the sentiment column to numerical values\nlabel_encoder = LabelEncoder()\ndata['sentiment_encoded'] = label_encoder.fit_transform(data['sentiment'])\n\n# Split the data into features and target\nX = data['text']\ny = data['sentiment_encoded']\n\n# Perform a train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n\n# Fit and transform the vectorizer on the training data and transform the testing data\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Show the shape of the resulting TF-IDF matrices\nX_train_tfidf.shape, X_test_tfidf.shape\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Initialize the Multinomial Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the classifier\nnb_classifier.fit(X_train_tfidf, y_train)\n\n# Predict the labels for the test set\ny_pred = nb_classifier.predict(X_test_tfidf)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\n# Create a confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\naccuracy, report\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import average_precision_score\n\n# Binarize the output labels for the multi-class case\ny_test_binarized = label_binarize(y_test, classes=range(len(label_encoder.classes_)))\n\n# Initialize a dictionary to hold the precision-recall curves for each class\nprecision_recall_curve_dict = {}\n\n# Calculate the precision-recall curve and average precision for each class\nfor i, class_label in enumerate(label_encoder.classes_):\n    class_precisions, class_recalls, class_thresholds = precision_recall_curve(y_test_binarized[:, i], y_scores[:, i])\n    precision_recall_curve_dict[class_label] = (class_precisions, class_recalls)\n    avg_precision = average_precision_score(y_test_binarized[:, i], y_scores[:, i])\n    print(f\"Average precision-recall score for class '{class_label}': {avg_precision:.2f}\")\n\n# Plot the precision-recall curve for each class\nplt.figure(figsize=(10, 6))\n\nfor class_label, (class_precisions, class_recalls) in precision_recall_curve_dict.items():\n    plt.plot(class_recalls, class_precisions, lw=2, label=f'Precision-Recall curve of class {class_label}')\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall curve per class\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\n# Let's check the actual counts for each sentiment in y_train and y_pred\ntrain_counts = pd.Series(y_train).value_counts().sort_index()\npred_counts = pd.Series(y_pred).value_counts().sort_index()\n\n# Now we plot the actual counts to verify the distribution\nfig, ax = plt.subplots(1, 2, figsize=(14, 7), sharey=True)\n\nsns.barplot(x=train_counts.index, y=train_counts.values, ax=ax[0], palette=\"viridis\")\nax[0].set_title('Distribution of Actual Sentiments (Training Set)')\nax[0].set_xlabel('Sentiment')\nax[0].set_ylabel('Count')\nax[0].set_xticklabels(label_encoder.inverse_transform(train_counts.index))\n\nsns.barplot(x=pred_counts.index, y=pred_counts.values, ax=ax[1], palette=\"viridis\")\nax[1].set_title('Distribution of Predicted Sentiments (Test Set)')\nax[1].set_xlabel('Sentiment')\nax[1].set_xticklabels(label_encoder.inverse_transform(pred_counts.index))\n\nplt.tight_layout()\nplt.show()\n\n# Show actual counts for verification\ntrain_counts, pred_counts"
  },
  {
    "objectID": "Code.html",
    "href": "Code.html",
    "title": "Code Section",
    "section": "",
    "text": "Link to my GitHubMy GitHub\nThis is a link to formula 1 Formula 1"
  },
  {
    "objectID": "Code.html#first-is-links-section",
    "href": "Code.html#first-is-links-section",
    "title": "Code Section",
    "section": "",
    "text": "Link to my GitHubMy GitHub\nThis is a link to formula 1 Formula 1"
  },
  {
    "objectID": "Data Exploration.html",
    "href": "Data Exploration.html",
    "title": "Explore the Data",
    "section": "",
    "text": "In this page we will explore the data in various kinds of ways to see what might catch out eyes\n\n\nThe data I have here is the accident summary conducted by the Fatality Analysis Reporting System (FARS); FARS is a nationwide census providing NHTSA, Congress and the American public yearly data regarding fatal injuries suffered in motor vehicle traffic crashes. This data contains very detailed information that records the condition of each accident.\nThe data was separated into different files, which can be joined together by a universal column, case number. So we can combine different files to get different information. \n\n\n\nSome basic summary statistics are mean, standard deviation, minimum, median, and variance. Among all these descriptive statistics, the only meaningful column is time, specifically, hour. That is when the accident happened during the day. \n\n\n\nThere are a couple of visulaized plots help us to see intuitivalely\n\n\nBelow the plot provides an overview of accident summaries distributed by state, allowing us to identify the states with the highest and lowest accident counts. \nFrom the chart, we can observe that the distribution is not uniform across states. Some states have a significantly higher number of entries compared to others. This could be due to various factors such as population, geographical size, traffic volume, or data collection methods.\n\n\n\nThis plot reveals the days of the week that are most likely to experience fatal crashes \nThe distribution of entries is relatively uniform across the days of the week. There seems to be a slight increase in entries on Saturdays and Sundays. This could suggest a potential increase in incidents or events during the weekends, but further analysis would be needed to confirm any such trends and to understand their causes.\n\n\n\nThis plot illustrates the direction in which a vehicle is most likely to experience an accident \nFrom this chart, we can observe that: “Non-Harmful Event” and “12 Clock Point” are the most common areas of impact recorded in the dataset. There are various other areas of impact with fewer occurrences. This information could be useful for understanding the common types of impacts and their frequencies, which might help in identifying areas for safety improvements.\n\n\n\n\nIn the Correlation Analysis step, we will:\n\nCalculate the correlation matrix for the numerical variables in the dataset.\nVisualize the correlations using a heatmap.\n\n\n\nLet’s start by calculating the correlation matrix. \nAs we can see from the matrix, STATE_x and ST_CASE have a very high positive correlation (almost 1), which is expected as the case number (ST_CASE) is likely assigned sequentially within each state.\nHOUR and MINUTE have a positive correlation of 0.2644, indicating a moderate relationship.\nEVENTNUM and VNUMBER1 also have a strong positive correlation of 0.8227, suggesting a strong relationship between these two variables.\nAOI1 and SOE have a positive correlation of 0.5630, indicating a moderate to strong relationship.\n\n\n\nNext, we will visualize these correlations using a heatmap to make it easier to interpret the relationships between variables. \nThe heatmap above visualizes the correlation matrix, providing a color-coded representation of the correlation coefficients between each pair of numerical variables.\nFrom the heatmap, we can easily identify strong positive correlations (dark red areas) and observe the relationships between different variables. For example, the strong positive correlation between “EVENTNUM” and “VNUMBER1” is clearly visible.\n\n\n\nTo investigate the relationship between the day of the week and the hour of the day, we can create a heatmap that displays the count of entries for each combination of day and hour. This will help us visualize if there are certain times of the day that are more prone to incidents on specific weekdays \nFrom this visualization, we can observe that:\nThe hours with the most entries tend to be in the afternoon and evening, particularly from around 14:00 to 18:00. This could be due to increased traffic during these hours.\nThere are fewer entries during the early morning hours, which is expected since there is typically less traffic at that time.\nSaturday and Sunday show a slightly different pattern compared to the weekdays, with more entries occurring late at night and in the early morning hours. This could be indicative of different driving behaviors during the weekends, potentially related to social activities.\nThe column labeled “Unknown” represents entries where the hour was not recorded or was unknown. These entries are spread across all days of the week.\n\n\n\n\nBased on the exploratory data analysis we’ve conducted so far, here are some potential hypotheses and questions:\n\n\nHypothesis: There are more incidents in the afternoon and evening compared to other times of the day. Potential Analysis: Investigate if certain types of incidents are more likely to occur during these times.\n\n\n\nHypothesis: Driving behavior during the weekends, especially late at night and in the early morning hours, leads to more incidents.\nPotential Analysis: Examine the types of incidents that occur during these times and if they are different from weekday incidents.\n\n\n\nQuestion: Why do some states have significantly more incidents recorded in the dataset? Is it due to population, traffic volume, or data collection methods?\nPotential Analysis: Normalize the data by population or traffic volume to better understand the state-wise distribution.\n\n\n\nHypothesis: Certain areas of impact, such as the “Non-Harmful Event” and “12 Clock Point”, are more common.\nPotential Analysis: Investigate the circumstances that lead to these common impact areas.\n\n\n\nHypothesis: The “Motor Vehicle In-Transport” event is the most common sequence of events leading to incidents.\nPotential Analysis: Explore what specific situations or factors contribute to this sequence of events.\n\n\n\n\nIf applicable, group or segment the data based on relevant criteria to uncover insights within specific subgroups.\n\n\n The data grouped by state shows the total number of entries (incidents) for each state:\n\nCalifornia and Texas have the highest number of entries, with 11,952 and 11,787 incidents respectively.\nStates like Alaska, District of Columbia, and Rhode Island have the lowest number of entries, all below 150 incidents.\nThis distribution could be influenced by various factors such as population, geographical size, traffic volume, and data collection methods.\n\n\n\n\n From this distribution, we can observe that:\n\nThe number of incidents is higher in the evening, followed by the afternoon and night.\nThe morning has the lowest number of incidents.\nThere are a significant number of incidents during the night, which could be worth investigating further, especially given the reduced traffic volumes during these hours.\n\n\n\n\n\nTo identify outliers, we can use various methods such as:\n\nZ-Scores: Calculate the Z-score of each data point and identify points with a Z-score beyond a certain threshold (e.g., |Z| &gt; 3).\nIQR Method: Calculate the Interquartile Range (IQR) and identify points beyond 1.5 * IQR from the quartiles.\nVisualizations: Use box plots to visually identify outliers.\n\nSince we have multiple numerical variables in our dataset, we will start by using box plots to visually identify outliers in these variables. \nFrom the box plots, we can observe that:\n\nSTATE_x: There are no visible outliers.\nST_CASE: This is a case number, and it doesn’t have outliers in the traditional sense\nDAY: There are no visible outliers.\nHOUR: There are values set to 99, which likely represent unknown or missing values rather than outliers.\nMINUTE: Similar to “HOUR”, there are values set to 99.\nEVENTNUM, VNUMBER1, AOI1, SOE: These variables have a significant number of high values that could be considered outliers. However, without more context on what these numbers represent, it’s challenging to definitively label them as outliers.\n\nTo properly handle the potential outliers, we would need additional context on the data and the variables, especially for the ones with coded values (EVENTNUM, VNUMBER1, AOI1, SOE).\n\n\n\n\n\nThe dataset contains information about various incidents, with a total of 112,725 entries. There are 13 columns, consisting of both numerical and categorical variables.\n\n\n\n\n\n\nSTATE_x: Represents the state codes, ranging from 1 to 56.\nST_CASE: A unique case number assigned to each incident.\nDAY: Day of the incident, ranging from 1 to 31.\nHOUR: Hour of the day when the incident occurred, ranging from 0 to 99 (with 99 representing unknown values).\nMINUTE: Minute of the hour when the incident occurred, ranging from 0 to 99 (with 99 representing unknown values).\nEVENTNUM, VNUMBER1, AOI1, SOE: Coded variables representing various aspects of the incidents.\n\n\n\n\n\nSTATENAME_x: The name of the state where the incident occurred.\nDAY_WEEKNAME: The day of the week when the incident occurred.\nAOI1NAME: Descriptions related to the area of impact.\nSOENAME: Descriptions related to the sequence of events.\n\n\n\n\n\n\n\n\nSTATENAME_x: California and Texas have the highest number of incidents.\nDAY_WEEKNAME: Incidents are fairly evenly distributed across days of the week, with a slight increase on Saturdays and Sundays.\nAOI1NAME: “Non-Harmful Event” and “12 Clock Point” are the most common areas of impact\nSOENAME: “Motor Vehicle In-Transport” is the most common sequence of events.\n\n\n\n\n\nA strong positive correlation exists between STATE_x and ST_CASE, as case numbers are likely assigned sequentially within each state. HOUR and MINUTE have a positive correlation, suggesting a moderate relationship. EVENTNUM and VNUMBER1 also show a strong positive correlation. AOI1 and SOE have a moderate to strong positive correlation.\n\n\n\nHypotheses were generated related to time of day and incidents, weekend driving behavior, state-wise distribution of incidents, impact areas, and sequence of events.\n\n\n\nBy State:\nCalifornia and Texas have the highest number of incidents. Alaska, District of Columbia, and Rhode Island have the lowest.\nBy Time of Day:\nThe evening has the highest number of incidents, followed by the afternoon and night. The morning has the lowest number of incidents.\n\n\n\nPotential outliers were identified in the variables HOUR, MINUTE, EVENTNUM, VNUMBER1, AOI1, and SOE. The values 99 in HOUR and MINUTE are likely placeholders for unknown values.\n\n\n\n\nThe analysis was performed using Python, leveraging libraries such as:\nPandas: For data manipulation and analysis. Matplotlib and Seaborn: For data visualization. NumPy: For numerical computations."
  },
  {
    "objectID": "Data Exploration.html#data-understanding",
    "href": "Data Exploration.html#data-understanding",
    "title": "Explore the Data",
    "section": "",
    "text": "The data I have here is the accident summary conducted by the Fatality Analysis Reporting System (FARS); FARS is a nationwide census providing NHTSA, Congress and the American public yearly data regarding fatal injuries suffered in motor vehicle traffic crashes. This data contains very detailed information that records the condition of each accident.\nThe data was separated into different files, which can be joined together by a universal column, case number. So we can combine different files to get different information."
  },
  {
    "objectID": "Data Exploration.html#descriptive-statistics",
    "href": "Data Exploration.html#descriptive-statistics",
    "title": "Explore the Data",
    "section": "",
    "text": "Some basic summary statistics are mean, standard deviation, minimum, median, and variance. Among all these descriptive statistics, the only meaningful column is time, specifically, hour. That is when the accident happened during the day."
  },
  {
    "objectID": "Data Exploration.html#data-visualization",
    "href": "Data Exploration.html#data-visualization",
    "title": "Explore the Data",
    "section": "",
    "text": "There are a couple of visulaized plots help us to see intuitivalely\n\n\nBelow the plot provides an overview of accident summaries distributed by state, allowing us to identify the states with the highest and lowest accident counts. \nFrom the chart, we can observe that the distribution is not uniform across states. Some states have a significantly higher number of entries compared to others. This could be due to various factors such as population, geographical size, traffic volume, or data collection methods.\n\n\n\nThis plot reveals the days of the week that are most likely to experience fatal crashes \nThe distribution of entries is relatively uniform across the days of the week. There seems to be a slight increase in entries on Saturdays and Sundays. This could suggest a potential increase in incidents or events during the weekends, but further analysis would be needed to confirm any such trends and to understand their causes.\n\n\n\nThis plot illustrates the direction in which a vehicle is most likely to experience an accident \nFrom this chart, we can observe that: “Non-Harmful Event” and “12 Clock Point” are the most common areas of impact recorded in the dataset. There are various other areas of impact with fewer occurrences. This information could be useful for understanding the common types of impacts and their frequencies, which might help in identifying areas for safety improvements."
  },
  {
    "objectID": "Data Exploration.html#correlation-analysis",
    "href": "Data Exploration.html#correlation-analysis",
    "title": "Explore the Data",
    "section": "",
    "text": "In the Correlation Analysis step, we will:\n\nCalculate the correlation matrix for the numerical variables in the dataset.\nVisualize the correlations using a heatmap.\n\n\n\nLet’s start by calculating the correlation matrix. \nAs we can see from the matrix, STATE_x and ST_CASE have a very high positive correlation (almost 1), which is expected as the case number (ST_CASE) is likely assigned sequentially within each state.\nHOUR and MINUTE have a positive correlation of 0.2644, indicating a moderate relationship.\nEVENTNUM and VNUMBER1 also have a strong positive correlation of 0.8227, suggesting a strong relationship between these two variables.\nAOI1 and SOE have a positive correlation of 0.5630, indicating a moderate to strong relationship.\n\n\n\nNext, we will visualize these correlations using a heatmap to make it easier to interpret the relationships between variables. \nThe heatmap above visualizes the correlation matrix, providing a color-coded representation of the correlation coefficients between each pair of numerical variables.\nFrom the heatmap, we can easily identify strong positive correlations (dark red areas) and observe the relationships between different variables. For example, the strong positive correlation between “EVENTNUM” and “VNUMBER1” is clearly visible.\n\n\n\nTo investigate the relationship between the day of the week and the hour of the day, we can create a heatmap that displays the count of entries for each combination of day and hour. This will help us visualize if there are certain times of the day that are more prone to incidents on specific weekdays \nFrom this visualization, we can observe that:\nThe hours with the most entries tend to be in the afternoon and evening, particularly from around 14:00 to 18:00. This could be due to increased traffic during these hours.\nThere are fewer entries during the early morning hours, which is expected since there is typically less traffic at that time.\nSaturday and Sunday show a slightly different pattern compared to the weekdays, with more entries occurring late at night and in the early morning hours. This could be indicative of different driving behaviors during the weekends, potentially related to social activities.\nThe column labeled “Unknown” represents entries where the hour was not recorded or was unknown. These entries are spread across all days of the week."
  },
  {
    "objectID": "Data Exploration.html#hypothesis-generation",
    "href": "Data Exploration.html#hypothesis-generation",
    "title": "Explore the Data",
    "section": "",
    "text": "Based on the exploratory data analysis we’ve conducted so far, here are some potential hypotheses and questions:\n\n\nHypothesis: There are more incidents in the afternoon and evening compared to other times of the day. Potential Analysis: Investigate if certain types of incidents are more likely to occur during these times.\n\n\n\nHypothesis: Driving behavior during the weekends, especially late at night and in the early morning hours, leads to more incidents.\nPotential Analysis: Examine the types of incidents that occur during these times and if they are different from weekday incidents.\n\n\n\nQuestion: Why do some states have significantly more incidents recorded in the dataset? Is it due to population, traffic volume, or data collection methods?\nPotential Analysis: Normalize the data by population or traffic volume to better understand the state-wise distribution.\n\n\n\nHypothesis: Certain areas of impact, such as the “Non-Harmful Event” and “12 Clock Point”, are more common.\nPotential Analysis: Investigate the circumstances that lead to these common impact areas.\n\n\n\nHypothesis: The “Motor Vehicle In-Transport” event is the most common sequence of events leading to incidents.\nPotential Analysis: Explore what specific situations or factors contribute to this sequence of events."
  },
  {
    "objectID": "Data Exploration.html#data-grouping-and-segmentation",
    "href": "Data Exploration.html#data-grouping-and-segmentation",
    "title": "Explore the Data",
    "section": "",
    "text": "If applicable, group or segment the data based on relevant criteria to uncover insights within specific subgroups.\n\n\n The data grouped by state shows the total number of entries (incidents) for each state:\n\nCalifornia and Texas have the highest number of entries, with 11,952 and 11,787 incidents respectively.\nStates like Alaska, District of Columbia, and Rhode Island have the lowest number of entries, all below 150 incidents.\nThis distribution could be influenced by various factors such as population, geographical size, traffic volume, and data collection methods.\n\n\n\n\n From this distribution, we can observe that:\n\nThe number of incidents is higher in the evening, followed by the afternoon and night.\nThe morning has the lowest number of incidents.\nThere are a significant number of incidents during the night, which could be worth investigating further, especially given the reduced traffic volumes during these hours."
  },
  {
    "objectID": "Data Exploration.html#identifying-outliers",
    "href": "Data Exploration.html#identifying-outliers",
    "title": "Explore the Data",
    "section": "",
    "text": "To identify outliers, we can use various methods such as:\n\nZ-Scores: Calculate the Z-score of each data point and identify points with a Z-score beyond a certain threshold (e.g., |Z| &gt; 3).\nIQR Method: Calculate the Interquartile Range (IQR) and identify points beyond 1.5 * IQR from the quartiles.\nVisualizations: Use box plots to visually identify outliers.\n\nSince we have multiple numerical variables in our dataset, we will start by using box plots to visually identify outliers in these variables. \nFrom the box plots, we can observe that:\n\nSTATE_x: There are no visible outliers.\nST_CASE: This is a case number, and it doesn’t have outliers in the traditional sense\nDAY: There are no visible outliers.\nHOUR: There are values set to 99, which likely represent unknown or missing values rather than outliers.\nMINUTE: Similar to “HOUR”, there are values set to 99.\nEVENTNUM, VNUMBER1, AOI1, SOE: These variables have a significant number of high values that could be considered outliers. However, without more context on what these numbers represent, it’s challenging to definitively label them as outliers.\n\nTo properly handle the potential outliers, we would need additional context on the data and the variables, especially for the ones with coded values (EVENTNUM, VNUMBER1, AOI1, SOE)."
  },
  {
    "objectID": "Data Exploration.html#report-and-discuss-your-methods-and-findings",
    "href": "Data Exploration.html#report-and-discuss-your-methods-and-findings",
    "title": "Explore the Data",
    "section": "",
    "text": "The dataset contains information about various incidents, with a total of 112,725 entries. There are 13 columns, consisting of both numerical and categorical variables.\n\n\n\n\n\n\nSTATE_x: Represents the state codes, ranging from 1 to 56.\nST_CASE: A unique case number assigned to each incident.\nDAY: Day of the incident, ranging from 1 to 31.\nHOUR: Hour of the day when the incident occurred, ranging from 0 to 99 (with 99 representing unknown values).\nMINUTE: Minute of the hour when the incident occurred, ranging from 0 to 99 (with 99 representing unknown values).\nEVENTNUM, VNUMBER1, AOI1, SOE: Coded variables representing various aspects of the incidents.\n\n\n\n\n\nSTATENAME_x: The name of the state where the incident occurred.\nDAY_WEEKNAME: The day of the week when the incident occurred.\nAOI1NAME: Descriptions related to the area of impact.\nSOENAME: Descriptions related to the sequence of events.\n\n\n\n\n\n\n\n\nSTATENAME_x: California and Texas have the highest number of incidents.\nDAY_WEEKNAME: Incidents are fairly evenly distributed across days of the week, with a slight increase on Saturdays and Sundays.\nAOI1NAME: “Non-Harmful Event” and “12 Clock Point” are the most common areas of impact\nSOENAME: “Motor Vehicle In-Transport” is the most common sequence of events.\n\n\n\n\n\nA strong positive correlation exists between STATE_x and ST_CASE, as case numbers are likely assigned sequentially within each state. HOUR and MINUTE have a positive correlation, suggesting a moderate relationship. EVENTNUM and VNUMBER1 also show a strong positive correlation. AOI1 and SOE have a moderate to strong positive correlation.\n\n\n\nHypotheses were generated related to time of day and incidents, weekend driving behavior, state-wise distribution of incidents, impact areas, and sequence of events.\n\n\n\nBy State:\nCalifornia and Texas have the highest number of incidents. Alaska, District of Columbia, and Rhode Island have the lowest.\nBy Time of Day:\nThe evening has the highest number of incidents, followed by the afternoon and night. The morning has the lowest number of incidents.\n\n\n\nPotential outliers were identified in the variables HOUR, MINUTE, EVENTNUM, VNUMBER1, AOI1, and SOE. The values 99 in HOUR and MINUTE are likely placeholders for unknown values."
  },
  {
    "objectID": "Data Exploration.html#tools-and-software",
    "href": "Data Exploration.html#tools-and-software",
    "title": "Explore the Data",
    "section": "",
    "text": "The analysis was performed using Python, leveraging libraries such as:\nPandas: For data manipulation and analysis. Matplotlib and Seaborn: For data visualization. NumPy: For numerical computations."
  },
  {
    "objectID": "slides/slides.html#germany-vehicles",
    "href": "slides/slides.html#germany-vehicles",
    "title": "Intro to Cars",
    "section": "Germany Vehicles",
    "text": "Germany Vehicles\n\nAudi\nBMW\nMercedes-Benz\nPorsche"
  },
  {
    "objectID": "slides/slides.html#america-vehicles",
    "href": "slides/slides.html#america-vehicles",
    "title": "Intro to Cars",
    "section": "America Vehicles",
    "text": "America Vehicles\n\nFord\nLincoln\nJeep\nCorvette\nDodge"
  },
  {
    "objectID": "slides/slides.html#asia-vehicles",
    "href": "slides/slides.html#asia-vehicles",
    "title": "Intro to Cars",
    "section": "Asia Vehicles",
    "text": "Asia Vehicles\n\nHonda\nToyota\nXpeng\nNIO"
  },
  {
    "objectID": "slides/slides.html#europe-cars---audi-a-series",
    "href": "slides/slides.html#europe-cars---audi-a-series",
    "title": "Intro to Cars",
    "section": "Europe Cars - Audi A Series",
    "text": "Europe Cars - Audi A Series\n\n\nA1\nA3\nA4\nA5\nA6\nA7\nA8"
  },
  {
    "objectID": "slides/slides.html#audi-a4",
    "href": "slides/slides.html#audi-a4",
    "title": "Intro to Cars",
    "section": "Audi A4",
    "text": "Audi A4\n\nAudi A4"
  },
  {
    "objectID": "slides/slides.html#audi-a8",
    "href": "slides/slides.html#audi-a8",
    "title": "Intro to Cars",
    "section": "AUdi A8",
    "text": "AUdi A8\n\nAudi A8"
  },
  {
    "objectID": "slides/slides.html#a-citation",
    "href": "slides/slides.html#a-citation",
    "title": "Intro to Cars",
    "section": "A Citation",
    "text": "A Citation\nThis approach enables you to use the same bibliography with different citation styles without having to change anything about your document or the bibliography itself apart from the bibliography style when your paper is finally compiled for print. (Fenn 2006)"
  },
  {
    "objectID": "slides/slides.html#a-plot",
    "href": "slides/slides.html#a-plot",
    "title": "Intro to Cars",
    "section": "A Plot",
    "text": "A Plot\n\n\n\n\n\n\n\nFenn, Jürgen. 2006. “Managing Citations and Your Bibliography with BibTEX.” The PracTEX Journal. http://svn.tug.org/pracjourn/2006-4/fenn/fenn.pdf."
  },
  {
    "objectID": "DataMain.html",
    "href": "DataMain.html",
    "title": "Everything About Data",
    "section": "",
    "text": "Data\nData is the foundation of every data analysis, with more data, the more information we will get. On those sub-tab, you will find different steps of data analysis. It includes how I collected the data, how to clean those data, and in what ways I can explore them and elaborate based on those data."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Written by Xingrui Huo Froome\nGU ID: xh231\n\n\nAs I moved to a new place for grad school, my car moved with me and so does the car insurance. However, as I requested for the new quote, I found out that the price went higher than expected. So I changed my insurance company and during the conversation with the new insurance company. I found out that there is one discount that can be added to my policy if I agree to let the insurance company to send me a device and put it in my vehicle to record my daily drive data.\nI found that very interesting because the insurance staff told me that as I driving, the device will record data and send it back to the insurance company. As to every end of policy, I will receive discount based on my driving records.\nI start to become interested about this little device. How does it record the data? What kind of data does it record? What kind of result can it get based on the data it records? I feel like that I can do the same thing, collect the same data, and do some similar analysis on myself. So I can do driver behavior analysis on myself. Of course, the most important thing in here is, how to do driver behavior analysis?\nBelow are two academic sources and some questions to start with:\n\n\n\n(Kaplan et al. 2015)\nThis is a link to source\n\n\nDriver drowsiness and distraction are two main reasons for traffic accidents and the related financial losses. Therefore, researchers have been working for more than a decade on designing driver inattention monitoring systems. As a result, several detection techniques for the detection of both drowsiness and distraction have been proposed in the literature. Some of these techniques were successfully adopted and implemented by the leading car companies. This paper discusses and provides a comprehensive insight into the well-established techniques for driver inattention monitoring and introduces the use of most recent and futuristic solutions exploiting mobile technologies such as smartphones and wearable devices. Then, a proposal is made for the active of such systems into car-to-car communication to support vehicular ad hoc network’s (VANET’s) primary aim of safe driving. We call this approach the dissemination of driver behavior via C2C communication. Throughout this paper, the most remarkable studies of the last five years were examined thoroughly in order to reveal the recent driver monitoring techniques and demonstrate the basic pros and cons. In addition, the studies were categorized into two groups: driver drowsiness and distraction. Then, research on the driver drowsiness was further divided into two main subgroups based on the exploitation of either visual features or nonvisual features. A comprehensive compilation, including used features, classification methods, accuracy rates, system parameters, and environmental details, was represented as tables to highlight the (dis)advantages and/or limitations of the aforementioned categories. A similar approach was also taken for the methods used for the detection of driver distraction.\n\n\n\nthe literature review highlighted the need for combining techniques, the importance of alerting mechanisms, and the potential of C2C communication for improving driver safety. Despite limitations, ongoing advancements indicate a promising future for driver monitoring and assistance systems.\n\n\n\n\n(Chan et al. 2019)\nThis is a link to source\n\n\nHuman factors are the primary catalyst for traffic accidents. Among different factors, fatigue, distraction, drunkenness, and/or recklessness are the most common types of abnormal driving behavior that leads to an accident. With technological advances, modern smartphones have the capabilities for driving behavior analysis. There has not yet been a comprehensive review on methodologies utilizing only a smartphone for drowsiness detection and abnormal driver behavior detection. In this paper, different methodologies proposed by different authors are discussed. It includes the sensing schemes, detection algorithms, and their corresponding accuracy and limitations. Challenges and possible solutions such as integration of the smartphone behavior classification system with the concept of context-aware, mobile crowdsensing, and active steering control are analyzed. The issue of model training and updating on the smartphone and cloud environment is also included.\n\n\n\nThe paper discusses the use of smartphone data as a valuable resource for analyzing driver behavior. It reviews various methodologies proposed by different authors for detecting abnormal driving patterns. While smartphone-based systems offer advantages over telematics boxes, there are significant challenges to consider for accurate driver behavior classification.\nKey challenges include the impact of varying light conditions on vision-based methodologies and the need to eliminate noise and external factors in sensor-based approaches, such as road conditions and vehicle components. Future research should address these issues. Additionally, eliminating the requirement for mounting the smartphone in a fixed position would enhance flexibility and convenience for drivers.\nThe paper also explores potential solutions, such as integrating smartphone-based driver behavior analysis with context-awareness, crowdsensing, and steering control, as well as mobile or cloud-based algorithm training and updates. While these solutions may not be perfect, they have the potential to address some of the challenges, suggesting that there is room for further improvement in smartphone-based behavior analysis systems.\n\n\n\n\n\n\n\nHow can we accurately measure and quantify aggressive driving behavior using telematics data?\nWhat are the most effective strategies for detecting and reducing instances of distracted driving?\nCan machine learning algorithms predict the likelihood of a driver engaging in risky behaviors based on historical data?\nHow does driver behavior vary across different age groups, genders, and experience levels, and what implications does this have for road safety?\nHow can data analytics be used to design targeted driver training programs that address specific behavioral issues?\n\n\n\n\n\nHow do external factors like road conditions and traffic congestion influence driver behavior, and how can this information be used to improve road safety?\nHow do weather conditions, such as rain, snow, and fog, affect road safety, and what strategies can be employed to mitigate their impact?\nHow can data science be used to analyze the effectiveness of road safety policies, such as seat belt laws and speed limits, in reducing accidents?\nWhat role does human psychology play in road safety, and how can behavioral insights be used to design safer road systems?\n\n\n\n\n\nHow does driver behavior impact fuel efficiency, and how can eco-driving habits be promoted to reduce emissions and fuel consumption?\nHow can driver behavior analysis be used to optimize traffic flow and reduce congestion in urban areas?\nHow can driver behavior data be anonymized and protected to ensure privacy while still extracting valuable insights for road safety?"
  },
  {
    "objectID": "Introduction.html#research-background",
    "href": "Introduction.html#research-background",
    "title": "Introduction",
    "section": "",
    "text": "As I moved to a new place for grad school, my car moved with me and so does the car insurance. However, as I requested for the new quote, I found out that the price went higher than expected. So I changed my insurance company and during the conversation with the new insurance company. I found out that there is one discount that can be added to my policy if I agree to let the insurance company to send me a device and put it in my vehicle to record my daily drive data.\nI found that very interesting because the insurance staff told me that as I driving, the device will record data and send it back to the insurance company. As to every end of policy, I will receive discount based on my driving records.\nI start to become interested about this little device. How does it record the data? What kind of data does it record? What kind of result can it get based on the data it records? I feel like that I can do the same thing, collect the same data, and do some similar analysis on myself. So I can do driver behavior analysis on myself. Of course, the most important thing in here is, how to do driver behavior analysis?\nBelow are two academic sources and some questions to start with:"
  },
  {
    "objectID": "Introduction.html#driver-behavior-analysis-for-safe-driving-a-survey",
    "href": "Introduction.html#driver-behavior-analysis-for-safe-driving-a-survey",
    "title": "Introduction",
    "section": "",
    "text": "(Kaplan et al. 2015)\nThis is a link to source\n\n\nDriver drowsiness and distraction are two main reasons for traffic accidents and the related financial losses. Therefore, researchers have been working for more than a decade on designing driver inattention monitoring systems. As a result, several detection techniques for the detection of both drowsiness and distraction have been proposed in the literature. Some of these techniques were successfully adopted and implemented by the leading car companies. This paper discusses and provides a comprehensive insight into the well-established techniques for driver inattention monitoring and introduces the use of most recent and futuristic solutions exploiting mobile technologies such as smartphones and wearable devices. Then, a proposal is made for the active of such systems into car-to-car communication to support vehicular ad hoc network’s (VANET’s) primary aim of safe driving. We call this approach the dissemination of driver behavior via C2C communication. Throughout this paper, the most remarkable studies of the last five years were examined thoroughly in order to reveal the recent driver monitoring techniques and demonstrate the basic pros and cons. In addition, the studies were categorized into two groups: driver drowsiness and distraction. Then, research on the driver drowsiness was further divided into two main subgroups based on the exploitation of either visual features or nonvisual features. A comprehensive compilation, including used features, classification methods, accuracy rates, system parameters, and environmental details, was represented as tables to highlight the (dis)advantages and/or limitations of the aforementioned categories. A similar approach was also taken for the methods used for the detection of driver distraction.\n\n\n\nthe literature review highlighted the need for combining techniques, the importance of alerting mechanisms, and the potential of C2C communication for improving driver safety. Despite limitations, ongoing advancements indicate a promising future for driver monitoring and assistance systems."
  },
  {
    "objectID": "Introduction.html#a-comprehensive-review-of-driver-behavior-analysis-utilizing-smartphones",
    "href": "Introduction.html#a-comprehensive-review-of-driver-behavior-analysis-utilizing-smartphones",
    "title": "Introduction",
    "section": "",
    "text": "(Chan et al. 2019)\nThis is a link to source\n\n\nHuman factors are the primary catalyst for traffic accidents. Among different factors, fatigue, distraction, drunkenness, and/or recklessness are the most common types of abnormal driving behavior that leads to an accident. With technological advances, modern smartphones have the capabilities for driving behavior analysis. There has not yet been a comprehensive review on methodologies utilizing only a smartphone for drowsiness detection and abnormal driver behavior detection. In this paper, different methodologies proposed by different authors are discussed. It includes the sensing schemes, detection algorithms, and their corresponding accuracy and limitations. Challenges and possible solutions such as integration of the smartphone behavior classification system with the concept of context-aware, mobile crowdsensing, and active steering control are analyzed. The issue of model training and updating on the smartphone and cloud environment is also included.\n\n\n\nThe paper discusses the use of smartphone data as a valuable resource for analyzing driver behavior. It reviews various methodologies proposed by different authors for detecting abnormal driving patterns. While smartphone-based systems offer advantages over telematics boxes, there are significant challenges to consider for accurate driver behavior classification.\nKey challenges include the impact of varying light conditions on vision-based methodologies and the need to eliminate noise and external factors in sensor-based approaches, such as road conditions and vehicle components. Future research should address these issues. Additionally, eliminating the requirement for mounting the smartphone in a fixed position would enhance flexibility and convenience for drivers.\nThe paper also explores potential solutions, such as integrating smartphone-based driver behavior analysis with context-awareness, crowdsensing, and steering control, as well as mobile or cloud-based algorithm training and updates. While these solutions may not be perfect, they have the potential to address some of the challenges, suggesting that there is room for further improvement in smartphone-based behavior analysis systems."
  },
  {
    "objectID": "Introduction.html#questions",
    "href": "Introduction.html#questions",
    "title": "Introduction",
    "section": "",
    "text": "How can we accurately measure and quantify aggressive driving behavior using telematics data?\nWhat are the most effective strategies for detecting and reducing instances of distracted driving?\nCan machine learning algorithms predict the likelihood of a driver engaging in risky behaviors based on historical data?\nHow does driver behavior vary across different age groups, genders, and experience levels, and what implications does this have for road safety?\nHow can data analytics be used to design targeted driver training programs that address specific behavioral issues?\n\n\n\n\n\nHow do external factors like road conditions and traffic congestion influence driver behavior, and how can this information be used to improve road safety?\nHow do weather conditions, such as rain, snow, and fog, affect road safety, and what strategies can be employed to mitigate their impact?\nHow can data science be used to analyze the effectiveness of road safety policies, such as seat belt laws and speed limits, in reducing accidents?\nWhat role does human psychology play in road safety, and how can behavioral insights be used to design safer road systems?\n\n\n\n\n\nHow does driver behavior impact fuel efficiency, and how can eco-driving habits be promoted to reduce emissions and fuel consumption?\nHow can driver behavior analysis be used to optimize traffic flow and reduce congestion in urban areas?\nHow can driver behavior data be anonymized and protected to ensure privacy while still extracting valuable insights for road safety?"
  },
  {
    "objectID": "ts.html",
    "href": "ts.html",
    "title": "Grouping by State",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\naccident = pd.read_csv('Data/FARS2021NationalCSV/accident.csv', encoding='ISO-8859-1')\nevent = pd.read_csv('Data/FARS2021NationalCSV/cevent.csv', encoding='ISO-8859-1')\naccident.columns = accident.columns.str.strip()\nevent.columns = event.columns.str.strip()\naccident_columns_to_drop = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n                   36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n                   64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n\n# Drop the specified columns by index\naccident = accident.drop(accident.columns[accident_columns_to_drop], axis=1)\n\nevent_columns_to_drop = [9, 10, 11, 12]\nevent = event.drop(event.columns[event_columns_to_drop], axis = 1)\ndf = pd.merge(accident, event, on='ST_CASE', how='inner')\ndf\n\n\n\n\n\n\n\n\nSTATE_x\nSTATENAME_x\nST_CASE\nDAYNAME\nDAY_WEEKNAME\nHOUR\nMINUTE\nSTATE_y\nSTATENAME_y\nEVENTNUM\nVNUMBER1\nAOI1\nAOI1NAME\nSOE\nSOENAME\n\n\n\n\n0\n1\nAlabama\n10001\n12\nFriday\n22\n10\n1\nAlabama\n1\n1\n12\n12 Clock Point\n12\nMotor Vehicle In-Transport\n\n\n1\n1\nAlabama\n10002\n11\nThursday\n18\n0\n1\nAlabama\n1\n1\n55\nNon-Harmful Event\n64\nRan Off Roadway - Left\n\n\n2\n1\nAlabama\n10002\n11\nThursday\n18\n0\n1\nAlabama\n2\n1\n11\n11 Clock Point\n25\nConcrete Traffic Barrier\n\n\n3\n1\nAlabama\n10002\n11\nThursday\n18\n0\n1\nAlabama\n3\n1\n55\nNon-Harmful Event\n69\nRe-entering Roadway\n\n\n4\n1\nAlabama\n10002\n11\nThursday\n18\n0\n1\nAlabama\n4\n1\n55\nNon-Harmful Event\n63\nRan Off Roadway - Right\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n112720\n56\nWyoming\n560102\n15\nWednesday\n10\n34\n56\nWyoming\n1\n1\n55\nNon-Harmful Event\n63\nRan Off Roadway - Right\n\n\n112721\n56\nWyoming\n560102\n15\nWednesday\n10\n34\n56\nWyoming\n2\n1\n0\nNon-Collision\n1\nRollover/Overturn\n\n\n112722\n56\nWyoming\n560103\n19\nSunday\n17\n9\n56\nWyoming\n1\n1\n12\n12 Clock Point\n8\nPedestrian\n\n\n112723\n56\nWyoming\n560104\n20\nMonday\n6\n30\n56\nWyoming\n1\n1\n55\nNon-Harmful Event\n68\nCross Centerline\n\n\n112724\n56\nWyoming\n560104\n20\nMonday\n6\n30\n56\nWyoming\n2\n1\n12\n12 Clock Point\n12\nMotor Vehicle In-Transport\n\n\n\n\n112725 rows × 15 columns\nprint(df.shape)\n\n(112725, 15)\ndf = df.drop(columns=['STATE_y', 'STATENAME_y'])\nnumerical_vars = df.select_dtypes(include=[np.number])\nnumerical_summary = numerical_vars.describe()\n\n# Calculate variance for numerical variables (since it's not included in the describe method by default)\nvariance = numerical_vars.var()\n\n# Add variance to the summary statistics\nnumerical_summary.loc['variance'] = variance\n\nnumerical_summary\n\n\n\n\n\n\n\n\nSTATE_x\nST_CASE\nDAYNAME\nHOUR\nMINUTE\nEVENTNUM\nVNUMBER1\nAOI1\nSOE\n\n\n\n\ncount\n112725.000000\n1.127250e+05\n112725.000000\n112725.000000\n112725.000000\n112725.000000\n112725.000000\n112725.000000\n112725.000000\n\n\nmean\n27.475697\n2.756416e+05\n15.631076\n13.311040\n29.082990\n2.706480\n1.277764\n34.825301\n35.383633\n\n\nstd\n16.452688\n1.644243e+05\n8.871515\n10.491582\n18.481930\n3.216187\n2.629577\n30.664740\n25.523239\n\n\nmin\n1.000000\n1.000100e+04\n1.000000\n0.000000\n0.000000\n1.000000\n1.000000\n0.000000\n1.000000\n\n\n25%\n12.000000\n1.222640e+05\n8.000000\n7.000000\n14.000000\n1.000000\n1.000000\n12.000000\n12.000000\n\n\n50%\n27.000000\n2.704430e+05\n16.000000\n14.000000\n30.000000\n2.000000\n1.000000\n12.000000\n34.000000\n\n\n75%\n42.000000\n4.207750e+05\n23.000000\n19.000000\n44.000000\n3.000000\n1.000000\n55.000000\n63.000000\n\n\nmax\n56.000000\n5.601040e+05\n31.000000\n99.000000\n99.000000\n134.000000\n130.000000\n99.000000\n99.000000\n\n\nvariance\n270.690950\n2.703535e+10\n78.703777\n110.073297\n341.581745\n10.343860\n6.914676\n940.326272\n651.435726\nsns.set_style(\"whitegrid\")\n\n# Function to create bar plots for categorical variables\ndef plot_categorical_distribution(data, column_name, plot_size=(10, 6), rotation_angle=90):\n    plt.figure(figsize=plot_size)\n    ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette=\"husl\")\n    ax.set_title(f'Crash Distribution Summary of {column_name}', fontsize=15)\n    ax.set_ylabel(column_name, fontsize=12)\n    ax.set_xlabel('Count', fontsize=12)\n    plt.xticks(rotation=rotation_angle)\n    plt.show()\n\n# Plot the distribution of STATENAME_x\nplot_categorical_distribution(df, 'STATENAME_x')\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/2693093590.py:6: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette=\"husl\")\nplot_categorical_distribution(df, 'DAY_WEEKNAME', rotation_angle=0)\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/2693093590.py:6: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette=\"husl\")\nplot_categorical_distribution(df, 'AOI1NAME', plot_size=(10, 8))\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/2693093590.py:6: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette=\"husl\")\n# Calculate the correlation matrix for Step 4\ncorrelation_matrix = numerical_vars.corr()\n\ncorrelation_matrix\nplt.figure(figsize=(10, 8))\n\n# Create a heatmap to visualize the correlation matrix\nax = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\nax.set_title('Correlation Matrix', fontsize=15)\nplt.show()\nplt.figure(figsize=(10, 8))\nax = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\nax.set_title('Correlation Matrix', fontsize=15)\nplt.show()\n# handle the 99 (unknown) values\ndf['HOUR'] = df['HOUR'].astype(str).replace('99', 'Unknown')\n\n# Create a pivot table to count the number of DAY_WEEKNAME and HOUR\nhour_weekday_pivot = pd.pivot_table(df, index='DAY_WEEKNAME', columns='HOUR', aggfunc='size', fill_value=0)\n\n# Order the days of the week\ndays_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nhour_weekday_pivot = hour_weekday_pivot.reindex(days_order)\n\nplt.figure(figsize=(15, 7))\nax = sns.heatmap(hour_weekday_pivot, cmap=\"YlGnBu\", linewidths=.5)\nax.set_title('Number of Entries by Day of the Week and Hour of the Day', fontsize=15)\nax.set_xlabel('Hour of the Day', fontsize=12)\nax.set_ylabel('Day of the Week', fontsize=12)\nplt.show()\nHypothesis Generation Based on the exploratory data analysis we’ve conducted so far, here are some potential hypotheses and questions:\nTime of Day and Incidents:\nHypothesis: There are more incidents in the afternoon and evening compared to other times of the day. Potential Analysis: Investigate if certain types of incidents are more likely to occur during these times. Weekend Driving Behavior:\nHypothesis: Driving behavior during the weekends, especially late at night and in the early morning hours, leads to more incidents. Potential Analysis: Examine the types of incidents that occur during these times and if they are different from weekday incidents. State-wise Distribution:\nQuestion: Why do some states have significantly more incidents recorded in the dataset? Is it due to population, traffic volume, or data collection methods? Potential Analysis: Normalize the data by population or traffic volume to better understand the state-wise distribution. Impact Areas:\nHypothesis: Certain areas of impact, such as the “Non-Harmful Event” and “12 Clock Point”, are more common. Potential Analysis: Investigate the circumstances that lead to these common impact areas. Sequence of Events:\nHypothesis: The “Motor Vehicle In-Transport” event is the most common sequence of events leading to incidents. Potential Analysis: Explore what specific situations or factors contribute to this sequence of events.\nCalifornia and Texas have the highest number of entries, with 11,952 and 11,787 incidents respectively. States like Alaska, District of Columbia, and Rhode Island have the lowest number of entries, all below 150 incidents.\n# Group the data by state and \n# calculate the total number of entries for each state\nstate_group = df.groupby('STATENAME_x').size().sort_values(ascending=False)\n\nstate_group\n\nplt.figure(figsize=(12, 8))\n\n# Create a bar plot for the number of entries by state\nax = sns.barplot(x=state_group.index, y=state_group.values, palette=\"husl\")\n\nax.set_title('Number of Entries by State', fontsize=15)\nax.set_xlabel('State', fontsize=12)\nax.set_ylabel('Number of Entries', fontsize=12)\nplt.xticks(rotation=90)\nplt.show()\n\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/29044026.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.barplot(x=state_group.index, y=state_group.values, palette=\"husl\")"
  },
  {
    "objectID": "ts.html#grouping-by-time-of-the-day",
    "href": "ts.html#grouping-by-time-of-the-day",
    "title": "Grouping by State",
    "section": "Grouping by Time of the Day",
    "text": "Grouping by Time of the Day\n\n# Convert HOUR back to numeric, treating \"Unknown\" as a missing value\ndf['HOUR'] = pd.to_numeric(df['HOUR'], errors='coerce')\n\n# Define a function to categorize the time of day\ndef categorize_time_of_day(hour):\n    if pd.isna(hour):\n        return \"Unknown\"\n    elif 6 &lt;= hour &lt; 12:\n        return \"Morning\"\n    elif 12 &lt;= hour &lt; 18:\n        return \"Afternoon\"\n    elif 18 &lt;= hour &lt; 24:\n        return \"Evening\"\n    else:\n        return \"Night\"\n\n# Apply the function to create a new variable \"TIME_OF_DAY\"\ndf['TIME_OF_DAY'] = df['HOUR'].apply(categorize_time_of_day)\n\n# Group the data by \"TIME_OF_DAY\" and calculate the total number of entries for each time segment\ntime_of_day_group = df.groupby('TIME_OF_DAY').size().sort_index()\n\ntime_of_day_group\n\n# Set the size of the plot\nplt.figure(figsize=(10, 6))\n\n# Create a bar plot for the number of entries by time of day\nax = sns.barplot(x=time_of_day_group.index, y=time_of_day_group.values, palette=\"husl\")\n\n# Set the title and labels\nax.set_title('Number of Entries by Time of Day', fontsize=15)\nax.set_xlabel('Time of Day', fontsize=12)\nax.set_ylabel('Number of Entries', fontsize=12)\n\n# Show the plot\nplt.show()\n\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3951407133.py:29: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.barplot(x=time_of_day_group.index, y=time_of_day_group.values, palette=\"husl\")\n\n\n\n\n\nStep 7\n\n\nplt.figure(figsize=(15, 10))\nfor i, column in enumerate(numerical_vars.columns, 1):\n    plt.subplot(3, 3, i)\n    sns.boxplot(x=df[column])\n    plt.title(column)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nEVENTNUM, VNUMBER1, AOI1, SOE: These variables have a significant number of high values that could be considered outliers. However, without more context on what these numbers represent, it’s challenging to definitively label them as outliers. To properly handle the potential outliers, we would need additional context on the data and the variables, especially for the ones with coded values (EVENTNUM, VNUMBER1, AOI1, SOE)."
  },
  {
    "objectID": "ts.html#data-cleaning-for-naïve-bayes",
    "href": "ts.html#data-cleaning-for-naïve-bayes",
    "title": "Grouping by State",
    "section": "Data Cleaning for Naïve Bayes",
    "text": "Data Cleaning for Naïve Bayes\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n\n\n\ndata = pd.read_csv('Data/FARS2021NationalCSV/accident.csv', encoding='ISO-8859-1')\n\nNext is to create a new column that calculate the time gap between the notification time and the arrival time\n\n# Define a function to convert hours and minutes into minutes since the start of the day\ndef convert_to_minutes(hour_col, min_col):\n    return hour_col * 60 + min_col\n\n# Convert notification time and arrival time into minutes\ndata['NOT_MINUTES'] = convert_to_minutes(data['NOT_HOUR'], data['NOT_MIN'])\ndata['ARR_MINUTES'] = convert_to_minutes(data['ARR_HOUR'], data['ARR_MIN'])\n\n# Calculate the time gap\ndata['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']\n\n# Handling cases where the time difference is negative due to crossing midnight\n# Assuming that EMS response times will be within a 24-hour period\ndata['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x &lt; 0 else 0))\n\n# Create the binary target variable\ndata['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] &gt; 10).astype(int)\n\n# Display the new columns\ndata[['NOT_HOUR', 'NOT_MIN', 'ARR_HOUR', 'ARR_MIN', 'TIME_DIFF', 'EMS_MORE_THAN_10_MIN']].head()\n\n\n\n\n\n\n\n\nNOT_HOUR\nNOT_MIN\nARR_HOUR\nARR_MIN\nTIME_DIFF\nEMS_MORE_THAN_10_MIN\n\n\n\n\n0\n22\n13\n22\n25\n12\n1\n\n\n1\n99\n99\n19\n9\n-3450\n0\n\n\n2\n9\n29\n9\n40\n11\n1\n\n\n3\n16\n20\n16\n28\n8\n0\n\n\n4\n22\n20\n22\n30\n10\n0\n\n\n\n\n\n\n\nHere we have some rows that notification time is 99, which indicates invalid time, we will remove it first\n\n# Remove records with placeholder values for hours or minutes (assuming '99' is the placeholder value)\nvalid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]\n\nHere, for feature selection, there are many possible factors that affect the time EMS took on the road. In addition, there are many dynamic factors that could possibly affect the time as well, and as of current time, there is no way to record every single one of them. What we can do right now is to record those data that might affect the traffic flow, and applied those as features.\n\n# Redefine the feature selection without the specified columns\nrevised_features = [\n    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', \n    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'\n]\n\n\nselected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')\n\n# Define the target variable\ny_revised = valid_data['EMS_MORE_THAN_10_MIN']\n\n# Split the revised data into training and testing sets\nX_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(\n    selected_features_revised, y_revised, test_size=0.2, random_state=0)\n\nmodel_revised = GaussianNB()\n\n# Train the revised model\nmodel_revised.fit(X_train_revised, y_train_revised)\n\n# Predict on revised test data\ny_pred_revised = model_revised.predict(X_test_revised)\n\ncm_revised = confusion_matrix(y_test_revised, y_pred_revised)\n\naccuracy_revised = accuracy_score(y_test_revised, y_pred_revised)\nreport_revised = classification_report(y_test_revised, y_pred_revised)\n\nprint(accuracy_revised)\nprint(report_revised)\n\n# Plotting the confusion matrix for the revised model\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_revised, annot=True, fmt='d', cmap='Blues', xticklabels=['&lt;=10 min', '&gt;10 min'], yticklabels=['&lt;=10 min', '&gt;10 min'])\nplt.title('Confusion Matrix for Revised EMS Arrival Time Prediction')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\n0.47060544018718925\n              precision    recall  f1-score   support\n\n           0       0.79      0.31      0.45      2354\n           1       0.35      0.82      0.49      1065\n\n    accuracy                           0.47      3419\n   macro avg       0.57      0.56      0.47      3419\nweighted avg       0.65      0.47      0.46      3419\n\n\n\nText(72.72222222222221, 0.5, 'True Label')\n\n\n\n\n\nSo Based on the result, we can tell that\nPrecision for class 0 (EMS took 10 minutes or less): 79%\nRecall for class 0: 31%\nF1-score for class 0: 45%\nPrecision for class 1 (EMS took more than 10 minutes): 35%\nRecall for class 1: 82%\nF1-score for class 1: 49%\nFrom the confusion matrix, The high number of false positives (802) relative to true negatives (346) indicates that the model is overly pessimistic about the EMS response time, often predicting delays where there are none. The model has a better true positive rate, with 893 correctly predicted delays, but this comes at the cost of a high false-positive rate. The false-negative count (196) is lower than the false positives, which suggests that when the model predicts a quick response, it is somewhat more likely to be correct. However, in emergency response situations, even a small number of false negatives can be critical.\nour model is trying to predict whether an emergency medical service (EMS) will take more than 10 minutes to arrive at the scene of an accident.\nThis accuracy tells us what portion of the total predictions made by the model were correct. Our model has an accuracy of approximately 47.06%, which means that about 47 out of every 100 predictions it makes about EMS arrival times are correct. It’s not very high, so the model is not very reliable in its current state.\nPrecision tells us how often the model is correct when it predicts a certain event. For instance, when our model predicts that the EMS will take more than 10 minutes to arrive, it is correct 35% of the time. Conversely, when it predicts that EMS will take 10 minutes or less, it is correct 79% of the time. High precision for a category means that when the model predicts that category, it’s usually right.\nThe F1-score is 49%, and for predictions of 10 minutes or less, it’s 45%. This suggests that the model is slightly better at predicting longer arrival times than shorter ones, but it still isn’t highly accurate in either case.\nGiven the accuracy and the confusion matrix, we see that the model has an accuracy of approximately 47.06% on the test set, which is not very high and is close to random guessing. This could indicate that the model is underfitting. It is too simplistic and not capturing the underlying patterns in the data well enough to make accurate predictions on either the training or the test set.\n\n\n# Load the dataset\npdf_path = ('Data/FARS2021NationalCSV/accident.csv')\ndata = pd.read_csv(pdf_path, encoding='ISO-8859-1')\n\n# Preprocess the data as before\ndata['NOT_MINUTES'] = data['NOT_HOUR'] * 60 + data['NOT_MIN']\ndata['ARR_MINUTES'] = data['ARR_HOUR'] * 60 + data['ARR_MIN']\ndata['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']\ndata['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x &lt; 0 else 0))\ndata['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] &gt; 10).astype(int)\n\n# Remove invalid records (where NOT_HOUR or ARR_HOUR is 99)\nvalid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]\n\n# Redefine the feature selection without the specified columns\nrevised_features = [\n    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', \n    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'\n]\n\n# Prepare the feature matrix and target vector\nselected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')\ny_revised = valid_data['EMS_MORE_THAN_10_MIN']\n\n# Encode categorical variables\nlabel_encoders = {}\nfor column in selected_features_revised.select_dtypes(include=['object']).columns:\n    label_encoders[column] = LabelEncoder()\n    selected_features_revised[column] = label_encoders[column].fit_transform(selected_features_revised[column])\n\n# Split the dataset into train and test sets\nX_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(\n    selected_features_revised, y_revised, test_size=0.2, random_state=0)\n\n# Initialize and train the Gaussian Naive Bayes model\nmodel_revised = GaussianNB()\nmodel_revised.fit(X_train_revised, y_train_revised)\n\n# Predict probabilities for the test set\ny_scores_revised = model_revised.predict_proba(X_test_revised)[:, 1]\n\n# Compute precision-recall pairs for different probability thresholds\nprecision_revised, recall_revised, thresholds_revised = precision_recall_curve(y_test_revised, y_scores_revised)\n\n# Plot the Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall_revised, precision_revised, marker='.', label='Revised Naive Bayes')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Revised Model')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nfpr, tpr, roc_thresholds = roc_curve(y_test_revised, y_scores_revised)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "ts.html#naïve-bayes-for-labeled-text-data",
    "href": "ts.html#naïve-bayes-for-labeled-text-data",
    "title": "Grouping by State",
    "section": "Naïve Bayes for labeled text data",
    "text": "Naïve Bayes for labeled text data\n\nimport requests\nimport json\nimport re\nimport os\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=1\nverbose=True\n\nAPI_KEY='2133663c4ec54af8a9839f0c500203de'\nTOPIC1 = 'Motor vehicle crash'\n\nURLpost1 = {'apiKey': API_KEY,\n            'q': '+'+TOPIC1,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nprint(baseURL)\n\nresponse1 = requests.get(baseURL, URLpost1) \n\nresponse1 = response1.json() \n\nprint(json.dumps(response1, indent=2))\n\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\noutput_file_path = os.path.join(\"data\", f'{timestamp}-topic1-newapi-raw-data-driver-profilling.json')\n\nwith open(output_file_path, 'w') as outfile:\n    json.dump(response1, outfile, indent=4)\n\nwe need to preprocess the data to make sure there’s no noise for feature extraction and feature selection.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nfile_path = (\"Data/labeled_articles_sentiment.csv\")\ndata = pd.read_csv(file_path)\n\ndata['text'] = data['title'] + ' ' + data['description']\n\n# Encode the sentiment column to numerical values\nlabel_encoder = LabelEncoder()\ndata['sentiment_encoded'] = label_encoder.fit_transform(data['sentiment'])\n\n# Split the data into features and target\nX = data['text']\ny = data['sentiment_encoded']\n\n# Perform a train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n\n# Fit and transform the vectorizer on the training data and transform the testing data\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Show the shape of the resulting TF-IDF matrices\nX_train_tfidf.shape, X_test_tfidf.shape\n\n((80, 969), (20, 969))\n\n\nThe TF-IDF vectorization has been applied to both the training and testing text data, resulting in a feature space of 969 terms after limiting to a maximum of 5000 features. There are 80 articles in the training set and 20 in the testing set.\nThe next step is to perform feature selection to find the most relevant features for the Naive Bayes model. However, since the number of features is already quite manageable (969 features), and Naive Bayes handles high-dimensional data well, we might not need to reduce the feature space further. Instead, we’ll proceed with these features and train the Naive Bayes model.\nLet’s train a Multinomial Naive Bayes classifier, which is often used for text classification with features representing the frequency of words. After training the model, we’ll use it to make predictions on the test set and then evaluate the model’s performance.​\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Initialize the Multinomial Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the classifier\nnb_classifier.fit(X_train_tfidf, y_train)\n\n# Predict the labels for the test set\ny_pred = nb_classifier.predict(X_test_tfidf)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\n# Create a confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\naccuracy, report\n\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n(0.5,\n '              precision    recall  f1-score   support\\n\\n    negative       1.00      0.20      0.33         5\\n     neutral       0.00      0.00      0.00         6\\n    positive       0.47      1.00      0.64         9\\n\\n    accuracy                           0.50        20\\n   macro avg       0.49      0.40      0.33        20\\nweighted avg       0.46      0.50      0.37        20\\n')\n\n\nThe Multinomial Naive Bayes classifier has an overall accuracy of 50% on the test set. The classification report and confusion matrix give us a more detailed insight into the performance for each sentiment class:\nNegative Sentiment: The model has a high precision of 100% but a very low recall of 20%, indicating that while the predictions made as negative are all correct, the model fails to identify most of the negative instances. Neutral Sentiment: The model fails to correctly identify any neutral sentiments, as indicated by both precision and recall being 0%. Positive Sentiment: The model has a precision of 47% with a recall of 100%, suggesting that while it identifies all positive instances, it also incorrectly labels some non-positive instances as positive. The confusion matrix visualization shows the distribution of predictions across the actual sentiments. We can see that all negative and neutral sentiments are predominantly classified as positive, which is a sign of bias towards the positive class in the model’s predictions.\nEvaluation Metrics The accuracy metric alone is not sufficient to assess the performance of the Naive Bayes classifier. Precision, recall, and F1-score provide a more comprehensive evaluation. The precision tells us the accuracy of the positive predictions made, recall gives us a measure of the model’s ability to find all the positive instances, and the F1-score is a harmonic mean of precision and recall.\nOverfitting and Underfitting The model does not appear to be overfitting, as overfitting would typically present as a high accuracy on the training set but poor performance on the test set. However, the model might be underfitting since it is overly generalized, leading to poor performance across all metrics.\nModel Performance The model’s performance is not ideal, with a low F1-score for negative and neutral classes and a moderate F1-score for the positive class. This suggests that while the model can predict positive sentiments relatively well, it struggles to distinguish between negative and neutral sentiments.\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import average_precision_score\n\n# Binarize the output labels for the multi-class case\ny_test_binarized = label_binarize(y_test, classes=range(len(label_encoder.classes_)))\n\n# Initialize a dictionary to hold the precision-recall curves for each class\nprecision_recall_curve_dict = {}\n\n# Calculate the precision-recall curve and average precision for each class\nfor i, class_label in enumerate(label_encoder.classes_):\n    class_precisions, class_recalls, class_thresholds = precision_recall_curve(y_test_binarized[:, i], y_scores[:, i])\n    precision_recall_curve_dict[class_label] = (class_precisions, class_recalls)\n    avg_precision = average_precision_score(y_test_binarized[:, i], y_scores[:, i])\n    print(f\"Average precision-recall score for class '{class_label}': {avg_precision:.2f}\")\n\n# Plot the precision-recall curve for each class\nplt.figure(figsize=(10, 6))\n\nfor class_label, (class_precisions, class_recalls) in precision_recall_curve_dict.items():\n    plt.plot(class_recalls, class_precisions, lw=2, label=f'Precision-Recall curve of class {class_label}')\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall curve per class\")\nplt.legend(loc=\"best\")\nplt.show()\n\nAverage precision-recall score for class 'negative': 0.56\nAverage precision-recall score for class 'neutral': 0.45\nAverage precision-recall score for class 'positive': 0.48\n\n\n\n\n\n\n# Let's check the actual counts for each sentiment in y_train and y_pred\ntrain_counts = pd.Series(y_train).value_counts().sort_index()\npred_counts = pd.Series(y_pred).value_counts().sort_index()\n\n# Now we plot the actual counts to verify the distribution\nfig, ax = plt.subplots(1, 2, figsize=(14, 7), sharey=True)\n\nsns.barplot(x=train_counts.index, y=train_counts.values, ax=ax[0], palette=\"viridis\")\nax[0].set_title('Distribution of Actual Sentiments (Training Set)')\nax[0].set_xlabel('Sentiment')\nax[0].set_ylabel('Count')\nax[0].set_xticklabels(label_encoder.inverse_transform(train_counts.index))\n\nsns.barplot(x=pred_counts.index, y=pred_counts.values, ax=ax[1], palette=\"viridis\")\nax[1].set_title('Distribution of Predicted Sentiments (Test Set)')\nax[1].set_xlabel('Sentiment')\nax[1].set_xticklabels(label_encoder.inverse_transform(pred_counts.index))\n\nplt.tight_layout()\nplt.show()\n\n# Show actual counts for verification\ntrain_counts, pred_counts\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3963520661.py:8: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=train_counts.index, y=train_counts.values, ax=ax[0], palette=\"viridis\")\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3963520661.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[0].set_xticklabels(label_encoder.inverse_transform(train_counts.index))\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3963520661.py:14: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=pred_counts.index, y=pred_counts.values, ax=ax[1], palette=\"viridis\")\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3963520661.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[1].set_xticklabels(label_encoder.inverse_transform(pred_counts.index))\n\n\n\n\n\n(sentiment_encoded\n 0    20\n 1    12\n 2    48\n Name: count, dtype: int64,\n 0     1\n 2    19\n Name: count, dtype: int64)\n\n\nThe visualizations confirm the earlier discussions about the model’s performance and suggest potential avenues for improvement, such as addressing the class imbalance or exploring more sophisticated models and features.​"
  },
  {
    "objectID": "PCA python code.html",
    "href": "PCA python code.html",
    "title": "Rough Clean Data",
    "section": "",
    "text": "import pandas as pd\n\naccident_df = pd.read_csv('../../FARS2020NationalCSV/accident.csv', encoding = 'ISO-8859-1')\nperson_df = pd.read_csv('../../FARS2020NationalCSV/person.csv', encoding = 'ISO-8859-1')\n\nmerge_df = pd.merge(accident_df, person_df, on = \"ST_CASE\", how = 'inner')\n\n\ncolumns_to_keep = [\n    # Driver-Specific Information\n    'AGE', 'SEX', 'DRINKING', 'DRUGS', 'ALC_DET', 'ALC_STATUS', \n    'DRUG_DET', 'DSTATUS', 'INJ_SEV', 'PER_TYP', 'DAY_WEEK', 'HOUR_x', 'WEATHER', 'LGT_COND','STATE_x'\n]\n\n# Keeping only the selected columns and dropping the rest\nmerge_df = merge_df[columns_to_keep]\n\n\noutput_file_path = 'Data/PCA_data.csv'\nmerge_df.to_csv(output_file_path)\noutput_file_path\n\n'Data/PCA_data.csv'\n\n\n\nPCA\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\ndata = merge_df\nscaler = StandardScaler()\nscaled_pca_data = scaler.fit_transform(data)\n\n# Re-applying PCA\npca = PCA()\npca_transformed_data = pca.fit_transform(scaled_pca_data)\n\n# Explained variance ratio for each principal component\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Cumulative explained variance\ncumulative_explained_variance = np.cumsum(explained_variance_ratio)\n\n# Displaying the first few principal components' explained variance\nexplained_variance_ratio, cumulative_explained_variance\nplt.plot(cumulative_explained_variance, marker='o')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance by PCA Components')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\npc1 = pca_transformed_data[:, 0]  \npc2 = pca_transformed_data[:, 1]  \n\n\nplt.figure(figsize=(12, 10))\nplt.scatter(pc1, pc2, alpha=0.5)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA - First Two Principal Components with Two Colors')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nThere is no clear clustering of points, which suggests that there may not be distinct groups within the data based on these two principal components alone.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef biplot(score, coeff, labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0/(xs.max() - xs.min())\n    scaley = 1.0/(ys.max() - ys.min())\n    \n    plt.scatter(xs * scalex, ys * scaley, s=5)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color='r', alpha=0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color='black', ha='center', va='center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color='black', ha='center', va='center')\n\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n\n# PCA scores (the transformed data points)\npca_scores = pca_transformed_data\n\n# PCA loadings (the contributions of the original variables to the components)\npca_loadings = pca.components_\n\nplt.figure(figsize=(12,12))\nbiplot(pca_scores, pca_loadings.transpose(), labels=data.columns)\nplt.show()\n\n\n\n\nVariables like Drinking, drugs, and drug detection, are pointing in a similar way, this suggests that they are positively correlated with each other and contribute similarly to these components. Based on their direction, these factors have more influence on PC 1 than it has on PC 2.\nSex and age are similar to drugs and drinking but with less influence than those components.\nALC_STATUS has a strong loading on PC2 and some level of loading on PC1, indicating that this variable significantly differentiates the data along PC 2. Injury severity has a notable contribution to both PC1 and PC2, but more on PC2. This might be interpreted as injury severity varying along with alcohol status but also related to the other factors that align with PC1.\nThe variables associated with substance use (DRINKING, DRUGS, DRUG_DET, ALC_STATUS) are significant in explaining the variance in the data. This suggests that substance use is a major factor in the nature and severity of crashes. So, no drugs, no drinking if you are going to drive.\nThe positioning of INJ_SEV could indicate that crashes involving substance use or certain demographics (SEX) may result in different injury severities. Other variables like WEATHER, LGT_COND (light conditions), and HOUR have smaller angles with each other, suggesting they are somewhat correlated and may jointly influence the conditions under which accidents occur.\n\n\nt-SNE\n\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nsubset_data = data[['AGE', 'SEX', 'DRINKING', 'DRUGS']]\n\nimputer = SimpleImputer(strategy='median')\ndf_imputed = imputer.fit_transform(subset_data)\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_imputed)\nperplexities = [5, 20, 40]\n\nplt.figure(figsize=(12,12))  # Set the figure size for the subplot layout\n\nfor i, perp in enumerate(perplexities, 1):\n    tsne = TSNE(n_components=2, perplexity=perp, n_iter=300, random_state=42)\n    tsne_results = tsne.fit_transform(df_scaled)\n    \n    plt.subplot(1, len(perplexities), i)\n    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5, c= subset_data['SEX'], cmap='viridis')\n    plt.title(f't-SNE with Perplexity = {perp}')\n    plt.xlabel('t-SNE Component 1')\n    plt.ylabel('t-SNE Component 2')\n    plt.colorbar(scatter, label='SEX')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCompared to PCA, t-SNE us better at capturing the local structure of the data and can reveal clusters that might not be visible in PCA. PCA shows the overall structure and explains the variance in the data using fewer components, but might not reveal clusters as effectively as t-SNE.\n\n\nCompare\nWhen working with these two different methods, I can tell that the PCA is much faster when it comes to generating plots in a global data structure scale.\nt-SNE, on the other hand, is better at identifying clusters and local pattersn in a high-dimensional data. And for my dataset, the visualization is very sensitive to parameter setting."
  }
]