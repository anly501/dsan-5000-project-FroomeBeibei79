[
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data Section",
    "section": "",
    "text": "This is a link to data in GitHub Click here"
  },
  {
    "objectID": "Data.html#driver-side",
    "href": "Data.html#driver-side",
    "title": "Data Section",
    "section": "Driver Side",
    "text": "Driver Side\n\nGender\nGender is a significant factor that has an impact on driver behavior. When it comes to fatal injury odds in passenger vehicle accidents, “the fatal injury odds for females were lower than males.” (p21)\n\n\nYoung driver\nAnother publication focus on young driver affect driver behavior analysis, as statistics state that “The rate of drivers involved in fatal traffic crashes per 100,000 licensed drivers for young female drivers was 25.51 in 2021. For young male drivers in 2021 the involvement rate was 60.28, more than twice that of young female drivers.” (U.S. Department of Transportation, National Highway Traffic Safety Administration. (2022). Traffic Safety Facts 2021 Data (Report No. 813492))"
  },
  {
    "objectID": "Data Cleaning.html",
    "href": "Data Cleaning.html",
    "title": "Data Cleaning Code",
    "section": "",
    "text": "This is code section\n\nR Data\nThis will be a link to another page where you can see the code of how to clean the dataset in R\n\n\nPython Data\nThis will be a link to another page where you can see the code of how to clean the dataset in python"
  },
  {
    "objectID": "Data Cleaning R.html",
    "href": "Data Cleaning R.html",
    "title": "Xingrui's Project",
    "section": "",
    "text": "Below is the code to clean the gender dataset\n\nlibrary(readxl)\nlibrary(tidyverse)\ngender &lt;- read_excel(\"data/dl220.xls\")\ngender &lt;- gender[-(1:2),]\ncolnames(gender) &lt;- gender[1,]\ngender &lt;- gender[-1,]\ngender &lt;- gender[1:(nrow(gender) -4),]\ncolnames(gender)[1] &lt;- \"Year\"\ngender &lt;- gender[-1,]\ncolnames1 &lt;- paste0(colnames(gender), unlist(gender[1,]))\ncolnames(gender) &lt;- colnames1\ngender &lt;- gender[-1,]\n\nnames &lt;- which(duplicated(colnames(gender)))\nfor (col_index in names){\n  colnames(gender)[col_index] &lt;- paste0(colnames(gender)[col_index], \"_\", col_index)\n}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#it-will-show-some-detailed-information-regrading-different-manufactures",
    "href": "about.html#it-will-show-some-detailed-information-regrading-different-manufactures",
    "title": "About",
    "section": "it will show some detailed information regrading different manufactures",
    "text": "it will show some detailed information regrading different manufactures\n\nMore will coming soon…\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some sample data\nx = np.linspace(0, 2 * np.pi, 100)  # Create an array of values from 0 to 2*pi\ny = np.sin(x)  # Compute the sine of each value\n\n# Create a plot\nplt.figure(figsize=(8, 4))  # Set the figure size\nplt.plot(x, y, label='sin(x)', color='blue', linestyle='-', linewidth=2)  # Create the plot\n\n# Add labels and a legend\nplt.xlabel('x')  # X-axis label\nplt.ylabel('y')  # Y-axis label\nplt.title('Sine Function')  # Title of the plot\nplt.legend()\n\n# Show the plot\nplt.grid(True)  # Add grid lines\nplt.show()"
  },
  {
    "objectID": "NB.html",
    "href": "NB.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a probabilistic machine learning algorithm based on Bayes’ theorem, which is used for classification tasks. In its essence, it involves using probability to make predictions.\n\n\nNaive Bayes relies on Bayes’ theorem, which describes the probability of an event based on prior knowledge of related conditions. In the context of classification, it’s used to find the probability that a given instance belongs to a particular category based on its features.\n\n\n\nIt calculates the probability of each class given the input features, and the class with the highest probability is considered as the output.\nAssumption of Independence: The “Naive” part of Naive Bayes comes from the assumption that all the features are independent of each other given the class label. While this is a strong and often unrealistic assumption, in practice, Naive Bayes classifiers perform surprisingly well even when this condition is not met.\n\n\n\nThe main objective of Naive Bayes classification is to quickly and efficiently categorize new instances into predefined classes based on the statistical properties of the features of the training data.\n\n\n\n\nDue to its simplicity and the fact that it doesn’t require complex iterative optimization, it’s very fast and efficient, especially for high-dimensional datasets.\n\n\n\nIt can handle missing values by ignoring the missing features during computation.\n\n\n\nIt provides a good baseline model for classification tasks, offering a point of comparison for more complex algorithms.\n\n\n\n\n\n\nAssumes that the features follow a normal distribution. It’s useful when dealing with continuous data.\n\n\n\nUseful for discrete counts, such as word counts in text classification.\n\n\n\n\nSimilar to Multinomial Naive Bayes, but it’s specifically designed for binary/boolean features.\n\n\n\n\n\nUse when your features are continuous and you can assume a Gaussian distribution.\n\n\n\nIdeal for text classification problems where you have counts of word occurrences.\n\n\n\nUse when you’re dealing with binary or boolean features."
  },
  {
    "objectID": "NB.html#bayes-theorem-foundation",
    "href": "NB.html#bayes-theorem-foundation",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes relies on Bayes’ theorem, which describes the probability of an event based on prior knowledge of related conditions. In the context of classification, it’s used to find the probability that a given instance belongs to a particular category based on its features."
  },
  {
    "objectID": "NB.html#probabilistic-nature",
    "href": "NB.html#probabilistic-nature",
    "title": "Naïve Bayes",
    "section": "",
    "text": "It calculates the probability of each class given the input features, and the class with the highest probability is considered as the output.\nAssumption of Independence: The “Naive” part of Naive Bayes comes from the assumption that all the features are independent of each other given the class label. While this is a strong and often unrealistic assumption, in practice, Naive Bayes classifiers perform surprisingly well even when this condition is not met."
  },
  {
    "objectID": "NB.html#objectives",
    "href": "NB.html#objectives",
    "title": "Naïve Bayes",
    "section": "",
    "text": "The main objective of Naive Bayes classification is to quickly and efficiently categorize new instances into predefined classes based on the statistical properties of the features of the training data.\n\n\n\n\nDue to its simplicity and the fact that it doesn’t require complex iterative optimization, it’s very fast and efficient, especially for high-dimensional datasets.\n\n\n\nIt can handle missing values by ignoring the missing features during computation.\n\n\n\nIt provides a good baseline model for classification tasks, offering a point of comparison for more complex algorithms.\n\n\n\n\n\n\nAssumes that the features follow a normal distribution. It’s useful when dealing with continuous data.\n\n\n\nUseful for discrete counts, such as word counts in text classification.\n\n\n\n\nSimilar to Multinomial Naive Bayes, but it’s specifically designed for binary/boolean features.\n\n\n\n\n\nUse when your features are continuous and you can assume a Gaussian distribution.\n\n\n\nIdeal for text classification problems where you have counts of word occurrences.\n\n\n\nUse when you’re dealing with binary or boolean features."
  },
  {
    "objectID": "NB.html#record-data-of-naïve-bayes",
    "href": "NB.html#record-data-of-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "Record Data of Naïve Bayes",
    "text": "Record Data of Naïve Bayes\nFor the record data, we will focus on fatalties of accidents. We choose to predict the time for EMS took on the road. The time gap vary based on various conditions, we will use this time difference as our target variable for the Naive Bayes classification.\nHere in this dataset, there’s no specifc column that record the time EMS took on road, but there are columns that record when the EMS received the notification and the time EMS arrived at the scene. We will use this as the time gap, also our target variable.\n\n\n\ntime gap\n\n\nFor feature selection, there are many possible factors that affect the time EMS took on the road. In addition, there are many dynamic factors that could possibly affect the time as well, and as of current time, there is no way to record every single one of them. What we can do right now is to record those data that might affect the traffic flow, and applied those as features.\nThe result is shown below: \nThe confusion matrix is shown below: \n\nThe result\nPrecision for class 0 (EMS took 10 minutes or less): 79%\nRecall for class 0: 31%\nF1-score for class 0: 45%\nPrecision for class 1 (EMS took more than 10 minutes): 35%\nRecall for class 1: 82%\nF1-score for class 1: 49%\nFrom the confusion matrix, The high number of false positives (802) relative to true negatives (346) indicates that the model is overly pessimistic about the EMS response time, often predicting delays where there are none. The model has a better true positive rate, with 893 correctly predicted delays, but this comes at the cost of a high false-positive rate. The false-negative count (196) is lower than the false positives, which suggests that when the model predicts a quick response, it is somewhat more likely to be correct. However, in emergency response situations, even a small number of false negatives can be critical.\nour model is trying to predict whether an emergency medical service (EMS) will take more than 10 minutes to arrive at the scene of an accident.\nThis accuracy tells us what portion of the total predictions made by the model were correct. Our model has an accuracy of approximately 47.06%, which means that about 47 out of every 100 predictions it makes about EMS arrival times are correct. It’s not very high, so the model is not very reliable in its current state.\nPrecision tells us how often the model is correct when it predicts a certain event. For instance, when our model predicts that the EMS will take more than 10 minutes to arrive, it is correct 35% of the time. Conversely, when it predicts that EMS will take 10 minutes or less, it is correct 79% of the time. High precision for a category means that when the model predicts that category, it’s usually right.\nThe F1-score is 49%, and for predictions of 10 minutes or less, it’s 45%. This suggests that the model is slightly better at predicting longer arrival times than shorter ones, but it still isn’t highly accurate in either case.\nGiven the accuracy and the confusion matrix, we see that the model has an accuracy of approximately 47.06% on the test set, which is not very high and is close to random guessing. This could indicate that the model is underfitting. It is too simplistic and not capturing the underlying patterns in the data well enough to make accurate predictions on either the training or the test set.\nA PRC Curve for evaluating the performance of a classifier \nA ROC Curve for evaluating the performance of classification models"
  },
  {
    "objectID": "NB.html#text-data-of-naïve-bayes",
    "href": "NB.html#text-data-of-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "Text Data of Naïve Bayes",
    "text": "Text Data of Naïve Bayes\nThe text data was extracted using News API, and I also precleaned to make sure it does not have noise for future use \n\nThe result\nThe confusion matrix for text is shown below: \nThe Multinomial Naive Bayes classifier has an overall accuracy of 50% on the test set. The classification report and confusion matrix give us a more detailed insight into the performance for each sentiment class:\n\nNegative Sentiment: The model has a high precision of 100% but a very low recall of 20%, indicating that while the predictions made as negative are all correct, the model fails to identify most of the negative instances\nNeutral Sentiment: The model fails to correctly identify any neutral sentiments, as indicated by both precision and recall being 0%.\nPositive Sentiment: The model has a precision of 47% with a recall of 100%, suggesting that while it identifies all positive instances, it also incorrectly labels some non-positive instances as positive.\n\nThe confusion matrix visualization shows the distribution of predictions across the actual sentiments. We can see that all negative and neutral sentiments are predominantly classified as positive, which is a sign of bias towards the positive class in the model’s predictions.\n\nEvaluation Metrics\nThe accuracy metric alone is not sufficient to assess the performance of the Naive Bayes classifier. Precision, recall, and F1-score provide a more comprehensive evaluation. The precision tells us the accuracy of the positive predictions made, recall gives us a measure of the model’s ability to find all the positive instances, and the F1-score is a harmonic mean of precision and recall.\n\n\nOverfitting and Underfitting\nThe model does not appear to be overfitting, as overfitting would typically present as a high accuracy on the training set but poor performance on the test set. However, the model might be underfitting since it is overly generalized, leading to poor performance across all metrics.\n\n\nModel Performance\nThe model’s performance is not ideal, with a low F1-score for negative and neutral classes and a moderate F1-score for the positive class. This suggests that while the model can predict positive sentiments relatively well, it struggles to distinguish between negative and neutral sentiments.\n\n\n\nprc text\n\n\n\n\n\nresult text\n\n\nThe visualizations confirm the earlier discussions about the model’s performance and suggest potential avenues for improvement, such as addressing the class imbalance or exploring more sophisticated models and features.​"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Xingrui Huo, with GU net ID: xh231\nThis is a webpage that introduces me.\n\n\n\n\nGeorgetown University, Master of Science, Data Science & Analytics\nGeoorgwetown is an excellent unviersity that sits in Georgetown, Washington, D.C.\n\n\n\nI spent my undergraduate life at Rutgers, the State University of New Jersey. RU Rah Rah!\n\n\n\nBefore my undergrad, I stayed back in China for 18 years, born and raised up in the same place."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "",
    "text": "Georgetown University, Master of Science, Data Science & Analytics\nGeoorgwetown is an excellent unviersity that sits in Georgetown, Washington, D.C.\n\n\n\nI spent my undergraduate life at Rutgers, the State University of New Jersey. RU Rah Rah!\n\n\n\nBefore my undergrad, I stayed back in China for 18 years, born and raised up in the same place."
  },
  {
    "objectID": "index.html#sports",
    "href": "index.html#sports",
    "title": "About Me",
    "section": "Sports",
    "text": "Sports\nI love to bike, and watch Formula 1 during the weekend. Sometimes I also play the simulator on PC just to have fun, and experience the Formula 1 in a more intuitive way"
  },
  {
    "objectID": "index.html#things-in-life",
    "href": "index.html#things-in-life",
    "title": "About Me",
    "section": "Things in Life",
    "text": "Things in Life\nI like cars, A LOT. I love everything from the design of the car, the engine, transmission of the car, to the difference on configration of the car due to the different law and environmental requirments in different countries. I also have a very strong interest on analyzing anything data-related to the car. I have done a project before, which focuses on analyzing lap time driven by myself in game F1 22. Now I have switched tracks to collect data from real-world, and to see if the data can inspire me in some ways!"
  },
  {
    "objectID": "index.html#foods",
    "href": "index.html#foods",
    "title": "About Me",
    "section": "Foods!",
    "text": "Foods!\nI love trying new foods in every places around the world. I also very much engaged with trying to re-make the same food at home. Ramen, steak, burrito, everything! Below are something I have tried to cook at home, and it was very good at the end!\n\n\n\nRamen\nDumpling\nZongzi\n\n\n\nPizza\nBarbecue\nSteak"
  },
  {
    "objectID": "index.html#this-is-a-hilarious-quote-from-jeremy-clarkson",
    "href": "index.html#this-is-a-hilarious-quote-from-jeremy-clarkson",
    "title": "About Me",
    "section": "This is a hilarious quote from Jeremy Clarkson",
    "text": "This is a hilarious quote from Jeremy Clarkson\n\nSpeed has never killed anyone, suddenly becoming stationary… That’s what gets you – Jeremy Clarkson\n\n\nSome Requirements\nThis is an inline math equation:  \\(x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\).\nThe area of a circle with radius (r) is given by the formula:\n\\[\n[A = \\pi r^2]\n\\]\nWhere:\n\n(A) is the area of the circle.\n(pi) is approximately 3.14159.\n(r) is the radius of the circle.\n\n\n\n\n\ngraph LR;\n  A[A4, A5] --&gt; B(S4, S5);\n  B --&gt; C(RS4, RS5);\n\n\n\n\n\n\nThis approach enables you to use the same bibliography with different citation styles without having to change anything about your document or the bibliography itself apart from the bibliography style when your paper is finally compiled for print.\n[@fenn_managing_2006]"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "About Me",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne same engine have slightly different models based on differnt vehicles, country law’s requirments, and marketing needs↩︎"
  },
  {
    "objectID": "Data Exploration.html",
    "href": "Data Exploration.html",
    "title": "Explore the Data",
    "section": "",
    "text": "In this page we will explore the data in various kinds of ways to see what might catch out eyes\n\n\nThe data I have here is the accident summary conducted by the Fatality Analysis Reporting System (FARS); FARS is a nationwide census providing NHTSA, Congress and the American public yearly data regarding fatal injuries suffered in motor vehicle traffic crashes. This data contains very detailed information that records the condition of each accident.\nThe data was separated into different files, which can be joined together by a universal column, case number. So we can combine different files to get different information. \n\n\n\nSome basic summary statistics are mean, standard deviation, minimum, median, and variance. Among all these descriptive statistics, the only meaningful column is time, specifically, hour. That is when the accident happened during the day. \n\n\n\nThere are a couple of visulaized plots help us to see intuitivalely\n\n\nBelow the plot provides an overview of accident summaries distributed by state, allowing us to identify the states with the highest and lowest accident counts. \nFrom the chart, we can observe that the distribution is not uniform across states. Some states have a significantly higher number of entries compared to others. This could be due to various factors such as population, geographical size, traffic volume, or data collection methods.\n\n\n\nThis plot reveals the days of the week that are most likely to experience fatal crashes \nThe distribution of entries is relatively uniform across the days of the week. There seems to be a slight increase in entries on Saturdays and Sundays. This could suggest a potential increase in incidents or events during the weekends, but further analysis would be needed to confirm any such trends and to understand their causes.\n\n\n\nThis plot illustrates the direction in which a vehicle is most likely to experience an accident \nFrom this chart, we can observe that: “Non-Harmful Event” and “12 Clock Point” are the most common areas of impact recorded in the dataset. There are various other areas of impact with fewer occurrences. This information could be useful for understanding the common types of impacts and their frequencies, which might help in identifying areas for safety improvements.\n\n\n\n\nIn the Correlation Analysis step, we will:\n\nCalculate the correlation matrix for the numerical variables in the dataset.\nVisualize the correlations using a heatmap.\n\n\n\nLet’s start by calculating the correlation matrix. \nAs we can see from the matrix, STATE_x and ST_CASE have a very high positive correlation (almost 1), which is expected as the case number (ST_CASE) is likely assigned sequentially within each state.\nHOUR and MINUTE have a positive correlation of 0.2644, indicating a moderate relationship.\nEVENTNUM and VNUMBER1 also have a strong positive correlation of 0.8227, suggesting a strong relationship between these two variables.\nAOI1 and SOE have a positive correlation of 0.5630, indicating a moderate to strong relationship.\n\n\n\nNext, we will visualize these correlations using a heatmap to make it easier to interpret the relationships between variables. \nThe heatmap above visualizes the correlation matrix, providing a color-coded representation of the correlation coefficients between each pair of numerical variables.\nFrom the heatmap, we can easily identify strong positive correlations (dark red areas) and observe the relationships between different variables. For example, the strong positive correlation between “EVENTNUM” and “VNUMBER1” is clearly visible.\n\n\n\nTo investigate the relationship between the day of the week and the hour of the day, we can create a heatmap that displays the count of entries for each combination of day and hour. This will help us visualize if there are certain times of the day that are more prone to incidents on specific weekdays \nFrom this visualization, we can observe that:\nThe hours with the most entries tend to be in the afternoon and evening, particularly from around 14:00 to 18:00. This could be due to increased traffic during these hours.\nThere are fewer entries during the early morning hours, which is expected since there is typically less traffic at that time.\nSaturday and Sunday show a slightly different pattern compared to the weekdays, with more entries occurring late at night and in the early morning hours. This could be indicative of different driving behaviors during the weekends, potentially related to social activities.\nThe column labeled “Unknown” represents entries where the hour was not recorded or was unknown. These entries are spread across all days of the week.\n\n\n\n\nBased on the exploratory data analysis we’ve conducted so far, here are some potential hypotheses and questions:\n\n\nHypothesis: There are more incidents in the afternoon and evening compared to other times of the day. Potential Analysis: Investigate if certain types of incidents are more likely to occur during these times.\n\n\n\nHypothesis: Driving behavior during the weekends, especially late at night and in the early morning hours, leads to more incidents.\nPotential Analysis: Examine the types of incidents that occur during these times and if they are different from weekday incidents.\n\n\n\nQuestion: Why do some states have significantly more incidents recorded in the dataset? Is it due to population, traffic volume, or data collection methods?\nPotential Analysis: Normalize the data by population or traffic volume to better understand the state-wise distribution.\n\n\n\nHypothesis: Certain areas of impact, such as the “Non-Harmful Event” and “12 Clock Point”, are more common.\nPotential Analysis: Investigate the circumstances that lead to these common impact areas.\n\n\n\nHypothesis: The “Motor Vehicle In-Transport” event is the most common sequence of events leading to incidents.\nPotential Analysis: Explore what specific situations or factors contribute to this sequence of events.\n\n\n\n\nIf applicable, group or segment the data based on relevant criteria to uncover insights within specific subgroups.\n\n\n The data grouped by state shows the total number of entries (incidents) for each state:\n\nCalifornia and Texas have the highest number of entries, with 11,952 and 11,787 incidents respectively.\nStates like Alaska, District of Columbia, and Rhode Island have the lowest number of entries, all below 150 incidents.\nThis distribution could be influenced by various factors such as population, geographical size, traffic volume, and data collection methods.\n\n\n\n\n From this distribution, we can observe that:\n\nThe number of incidents is higher in the evening, followed by the afternoon and night.\nThe morning has the lowest number of incidents.\nThere are a significant number of incidents during the night, which could be worth investigating further, especially given the reduced traffic volumes during these hours.\n\n\n\n\n\nTo identify outliers, we can use various methods such as:\n\nZ-Scores: Calculate the Z-score of each data point and identify points with a Z-score beyond a certain threshold (e.g., |Z| &gt; 3).\nIQR Method: Calculate the Interquartile Range (IQR) and identify points beyond 1.5 * IQR from the quartiles.\nVisualizations: Use box plots to visually identify outliers.\n\nSince we have multiple numerical variables in our dataset, we will start by using box plots to visually identify outliers in these variables. \nFrom the box plots, we can observe that:\n\nSTATE_x: There are no visible outliers.\nST_CASE: This is a case number, and it doesn’t have outliers in the traditional sense\nDAY: There are no visible outliers.\nHOUR: There are values set to 99, which likely represent unknown or missing values rather than outliers.\nMINUTE: Similar to “HOUR”, there are values set to 99.\nEVENTNUM, VNUMBER1, AOI1, SOE: These variables have a significant number of high values that could be considered outliers. However, without more context on what these numbers represent, it’s challenging to definitively label them as outliers.\n\nTo properly handle the potential outliers, we would need additional context on the data and the variables, especially for the ones with coded values (EVENTNUM, VNUMBER1, AOI1, SOE).\n\n\n\n\n\nThe dataset contains information about various incidents, with a total of 112,725 entries. There are 13 columns, consisting of both numerical and categorical variables.\n\n\n\n\n\n\nSTATE_x: Represents the state codes, ranging from 1 to 56.\nST_CASE: A unique case number assigned to each incident.\nDAY: Day of the incident, ranging from 1 to 31.\nHOUR: Hour of the day when the incident occurred, ranging from 0 to 99 (with 99 representing unknown values).\nMINUTE: Minute of the hour when the incident occurred, ranging from 0 to 99 (with 99 representing unknown values).\nEVENTNUM, VNUMBER1, AOI1, SOE: Coded variables representing various aspects of the incidents.\n\n\n\n\n\nSTATENAME_x: The name of the state where the incident occurred.\nDAY_WEEKNAME: The day of the week when the incident occurred.\nAOI1NAME: Descriptions related to the area of impact.\nSOENAME: Descriptions related to the sequence of events.\n\n\n\n\n\n\n\n\nSTATENAME_x: California and Texas have the highest number of incidents.\nDAY_WEEKNAME: Incidents are fairly evenly distributed across days of the week, with a slight increase on Saturdays and Sundays.\nAOI1NAME: “Non-Harmful Event” and “12 Clock Point” are the most common areas of impact\nSOENAME: “Motor Vehicle In-Transport” is the most common sequence of events.\n\n\n\n\n\nA strong positive correlation exists between STATE_x and ST_CASE, as case numbers are likely assigned sequentially within each state. HOUR and MINUTE have a positive correlation, suggesting a moderate relationship. EVENTNUM and VNUMBER1 also show a strong positive correlation. AOI1 and SOE have a moderate to strong positive correlation.\n\n\n\nHypotheses were generated related to time of day and incidents, weekend driving behavior, state-wise distribution of incidents, impact areas, and sequence of events.\n\n\n\nBy State:\nCalifornia and Texas have the highest number of incidents. Alaska, District of Columbia, and Rhode Island have the lowest.\nBy Time of Day:\nThe evening has the highest number of incidents, followed by the afternoon and night. The morning has the lowest number of incidents.\n\n\n\nPotential outliers were identified in the variables HOUR, MINUTE, EVENTNUM, VNUMBER1, AOI1, and SOE. The values 99 in HOUR and MINUTE are likely placeholders for unknown values.\n\n\n\n\nThe analysis was performed using Python, leveraging libraries such as:\nPandas: For data manipulation and analysis. Matplotlib and Seaborn: For data visualization. NumPy: For numerical computations."
  },
  {
    "objectID": "Data Exploration.html#data-understanding",
    "href": "Data Exploration.html#data-understanding",
    "title": "Explore the Data",
    "section": "",
    "text": "The data I have here is the accident summary conducted by the Fatality Analysis Reporting System (FARS); FARS is a nationwide census providing NHTSA, Congress and the American public yearly data regarding fatal injuries suffered in motor vehicle traffic crashes. This data contains very detailed information that records the condition of each accident.\nThe data was separated into different files, which can be joined together by a universal column, case number. So we can combine different files to get different information."
  },
  {
    "objectID": "Data Exploration.html#descriptive-statistics",
    "href": "Data Exploration.html#descriptive-statistics",
    "title": "Explore the Data",
    "section": "",
    "text": "Some basic summary statistics are mean, standard deviation, minimum, median, and variance. Among all these descriptive statistics, the only meaningful column is time, specifically, hour. That is when the accident happened during the day."
  },
  {
    "objectID": "Data Exploration.html#data-visualization",
    "href": "Data Exploration.html#data-visualization",
    "title": "Explore the Data",
    "section": "",
    "text": "There are a couple of visulaized plots help us to see intuitivalely\n\n\nBelow the plot provides an overview of accident summaries distributed by state, allowing us to identify the states with the highest and lowest accident counts. \nFrom the chart, we can observe that the distribution is not uniform across states. Some states have a significantly higher number of entries compared to others. This could be due to various factors such as population, geographical size, traffic volume, or data collection methods.\n\n\n\nThis plot reveals the days of the week that are most likely to experience fatal crashes \nThe distribution of entries is relatively uniform across the days of the week. There seems to be a slight increase in entries on Saturdays and Sundays. This could suggest a potential increase in incidents or events during the weekends, but further analysis would be needed to confirm any such trends and to understand their causes.\n\n\n\nThis plot illustrates the direction in which a vehicle is most likely to experience an accident \nFrom this chart, we can observe that: “Non-Harmful Event” and “12 Clock Point” are the most common areas of impact recorded in the dataset. There are various other areas of impact with fewer occurrences. This information could be useful for understanding the common types of impacts and their frequencies, which might help in identifying areas for safety improvements."
  },
  {
    "objectID": "Data Exploration.html#correlation-analysis",
    "href": "Data Exploration.html#correlation-analysis",
    "title": "Explore the Data",
    "section": "",
    "text": "In the Correlation Analysis step, we will:\n\nCalculate the correlation matrix for the numerical variables in the dataset.\nVisualize the correlations using a heatmap.\n\n\n\nLet’s start by calculating the correlation matrix. \nAs we can see from the matrix, STATE_x and ST_CASE have a very high positive correlation (almost 1), which is expected as the case number (ST_CASE) is likely assigned sequentially within each state.\nHOUR and MINUTE have a positive correlation of 0.2644, indicating a moderate relationship.\nEVENTNUM and VNUMBER1 also have a strong positive correlation of 0.8227, suggesting a strong relationship between these two variables.\nAOI1 and SOE have a positive correlation of 0.5630, indicating a moderate to strong relationship.\n\n\n\nNext, we will visualize these correlations using a heatmap to make it easier to interpret the relationships between variables. \nThe heatmap above visualizes the correlation matrix, providing a color-coded representation of the correlation coefficients between each pair of numerical variables.\nFrom the heatmap, we can easily identify strong positive correlations (dark red areas) and observe the relationships between different variables. For example, the strong positive correlation between “EVENTNUM” and “VNUMBER1” is clearly visible.\n\n\n\nTo investigate the relationship between the day of the week and the hour of the day, we can create a heatmap that displays the count of entries for each combination of day and hour. This will help us visualize if there are certain times of the day that are more prone to incidents on specific weekdays \nFrom this visualization, we can observe that:\nThe hours with the most entries tend to be in the afternoon and evening, particularly from around 14:00 to 18:00. This could be due to increased traffic during these hours.\nThere are fewer entries during the early morning hours, which is expected since there is typically less traffic at that time.\nSaturday and Sunday show a slightly different pattern compared to the weekdays, with more entries occurring late at night and in the early morning hours. This could be indicative of different driving behaviors during the weekends, potentially related to social activities.\nThe column labeled “Unknown” represents entries where the hour was not recorded or was unknown. These entries are spread across all days of the week."
  },
  {
    "objectID": "Data Exploration.html#hypothesis-generation",
    "href": "Data Exploration.html#hypothesis-generation",
    "title": "Explore the Data",
    "section": "",
    "text": "Based on the exploratory data analysis we’ve conducted so far, here are some potential hypotheses and questions:\n\n\nHypothesis: There are more incidents in the afternoon and evening compared to other times of the day. Potential Analysis: Investigate if certain types of incidents are more likely to occur during these times.\n\n\n\nHypothesis: Driving behavior during the weekends, especially late at night and in the early morning hours, leads to more incidents.\nPotential Analysis: Examine the types of incidents that occur during these times and if they are different from weekday incidents.\n\n\n\nQuestion: Why do some states have significantly more incidents recorded in the dataset? Is it due to population, traffic volume, or data collection methods?\nPotential Analysis: Normalize the data by population or traffic volume to better understand the state-wise distribution.\n\n\n\nHypothesis: Certain areas of impact, such as the “Non-Harmful Event” and “12 Clock Point”, are more common.\nPotential Analysis: Investigate the circumstances that lead to these common impact areas.\n\n\n\nHypothesis: The “Motor Vehicle In-Transport” event is the most common sequence of events leading to incidents.\nPotential Analysis: Explore what specific situations or factors contribute to this sequence of events."
  },
  {
    "objectID": "Data Exploration.html#data-grouping-and-segmentation",
    "href": "Data Exploration.html#data-grouping-and-segmentation",
    "title": "Explore the Data",
    "section": "",
    "text": "If applicable, group or segment the data based on relevant criteria to uncover insights within specific subgroups.\n\n\n The data grouped by state shows the total number of entries (incidents) for each state:\n\nCalifornia and Texas have the highest number of entries, with 11,952 and 11,787 incidents respectively.\nStates like Alaska, District of Columbia, and Rhode Island have the lowest number of entries, all below 150 incidents.\nThis distribution could be influenced by various factors such as population, geographical size, traffic volume, and data collection methods.\n\n\n\n\n From this distribution, we can observe that:\n\nThe number of incidents is higher in the evening, followed by the afternoon and night.\nThe morning has the lowest number of incidents.\nThere are a significant number of incidents during the night, which could be worth investigating further, especially given the reduced traffic volumes during these hours."
  },
  {
    "objectID": "Data Exploration.html#identifying-outliers",
    "href": "Data Exploration.html#identifying-outliers",
    "title": "Explore the Data",
    "section": "",
    "text": "To identify outliers, we can use various methods such as:\n\nZ-Scores: Calculate the Z-score of each data point and identify points with a Z-score beyond a certain threshold (e.g., |Z| &gt; 3).\nIQR Method: Calculate the Interquartile Range (IQR) and identify points beyond 1.5 * IQR from the quartiles.\nVisualizations: Use box plots to visually identify outliers.\n\nSince we have multiple numerical variables in our dataset, we will start by using box plots to visually identify outliers in these variables. \nFrom the box plots, we can observe that:\n\nSTATE_x: There are no visible outliers.\nST_CASE: This is a case number, and it doesn’t have outliers in the traditional sense\nDAY: There are no visible outliers.\nHOUR: There are values set to 99, which likely represent unknown or missing values rather than outliers.\nMINUTE: Similar to “HOUR”, there are values set to 99.\nEVENTNUM, VNUMBER1, AOI1, SOE: These variables have a significant number of high values that could be considered outliers. However, without more context on what these numbers represent, it’s challenging to definitively label them as outliers.\n\nTo properly handle the potential outliers, we would need additional context on the data and the variables, especially for the ones with coded values (EVENTNUM, VNUMBER1, AOI1, SOE)."
  },
  {
    "objectID": "Data Exploration.html#report-and-discuss-your-methods-and-findings",
    "href": "Data Exploration.html#report-and-discuss-your-methods-and-findings",
    "title": "Explore the Data",
    "section": "",
    "text": "The dataset contains information about various incidents, with a total of 112,725 entries. There are 13 columns, consisting of both numerical and categorical variables.\n\n\n\n\n\n\nSTATE_x: Represents the state codes, ranging from 1 to 56.\nST_CASE: A unique case number assigned to each incident.\nDAY: Day of the incident, ranging from 1 to 31.\nHOUR: Hour of the day when the incident occurred, ranging from 0 to 99 (with 99 representing unknown values).\nMINUTE: Minute of the hour when the incident occurred, ranging from 0 to 99 (with 99 representing unknown values).\nEVENTNUM, VNUMBER1, AOI1, SOE: Coded variables representing various aspects of the incidents.\n\n\n\n\n\nSTATENAME_x: The name of the state where the incident occurred.\nDAY_WEEKNAME: The day of the week when the incident occurred.\nAOI1NAME: Descriptions related to the area of impact.\nSOENAME: Descriptions related to the sequence of events.\n\n\n\n\n\n\n\n\nSTATENAME_x: California and Texas have the highest number of incidents.\nDAY_WEEKNAME: Incidents are fairly evenly distributed across days of the week, with a slight increase on Saturdays and Sundays.\nAOI1NAME: “Non-Harmful Event” and “12 Clock Point” are the most common areas of impact\nSOENAME: “Motor Vehicle In-Transport” is the most common sequence of events.\n\n\n\n\n\nA strong positive correlation exists between STATE_x and ST_CASE, as case numbers are likely assigned sequentially within each state. HOUR and MINUTE have a positive correlation, suggesting a moderate relationship. EVENTNUM and VNUMBER1 also show a strong positive correlation. AOI1 and SOE have a moderate to strong positive correlation.\n\n\n\nHypotheses were generated related to time of day and incidents, weekend driving behavior, state-wise distribution of incidents, impact areas, and sequence of events.\n\n\n\nBy State:\nCalifornia and Texas have the highest number of incidents. Alaska, District of Columbia, and Rhode Island have the lowest.\nBy Time of Day:\nThe evening has the highest number of incidents, followed by the afternoon and night. The morning has the lowest number of incidents.\n\n\n\nPotential outliers were identified in the variables HOUR, MINUTE, EVENTNUM, VNUMBER1, AOI1, and SOE. The values 99 in HOUR and MINUTE are likely placeholders for unknown values."
  },
  {
    "objectID": "Data Exploration.html#tools-and-software",
    "href": "Data Exploration.html#tools-and-software",
    "title": "Explore the Data",
    "section": "",
    "text": "The analysis was performed using Python, leveraging libraries such as:\nPandas: For data manipulation and analysis. Matplotlib and Seaborn: For data visualization. NumPy: For numerical computations."
  },
  {
    "objectID": "Code.html",
    "href": "Code.html",
    "title": "Code Section",
    "section": "",
    "text": "Link to my GitHubMy GitHub\nThis is a link to formula 1 Formula 1"
  },
  {
    "objectID": "Code.html#first-is-links-section",
    "href": "Code.html#first-is-links-section",
    "title": "Code Section",
    "section": "",
    "text": "Link to my GitHubMy GitHub\nThis is a link to formula 1 Formula 1"
  },
  {
    "objectID": "clustering cleaning python.html",
    "href": "clustering cleaning python.html",
    "title": "Elbow Method",
    "section": "",
    "text": "import pandas as pd\n\naccident_df = pd.read_csv('../../FARS2020NationalCSV/accident.csv', encoding = 'ISO-8859-1')\nvehicle_df = pd.read_csv('../../FARS2020NationalCSV/vehicle.csv', encoding = 'ISO-8859-1')\n\n\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_1599/3702391140.py:4: DtypeWarning: Columns (56,58,176,178) have mixed types. Specify dtype option on import or set low_memory=False.\n  vehicle_df = pd.read_csv('../../FARS2020NationalCSV/vehicle.csv', encoding = 'ISO-8859-1')\naccident_df = accident_df[['WEATHER', 'ST_CASE','STATE', 'LGT_COND', 'FATALS']]\nvehicle_df = vehicle_df[['ST_CASE','DR_DRINK', 'DR_ZIP', 'HIT_RUN', 'L_STATE', 'MAKENAME', 'MAK_MODNAME', 'MOD_YEAR', 'VSURCOND', 'VPICMAKENAME', 'VPICMODELNAME']]\nmerge_df = pd.merge(accident_df, vehicle_df, on = \"ST_CASE\", how = 'inner')\nmerge_df = merge_df[(merge_df['WEATHER'] &lt;= 90) & (merge_df['VSURCOND'] &lt;= 90) & (merge_df['MOD_YEAR'] &lt;= 3000)]\noutput_file_path = 'Data/clustering_data.csv'\nmerge_df.to_csv(output_file_path)\noutput_file_path\n\n'Data/clustering_data.csv'\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Preparing data for K-means clustering\n# Selecting only numerical features relevant for environmental analysis\nkmeans_data = merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']]\n\n# Using the Elbow Method to find the optimal number of clusters\nsse = []  # Sum of squared distances\nfor k in range(1, 11):  # Testing for 1 to 10 clusters\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(kmeans_data)\n    sse.append(kmeans.inertia_)  # Inertia: Sum of squared distances of samples to their closest cluster center\n\n# Plotting the Elbow Curve\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 11), sse, marker='o')\nplt.title('Elbow Method for Optimal k')\nplt.xlabel('Number of clusters')\nplt.ylabel('Sum of squared distances')\nplt.xticks(range(1, 11))\nplt.show()\n\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nHere we define the number of k is 4"
  },
  {
    "objectID": "clustering cleaning python.html#silhouette",
    "href": "clustering cleaning python.html#silhouette",
    "title": "Elbow Method",
    "section": "Silhouette",
    "text": "Silhouette\n\nfrom sklearn.metrics import silhouette_score\n\n# Assuming 'features' is your data prepared for clustering\nfeatures = merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']]\n\nsilhouette_scores = []\nrange_n_clusters = list(range(2, 10))  # Example range from 2 to 9 clusters\n\nfor n_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(features)\n    silhouette_avg = silhouette_score(features, clusters)\n    silhouette_scores.append(silhouette_avg)\n    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")\n\n# Plotting the silhouette scores\nplt.figure(figsize=(10, 6))\nplt.plot(range_n_clusters, silhouette_scores, marker='o')\nplt.title('Silhouette Method For Optimal k')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Average Silhouette Score')\nplt.xticks(range_n_clusters)\nplt.show()\n\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nFor n_clusters = 2, the average silhouette_score is : 0.8309870559164231\nFor n_clusters = 3, the average silhouette_score is : 0.6233315920118019\nFor n_clusters = 4, the average silhouette_score is : 0.6441922256874982\nFor n_clusters = 5, the average silhouette_score is : 0.6628101906529784\nFor n_clusters = 6, the average silhouette_score is : 0.6846456347051649\nFor n_clusters = 7, the average silhouette_score is : 0.746251051575895\nFor n_clusters = 8, the average silhouette_score is : 0.7242918837192516\nFor n_clusters = 9, the average silhouette_score is : 0.8069782666170565"
  },
  {
    "objectID": "clustering cleaning python.html#results-of-k-means-clustering",
    "href": "clustering cleaning python.html#results-of-k-means-clustering",
    "title": "Elbow Method",
    "section": "Results of K-Means Clustering",
    "text": "Results of K-Means Clustering\nClusters Analysis: The clusters appear to be differentiated mostly along the ‘WEATHER’ and ‘VSURCOND’ axes. This suggests that weather conditions and road surface conditions play significant roles in differentiating between clusters. The histograms on the diagonal show the distribution of each feature within each cluster. For instance, one of the clusters (in red) seems to have a higher frequency of incidents occurring under a specific weather condition(1 and 2, that is clear or rain) whereas the light and road surface conditions are more spread out. This is because these are two most common weather condition happen on the road, whereas others are less likely to happen or in some states it won’t ever happen. Also, we can see from the (0,0) plot that the number of accidents happened during clear and rain are much larger than accidents happened in other weather condition.\nRelationships between Features: In the scatter plots, you can see how the clusters are distributed with respect to two features at a time. For example, in the ‘WEATHER’ versus ‘LGT_COND’ scatter plot, you can see that some clusters are more prevalent under certain combinations of weather and light conditions. It looks like the blue, purple, and orange clusters are more scattered across conditions, indicating light condition might not have a strong impact on chance of fatal crashes, while the red and orange clusters also represent more specific conditions (Rain, Sleet or Hail, Blowing Snow or Freezing Rain, which will cause road surface condition getting worse and the chance of having a fatal crash increases dramatically)."
  },
  {
    "objectID": "clustering cleaning python.html#using-silhouette-methods-number-of-clusters",
    "href": "clustering cleaning python.html#using-silhouette-methods-number-of-clusters",
    "title": "Elbow Method",
    "section": "Using silhouette method’s number of clusters",
    "text": "Using silhouette method’s number of clusters\n\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Applying K-means clustering with 2 clusters generated by silhouette method\nkmeans = KMeans(n_clusters=9)\nclusters = kmeans.fit_predict(features)\n\n# Adding the cluster information to the original dataframe\nmerge_df['Cluster'] = clusters\n\n# Analyzing the clusters\nfor i in range(9):  # Looping through 2 clusters\n    cluster = merge_df[merge_df['Cluster'] == i]\n    print(f\"Cluster {i}:\")\n    print(f\"Weather Conditions: {cluster['WEATHER'].mode()[0]}\")\n    print(f\"Light Conditions: {cluster['LGT_COND'].mode()[0]}\")\n    print(f\"Road Surface Conditions: {cluster['VSURCOND'].mode()[0]}\")\n    print(f\"Average Fatalities: {cluster['FATALS'].mean()}\")\n    print()\n\n# Visualizing the clusters\nsns.pairplot(merge_df, vars=['WEATHER', 'LGT_COND', 'VSURCOND'], hue='Cluster', palette='bright')\nplt.title('K-Means Clustering with 2 Clusters')\nplt.show()\n\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\nCluster 0:\nWeather Conditions: 1\nLight Conditions: 1\nRoad Surface Conditions: 1\nAverage Fatalities: 1.1034950613263865\n\nCluster 1:\nWeather Conditions: 10\nLight Conditions: 1\nRoad Surface Conditions: 1\nAverage Fatalities: 1.1183602771362586\n\nCluster 2:\nWeather Conditions: 1\nLight Conditions: 3\nRoad Surface Conditions: 1\nAverage Fatalities: 1.101240446059391\n\nCluster 3:\nWeather Conditions: 1\nLight Conditions: 1\nRoad Surface Conditions: 11\nAverage Fatalities: 1.1072961373390557\n\nCluster 4:\nWeather Conditions: 2\nLight Conditions: 1\nRoad Surface Conditions: 2\nAverage Fatalities: 1.1211817726589886\n\nCluster 5:\nWeather Conditions: 1\nLight Conditions: 5\nRoad Surface Conditions: 1\nAverage Fatalities: 1.0831615120274913\n\nCluster 6:\nWeather Conditions: 1\nLight Conditions: 2\nRoad Surface Conditions: 1\nAverage Fatalities: 1.1269583381687363\n\nCluster 7:\nWeather Conditions: 5\nLight Conditions: 2\nRoad Surface Conditions: 1\nAverage Fatalities: 1.1491228070175439\n\nCluster 8:\nWeather Conditions: 10\nLight Conditions: 1\nRoad Surface Conditions: 10\nAverage Fatalities: 1.0806451612903225\n\n\n\n\n\n\nThe optimal number of clusters should be 2, but there was not much useful infomration we can get form that, so I used the second highest score, which is 9 clusters.\nBased on the plot and the ouput result, we can infer that 1. The highest risk of fatality seems to be associated with driving in the dark without proper lighting, especially in foggy conditions. 2. Weather that impacts visibility, like fog and heavy clouds, or adverse road conditions, such as blowing snow and non-standard surfaces, appears to increase the risk of fatal accidents. 3. Clusters with daylight, clear weather, and dry roads tend to have lower fatality rates, suggesting these are the safest conditions among the clusters analyzed. 4. Rainy conditions with wet roads also show an increased average in fatalities, indicating the hazardous nature of such conditions."
  },
  {
    "objectID": "clustering cleaning python.html#dbscan",
    "href": "clustering cleaning python.html#dbscan",
    "title": "Elbow Method",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nfeatures_standardized = scaler.fit_transform(features)\n\ndbscan = DBSCAN(eps=0.5, min_samples=500)\nclusters = dbscan.fit_predict(features_standardized)\n\n# Add the cluster labels to your dataframe\nmerge_df['Cluster'] = clusters\n\n# Check the number of points in each cluster\ncluster_counts = merge_df['Cluster'].value_counts()\nprint(cluster_counts)\n\nCluster\n 6     18038\n 0      8572\n 5      7097\n-1      3615\n 8      3046\n 2      1972\n 4      1282\n 7      1190\n 10      990\n 1       919\n 9       854\n 3       589\n 11      558\nName: count, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Reduce the data to two dimensions using PCA for visualization purposes\npca = PCA(n_components=2)\nfeatures_pca = pca.fit_transform(features_standardized)\n\n# Create a scatter plot of the PCA-reduced data color-coded by cluster label\nplt.figure(figsize=(10, 10))\nunique_labels = np.unique(clusters)\n# Generate a color map similar to plt.cm.Spectral but without black\ncolors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Skip noise points, they won't be plotted\n        continue\n\n    class_member_mask = (clusters == k)\n\n    xy = features_pca[class_member_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)\n\nplt.title('DBSCAN clustering (without noise)')\nplt.xlabel('PCA Feature 1')\nplt.ylabel('PCA Feature 2')\nplt.show()\n\n\n\n\nThis is a cluster result plotted without any noise points on. Clearly this plot cannot bring us any useful information, let’s try another way to see what will happen.\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standardize the features before applying DBSCAN\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']])\n\n# Apply DBSCAN to the standardized features\n# Note: You'll need to tune these parameters\ndbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\nclusters = dbscan.fit_predict(features_scaled)\n\n# Add cluster info to the original DataFrame\nmerge_df['Cluster'] = clusters\n\n# Reduce dimensions for visualization\npca = PCA(n_components=2)\nfeatures_pca = pca.fit_transform(features_scaled)\n\n# Visualize the clusters\nunique_labels = set(clusters)\nfor label in unique_labels:\n    plt.scatter(features_pca[clusters == label, 0], features_pca[clusters == label, 1], label=label)\n\nplt.title('DBSCAN Clustering')\nplt.xlabel('PCA Feature 1')\nplt.ylabel('PCA Feature 2')\nplt.show()\n\n\n\n\nThe plot shows a variety of clusters, each represented by a different color. Some clusters appear more densely packed, while others are sparse. The densely packed clusters likely represent more common combinations of weather, light, and road surface conditions that frequently occur together, while sparser ones may represent less common or more unique conditions. This is not an ideal result for us to see, and probablt DBSCAN is not a good model for this dataset."
  },
  {
    "objectID": "clustering cleaning python.html#hierarchical-clustering",
    "href": "clustering cleaning python.html#hierarchical-clustering",
    "title": "Elbow Method",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.decomposition import PCA\nimport sys\n\nsys.setrecursionlimit(10000)\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']])\n\n# Performing hierarchical clustering\nZ = linkage(features_scaled, method='ward')\n\n# Plotting the dendrogram\nplt.figure(figsize=(12, 8))\ndendrogram(Z, truncate_mode='lastp', p=50, show_leaf_counts=True)\n\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Index')\nplt.ylabel('Ward\\'s Distance')\nplt.show()\n\n# To cut the dendrogram and get the cluster labels, use the following:\nfrom scipy.cluster.hierarchy import fcluster\nk = 9  # for example, if we decide on 9 clusters\ncluster_labels = fcluster(Z, k, criterion='maxclust')\n\n# Adding the cluster information to the original dataframe\nmerge_df['Cluster'] = cluster_labels\n\n# Optionally reduce dimensions for visualization\npca = PCA(n_components=2)\nfeatures_pca = pca.fit_transform(features_scaled)\n\n# Visualize the clusters\nplt.figure(figsize=(8, 6))\nplt.scatter(features_pca[:, 0], features_pca[:, 1], c=merge_df['Cluster'], cmap='viridis')\nplt.title('Hierarchical Clustering Results')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\nThe plot shows PCA has been used to reduce the dimensionality of the data to two principal components, which are linear combinations of the original variables. The proximity of points within the same cluster (same color) suggests they are similar to each other based on the original features (weather, light, and road surface conditions).\nSome clusters are more tightly grouped, suggesting that there are well-defined conditions under which a number of accidents commonly occur. Based on the size of count and the size of data, most of the likely combination could be clear weather, daylight, and dry road surface condition, which is the most environmental condition for an accident, also the most common one. Some other clusters that are sparse or have points spread out may indicate less common or more variable conditions. Probably these are some adverse weather conditions like fod or snow with dusk or dawn light condition, with some potential hazardous road surface condition that is rarely happen in real life. All of those needs to be confirmed by further analysis."
  },
  {
    "objectID": "clustering cleaning python.html#conclusion",
    "href": "clustering cleaning python.html#conclusion",
    "title": "Elbow Method",
    "section": "Conclusion",
    "text": "Conclusion\nBased on those plots generated by differnt methods, I will say that K-Means is the best method for this dataset. It has the most intuitive result, and bring the most information. DBSCAN, on the other hand, is not as competitive as K-means as it seperate the dataset into too many small clusters, which is likely to capture noise and random fluctuations, also, in some cases, Each data point might become its own cluster, which defeats the purpose of clustering to find broader patterns.\nHierarchical Clustering is also a good method for this dataset as it generated just fine number of clusters and the clusters make sense logically and intuitively based on the domain knowledge and also make sense in the real world\nIn conclusion, our analysis of vehicle accident data through various clustering methods has provided us with meaningful insights into the conditions under which accidents are most likely to occur. By examining the environmental factors—specifically, weather conditions, lighting, and road surface states—we have identified patterns that could have significant implications for public safety and policy.\n\nPredominant Conditions for Accidents:\nA notable number of accidents occur under clear weather, during daylight, and on dry roads. This finding is somewhat counterintuitive as these conditions are typically considered safe. It suggests that other factors, such as driver distraction or traffic volume, might play a more significant role than expected.\n\n\nHigh-Risk Conditions:\nAlthough less frequent, accidents occurring under rare and extreme conditions, like severe crosswinds or during weather phenomena like blowing sand, are especially concerning. These events, while not common, have a high impact and could benefit from targeted, situation-specific safety measures.\n\n\nOverlap of Conditions:\nThe overlap between clusters in certain conditions, such as cloudy weather transitioning to rain, indicates that accidents under these conditions may not be distinctly more dangerous than others. This suggests that interventions should be balanced across a range of weather conditions rather than focusing on one in isolation\n\n\nPotential for Targeted Interventions:\nUnderstanding the commonalities within each cluster allows for the development of targeted interventions. For instance, enhancing street lighting and implementing anti-fog measures could mitigate the risk of accidents under specific conditions identified as higher risk.\nIn real-life terms, this analysis could help inform where and when to focus road safety resources to prevent accidents. It could lead to better-timed traffic advisories, more effective deployment of road maintenance crews, and smarter city planning regarding road design and lighting. On a personal level, it could help drivers be more aware of the conditions that are deceptively risky, like clear days, or truly dangerous, like foggy, poorly lit nights, potentially encouraging safer driving behaviors.\nUltimately, the goal of this analysis is to contribute to safer roads and to reduce the number and severity of traffic accidents, potentially saving lives and reducing injuries among road users."
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "The data I choose is the data that recorded the information about vehicle involved in fatal crashes. The dataset consists of 54,552 entries and 16 columns, reflecting various aspects of vehicle-related incidents. The columns are:\n\nWeather (WEATHER): An integer representing different weather conditions at the time of the incident.\nState Case Number (ST_CASE): A unique identifier for each case.\nState (STATE): An integer indicating the state where the incident occurred.\nLight Condition (LGT_COND): An integer code representing the light conditions during the incident.\nFatalities (FATALS): The number of fatalities in the incident.\nDriver Drinking (DR_DRINK): An indicator of whether the driver was drinking, represented as an integer.\nDriver ZIP Code (DR_ZIP): The ZIP code of the driver.\nHit and Run (HIT_RUN): An integer indicating whether the incident was a hit-and-run.\nDriver’s License State (L_STATE): The state of issuance of the driver’s license.\nMake Name (MAKENAME), Make and Model Name (MAK_MODNAME): Textual descriptions of the vehicle’s make and specific make-model combination.\nModel Year (MOD_YEAR): The year of the vehicle model.\nRoad Surface Condition (VSURCOND): An integer code representing the condition of the road surface.\nvPIC Make Name (VPICMAKENAME), vPIC Model Name (VPICMODELNAME): Textual descriptions of the vehicle’s make and model derived from the Vehicle Plant Inquiry Data (vPIC).\n\nEach row in the dataset represents an incident, providing a comprehensive view of various factors like environmental conditions, vehicle specifics, and the nature of the incident.\nWhat I’m trying to achieve is to the data to analyze what kind of environment(i.e. weather, road surface condition, light condition) would most likely to lead fatal crashes, and also which manufacture’s vehicles are involved in fatal crashes most."
  },
  {
    "objectID": "Clustering.html#k-means-clustering",
    "href": "Clustering.html#k-means-clustering",
    "title": "Clustering",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK_Means Clustering is a method used to partition data into groups (clusters) based on similarity. Imageine you are working in a retail store, and your boss want to understand, “Who will come to our store and buy goods from us?” Here you will have the data on customer who came to your store, on what they bought, how often they visit your store, and what goods will some customer will usually buy in one time. For example, you might notice that some customers, possibly students, come in every few days for essentials like milk, eggs, and bread, while others might visit less frequently but fill up multiple carts with a wide variety of items.\nThese distinct shopping behaviors suggest that your customers can be grouped into different segments. K-Means Clustering helps you do just that. By analyzing the purchase data, K-Means can categorize customers into segments based on their shopping habits. This segmentation is invaluable because it enables your store to tailor marketing strategies and product placements to cater to the specific needs and preferences of each group. For example, you might target the frequent, small-basket customers with quick checkout options and convenience goods, while the less frequent, large-basket shoppers might be more interested in bulk deals and a wider variety of products.\nAn essential aspect of K-Means Clustering is choosing the right number of clusters, denoted as ‘K’. This is where the Elbow Method comes in handy. It involves running the K-Means algorithm multiple times with varying numbers of clusters and plotting the average distance of data points from their cluster center for each run. As you increase K, there comes a point where increasing the number of clusters doesn’t significantly improve the closeness of data points to their centers. This point, evident as a bend or ‘elbow’ in the plot, is an indicator of an appropriate number of clusters. By using this method, you can determine the optimal number of customer segments, ensuring a more precise and effective customer segmentation strategy."
  },
  {
    "objectID": "Clustering.html#dbscan",
    "href": "Clustering.html#dbscan",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering technique that excels at grouping data points based on their density. It’s particularly effective when dealing with irregularly shaped clusters or data with a lot of ‘noise’ (irrelevant or outlier points). The process begins by selecting a random data point and searching for nearby points within a specified distance.\nIf enough points are found close to this initial point, DBSCAN treats this as a cluster. It then iteratively expands the cluster by finding all points close to each new point added to the cluster. Points that don’t belong closely to any cluster are marked as outliers.\nThis method is different from K-Means as it doesn’t require pre-setting the number of clusters. Instead, DBSCAN dynamically determines the number of clusters based on the data itself. However, it does require defining two key parameters: the distance threshold for points to be considered neighbors and the minimum number of points needed to form a dense region (or a cluster).\nLet’s think about a practical application in urban planning. Suppose a city’s planning department wants to analyze traffic accident data to improve road safety. They have a dataset of traffic incident locations spanning several years. Using DBSCAN, they can pinpoint clusters of accidents, indicating high-risk areas on the roads.\nThis method is ideal for such data, as it adeptly handles the irregular clustering of accidents around road networks and distinguishes isolated incidents (noise) from significant patterns. Identifying these clusters helps the city focus on specific areas for safety improvements, like traffic signal installation or intersection redesigns. In essence, DBSCAN aids in the efficient allocation of resources to areas where they can have the most substantial impact on improving road safety and traffic management."
  },
  {
    "objectID": "Clustering.html#hierarchical-clustering",
    "href": "Clustering.html#hierarchical-clustering",
    "title": "Clustering",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical Clustering is a method that arranges data into a hierarchy of nested clusters. There are two main approaches: Agglomerative (bottom-up) and Divisive (top-down). In the Agglomerative approach, each data point starts as its own cluster, and pairs of clusters are merged as one moves up the hierarchy. This is akin to constructing a family tree from the ground up, where individuals (data points) are grouped into families (clusters), and families into larger units, gradually forming the entire tree. The Divisive approach is the opposite, starting with all data points in one cluster and progressively dividing them into smaller clusters.\nThe result of this process is visualized in a dendrogram, a tree-like diagram. This diagram shows how every cluster is related to the others, and cutting the dendrogram at different levels gives different cluster groupings. To determine the best level for ‘cutting’, visual inspection is commonly used. Alternatively, one might use the Silhouette Score, which measures how similar an object is to its own cluster compared to other clusters. A high average silhouette score suggests well-defined clusters.\nApplying this to a real-world scenario if you still don’t quite catch up. Consider a large organization with a vast array of digital documents. Managing and organizing these documents can be a significant challenge. Hierarchical clustering can effectively categorize these documents based on content similarity. Starting from individual documents, the algorithm creates a hierarchy where documents are grouped together based on their similarities. For example, all finance-related documents might form one cluster, which can further be subdivided into more specific groups like tax documents, investment reports, and budget plans. This hierarchical organization of documents allows for easier navigation and retrieval, enabling employees to find relevant documents more efficiently. It’s akin to organizing a library where books are grouped by genres and sub-genres, making it easier for readers to find what they’re looking for."
  },
  {
    "objectID": "Clustering.html#k-means-clustering-1",
    "href": "Clustering.html#k-means-clustering-1",
    "title": "Clustering",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nElbow Method\nWe will first use elbow method to determine the optimal number of clusters \nHere we chose the optimal number as 5 And we can get a cluster result like below \nBefore we jump into result, we need to identify what does those number means\nHere, numbers in each features represent differnt conditions.\n\n\n\n\n\n\n\n\nWeather\nLight Condition\nRoad Surface Condition\n\n\n\n\n1. Clear\n1. Daylight\n0. Non - Trafficway or Driveway Access\n\n\n2. Rain\n2. Dark - Not Lighted\n1. Dry\n\n\n3. Sleet or Hail\n3. Dark - Lighted\n2. Wet\n\n\n4. Snow\n4. Dawn\n3. Snow\n\n\n5. Fog, Smog, Smoke\n5. Dusk\n4. Ice/Frost\n\n\n6. Severe Crosswinds\n6. Dark - Unknown Lighting\n5. Sand\n\n\n7. Blowing Sand, Soil, Dirt\n7. Other\n6. Water (Standing or Moving)\n\n\n8. Other\n8. Not Reported\n7. Oil\n\n\n9. N/A\n9. Reported as Unknown\n8. Other\n\n\n10. Cloudy\n\n9. Slush\n\n\n11. Blowing Snow\n\n10. Mud, Dirt or Gravel\n\n\n12. Freezing Rain or Drizzle\n\n\n\n\n\nSo let’s dive in the result\n\n\nResults of K-Means Clustering (Elbow Method)\nClusters Analysis: The clusters appear to be differentiated mostly along the ‘WEATHER’ and ‘VSURCOND’ axes. This suggests that weather conditions and road surface conditions play significant roles in differentiating between clusters. The histograms on the diagonal show the distribution of each feature within each cluster. For instance, one of the clusters (in red) seems to have a higher frequency of incidents occurring under a specific weather condition(1 and 2, that is clear or rain) whereas the light and road surface conditions are more spread out. This is because these are two most common weather condition happen on the road, whereas others are less likely to happen or in some states it won’t ever happen. Also, we can see from the (0,0) plot that the number of accidents happened during clear and rain are much larger than accidents happened in other weather condition.\nRelationships between Features: In the scatter plots, you can see how the clusters are distributed with respect to two features at a time. For example, in the ‘WEATHER’ versus ‘LGT_COND’ scatter plot, you can see that some clusters are more prevalent under certain combinations of weather and light conditions. It looks like the blue, purple, and orange clusters are more scattered across conditions, indicating light condition might not have a strong impact on chance of fatal crashes, while the red and orange clusters also represent more specific conditions (Rain, Sleet or Hail, Blowing Snow or Freezing Rain, which will cause road surface condition getting worse and the chance of having a fatal crash increases dramatically).\n\n\nSilhouette Method\nThe Silhouette method returned a quite similar result \nThe optimal number of clusters should be 2, but there was not much useful infomration we can get form that, so I used the second highest score, which is 9 clusters.\nAnd the results are \n\n\nResults of K-Means Clustering (Silhouette Method)\nBased on the plot and the ouput result, we can infer that\n\nThe highest risk of fatality seems to be associated with driving in the dark without proper lighting, especially in foggy conditions.\nWeather that impacts visibility, like fog and heavy clouds, or adverse road conditions, such as blowing snow and non-standard surfaces, appears to increase the risk of fatal accidents.\nClusters with daylight, clear weather, and dry roads tend to have lower fatality rates, suggesting these are the safest conditions among the clusters analyzed.\nRainy conditions with wet roads also show an increased average in fatalities, indicating the hazardous nature of such conditions."
  },
  {
    "objectID": "Clustering.html#dbscan-1",
    "href": "Clustering.html#dbscan-1",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\nThis is a cluster result plotted without any noise points on.\n\n\n\nno noise\n\n\nClearly this plot cannot bring us any useful information, let’s try another way to see what will happen.\n\n\n\nwayyyy to many clusters\n\n\nThe plot shows a variety of clusters, each represented by a different color. Some clusters appear more densely packed, while others are sparse. The densely packed clusters likely represent more common combinations of weather, light, and road surface conditions that frequently occur together, while sparser ones may represent less common or more unique conditions. This is not an ideal result for us to see, and probablt DBSCAN is not a good model for this dataset."
  },
  {
    "objectID": "Clustering.html#hierarchical-clustering-1",
    "href": "Clustering.html#hierarchical-clustering-1",
    "title": "Clustering",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n \nThe plot shows PCA has been used to reduce the dimensionality of the data to two principal components, which are linear combinations of the original variables. The proximity of points within the same cluster (same color) suggests they are similar to each other based on the original features (weather, light, and road surface conditions).\nSome clusters are more tightly grouped, suggesting that there are well-defined conditions under which a number of accidents commonly occur. Based on the size of count and the size of data, most of the likely combination could be clear weather, daylight, and dry road surface condition, which is the most environmental condition for an accident, also the most common one. Some other clusters that are sparse or have points spread out may indicate less common or more variable conditions. Probably these are some adverse weather conditions like fod or snow with dusk or dawn light condition, with some potential hazardous road surface condition that is rarely happen in real life. All of those needs to be confirmed by further analysis."
  },
  {
    "objectID": "Data Cleaning Python.html",
    "href": "Data Cleaning Python.html",
    "title": "Data cleaning in text data",
    "section": "",
    "text": "First I use NewsAPI to get text data from NewsAPI on different key words in terms of driver behavior analysis\nThe key words I used are: driving behavior, distracted driving, driver risk assessment, driver profilling"
  },
  {
    "objectID": "Data Cleaning Python.html#record-data",
    "href": "Data Cleaning Python.html#record-data",
    "title": "Data cleaning in text data",
    "section": "Record Data",
    "text": "Record Data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n\n\n\ndata = pd.read_csv('Data/FARS2021NationalCSV/accident.csv', encoding='ISO-8859-1')\n\n# Define a function to convert hours and minutes into minutes since the start of the day\ndef convert_to_minutes(hour_col, min_col):\n    return hour_col * 60 + min_col\n\n# Convert notification time and arrival time into minutes\ndata['NOT_MINUTES'] = convert_to_minutes(data['NOT_HOUR'], data['NOT_MIN'])\ndata['ARR_MINUTES'] = convert_to_minutes(data['ARR_HOUR'], data['ARR_MIN'])\n\n# Calculate the time gap\ndata['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']\n\n# Handling cases where the time difference is negative due to crossing midnight\n# Assuming that EMS response times will be within a 24-hour period\ndata['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x &lt; 0 else 0))\n\n# Create the binary target variable\ndata['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] &gt; 10).astype(int)\n\n# Display the new columns\ndata[['NOT_HOUR', 'NOT_MIN', 'ARR_HOUR', 'ARR_MIN', 'TIME_DIFF', 'EMS_MORE_THAN_10_MIN']].head()\n\nvalid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]\n\n# Redefine the feature selection without the specified columns\nrevised_features = [\n    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', \n    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'\n]\n\n\nselected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')\n\n# Define the target variable\ny_revised = valid_data['EMS_MORE_THAN_10_MIN']\n\n# Split the revised data into training and testing sets\nX_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(\n    selected_features_revised, y_revised, test_size=0.2, random_state=0)\n\nmodel_revised = GaussianNB()\n\n# Train the revised model\nmodel_revised.fit(X_train_revised, y_train_revised)\n\n# Predict on revised test data\ny_pred_revised = model_revised.predict(X_test_revised)\n\ncm_revised = confusion_matrix(y_test_revised, y_pred_revised)\n\naccuracy_revised = accuracy_score(y_test_revised, y_pred_revised)\nreport_revised = classification_report(y_test_revised, y_pred_revised)\n\nprint(accuracy_revised)\nprint(report_revised)\n\n# Plotting the confusion matrix for the revised model\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_revised, annot=True, fmt='d', cmap='Blues', xticklabels=['&lt;=10 min', '&gt;10 min'], yticklabels=['&lt;=10 min', '&gt;10 min'])\nplt.title('Confusion Matrix for Revised EMS Arrival Time Prediction')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\n\n# Load the dataset\npdf_path = ('Data/FARS2021NationalCSV/accident.csv')\ndata = pd.read_csv(pdf_path, encoding='ISO-8859-1')\n\n# Preprocess the data as before\ndata['NOT_MINUTES'] = data['NOT_HOUR'] * 60 + data['NOT_MIN']\ndata['ARR_MINUTES'] = data['ARR_HOUR'] * 60 + data['ARR_MIN']\ndata['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']\ndata['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x &lt; 0 else 0))\ndata['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] &gt; 10).astype(int)\n\n# Remove invalid records (where NOT_HOUR or ARR_HOUR is 99)\nvalid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]\n\n# Redefine the feature selection without the specified columns\nrevised_features = [\n    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', \n    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'\n]\n\n# Prepare the feature matrix and target vector\nselected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')\ny_revised = valid_data['EMS_MORE_THAN_10_MIN']\n\n# Encode categorical variables\nlabel_encoders = {}\nfor column in selected_features_revised.select_dtypes(include=['object']).columns:\n    label_encoders[column] = LabelEncoder()\n    selected_features_revised[column] = label_encoders[column].fit_transform(selected_features_revised[column])\n\n# Split the dataset into train and test sets\nX_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(\n    selected_features_revised, y_revised, test_size=0.2, random_state=0)\n\n# Initialize and train the Gaussian Naive Bayes model\nmodel_revised = GaussianNB()\nmodel_revised.fit(X_train_revised, y_train_revised)\n\n# Predict probabilities for the test set\ny_scores_revised = model_revised.predict_proba(X_test_revised)[:, 1]\n\n# Compute precision-recall pairs for different probability thresholds\nprecision_revised, recall_revised, thresholds_revised = precision_recall_curve(y_test_revised, y_scores_revised)\n\n# Plot the Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall_revised, precision_revised, marker='.', label='Revised Naive Bayes')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Revised Model')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nfpr, tpr, roc_thresholds = roc_curve(y_test_revised, y_scores_revised)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "Data Cleaning Python.html#text-data",
    "href": "Data Cleaning Python.html#text-data",
    "title": "Data cleaning in text data",
    "section": "Text Data",
    "text": "Text Data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nfile_path = (\"Data/labeled_articles_sentiment.csv\")\ndata = pd.read_csv(file_path)\n\ndata['text'] = data['title'] + ' ' + data['description']\n\n# Encode the sentiment column to numerical values\nlabel_encoder = LabelEncoder()\ndata['sentiment_encoded'] = label_encoder.fit_transform(data['sentiment'])\n\n# Split the data into features and target\nX = data['text']\ny = data['sentiment_encoded']\n\n# Perform a train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n\n# Fit and transform the vectorizer on the training data and transform the testing data\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Show the shape of the resulting TF-IDF matrices\nX_train_tfidf.shape, X_test_tfidf.shape\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Initialize the Multinomial Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the classifier\nnb_classifier.fit(X_train_tfidf, y_train)\n\n# Predict the labels for the test set\ny_pred = nb_classifier.predict(X_test_tfidf)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\n# Create a confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\naccuracy, report\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import average_precision_score\n\n# Binarize the output labels for the multi-class case\ny_test_binarized = label_binarize(y_test, classes=range(len(label_encoder.classes_)))\n\n# Initialize a dictionary to hold the precision-recall curves for each class\nprecision_recall_curve_dict = {}\n\n# Calculate the precision-recall curve and average precision for each class\nfor i, class_label in enumerate(label_encoder.classes_):\n    class_precisions, class_recalls, class_thresholds = precision_recall_curve(y_test_binarized[:, i], y_scores[:, i])\n    precision_recall_curve_dict[class_label] = (class_precisions, class_recalls)\n    avg_precision = average_precision_score(y_test_binarized[:, i], y_scores[:, i])\n    print(f\"Average precision-recall score for class '{class_label}': {avg_precision:.2f}\")\n\n# Plot the precision-recall curve for each class\nplt.figure(figsize=(10, 6))\n\nfor class_label, (class_precisions, class_recalls) in precision_recall_curve_dict.items():\n    plt.plot(class_recalls, class_precisions, lw=2, label=f'Precision-Recall curve of class {class_label}')\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall curve per class\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\n# Let's check the actual counts for each sentiment in y_train and y_pred\ntrain_counts = pd.Series(y_train).value_counts().sort_index()\npred_counts = pd.Series(y_pred).value_counts().sort_index()\n\n# Now we plot the actual counts to verify the distribution\nfig, ax = plt.subplots(1, 2, figsize=(14, 7), sharey=True)\n\nsns.barplot(x=train_counts.index, y=train_counts.values, ax=ax[0], palette=\"viridis\")\nax[0].set_title('Distribution of Actual Sentiments (Training Set)')\nax[0].set_xlabel('Sentiment')\nax[0].set_ylabel('Count')\nax[0].set_xticklabels(label_encoder.inverse_transform(train_counts.index))\n\nsns.barplot(x=pred_counts.index, y=pred_counts.values, ax=ax[1], palette=\"viridis\")\nax[1].set_title('Distribution of Predicted Sentiments (Test Set)')\nax[1].set_xlabel('Sentiment')\nax[1].set_xticklabels(label_encoder.inverse_transform(pred_counts.index))\n\nplt.tight_layout()\nplt.show()\n\n# Show actual counts for verification\ntrain_counts, pred_counts"
  },
  {
    "objectID": "Data Cleaning Python.html#pca",
    "href": "Data Cleaning Python.html#pca",
    "title": "Data cleaning in text data",
    "section": "PCA",
    "text": "PCA\nimport pandas as pd\n\naccident_df = pd.read_csv('../../FARS2020NationalCSV/accident.csv', encoding = 'ISO-8859-1')\nperson_df = pd.read_csv('../../FARS2020NationalCSV/person.csv', encoding = 'ISO-8859-1')\n\nmerge_df = pd.merge(accident_df, person_df, on = \"ST_CASE\", how = 'inner')\n\ncolumns_to_keep = [\n    # Driver-Specific Information\n    'AGE', 'SEX', 'DRINKING', 'DRUGS', 'ALC_DET', 'ALC_STATUS', \n    'DRUG_DET', 'DSTATUS', 'INJ_SEV', 'PER_TYP', 'DAY_WEEK', 'HOUR_x', 'WEATHER', 'LGT_COND','STATE_x'\n]\n\n# Keeping only the selected columns and dropping the rest\nmerge_df = merge_df[columns_to_keep]\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\ndata = merge_df\nscaler = StandardScaler()\nscaled_pca_data = scaler.fit_transform(data)\n\n# Re-applying PCA\npca = PCA()\npca_transformed_data = pca.fit_transform(scaled_pca_data)\n\n# Explained variance ratio for each principal component\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Cumulative explained variance\ncumulative_explained_variance = np.cumsum(explained_variance_ratio)\n\n# Displaying the first few principal components' explained variance\nexplained_variance_ratio, cumulative_explained_variance\nplt.plot(cumulative_explained_variance, marker='o')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance by PCA Components')\nplt.grid(True)\nplt.show()\n\npc1 = pca_transformed_data[:, 0]  \npc2 = pca_transformed_data[:, 1]  \n\n\nplt.figure(figsize=(12, 10))\nplt.scatter(pc1, pc2, alpha=0.5)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA - First Two Principal Components')\nplt.grid(True)\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef biplot(score, coeff, labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0/(xs.max() - xs.min())\n    scaley = 1.0/(ys.max() - ys.min())\n    \n    plt.scatter(xs * scalex, ys * scaley, s=5)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color='r', alpha=0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color='black', ha='center', va='center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color='black', ha='center', va='center')\n\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n\n# PCA scores (the transformed data points)\npca_scores = pca_transformed_data\n\n# PCA loadings (the contributions of the original variables to the components)\npca_loadings = pca.components_\n\nplt.figure(figsize=(12,12))\nbiplot(pca_scores, pca_loadings.transpose(), labels=data.columns)\nplt.show()"
  },
  {
    "objectID": "Data Cleaning Python.html#t-sne",
    "href": "Data Cleaning Python.html#t-sne",
    "title": "Data cleaning in text data",
    "section": "t-SNE",
    "text": "t-SNE\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nsubset_data = data[['AGE', 'SEX', 'DRINKING', 'DRUGS']]\n\nimputer = SimpleImputer(strategy='median')\ndf_imputed = imputer.fit_transform(subset_data)\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_imputed)\nperplexities = [5, 20, 40]\n\nplt.figure(figsize=(12,12))  # Set the figure size for the subplot layout\n\nfor i, perp in enumerate(perplexities, 1):\n    tsne = TSNE(n_components=2, perplexity=perp, n_iter=300, random_state=42)\n    tsne_results = tsne.fit_transform(df_scaled)\n    \n    plt.subplot(1, len(perplexities), i)\n    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5, c= subset_data['SEX'], cmap='viridis')\n    plt.title(f't-SNE with Perplexity = {perp}')\n    plt.xlabel('t-SNE Component 1')\n    plt.ylabel('t-SNE Component 2')\n    plt.colorbar(scatter, label='SEX')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Data Cleaning Python.html#rough-clean",
    "href": "Data Cleaning Python.html#rough-clean",
    "title": "Data cleaning in text data",
    "section": "Rough Clean",
    "text": "Rough Clean\nimport pandas as pd\n\naccident_df = pd.read_csv('../../FARS2020NationalCSV/accident.csv', encoding = 'ISO-8859-1')\nvehicle_df = pd.read_csv('../../FARS2020NationalCSV/vehicle.csv', encoding = 'ISO-8859-1')\naccident_df = accident_df[['WEATHER', 'ST_CASE','STATE', 'LGT_COND', 'FATALS']]\nvehicle_df = vehicle_df[['ST_CASE','DR_DRINK', 'DR_ZIP', 'HIT_RUN', 'L_STATE', 'MAKENAME', 'MAK_MODNAME', 'MOD_YEAR', 'VSURCOND', 'VPICMAKENAME', 'VPICMODELNAME']]\nmerge_df = pd.merge(accident_df, vehicle_df, on = \"ST_CASE\", how = 'inner')\nmerge_df = merge_df[(merge_df['WEATHER'] &lt;= 90) & (merge_df['VSURCOND'] &lt;= 90) & (merge_df['MOD_YEAR'] &lt;= 3000)]\noutput_file_path = 'Data/clustering_data.csv'\nmerge_df.to_csv(output_file_path)\noutput_file_path"
  },
  {
    "objectID": "Data Cleaning Python.html#k-means-clustering",
    "href": "Data Cleaning Python.html#k-means-clustering",
    "title": "Data cleaning in text data",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nElbow Method\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Preparing data for K-means clustering\n# Selecting only numerical features relevant for environmental analysis\nkmeans_data = merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']]\n\n# Using the Elbow Method to find the optimal number of clusters\nsse = []  # Sum of squared distances\nfor k in range(1, 11):  # Testing for 1 to 10 clusters\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(kmeans_data)\n    sse.append(kmeans.inertia_)  # Inertia: Sum of squared distances of samples to their closest cluster center\n\n# Plotting the Elbow Curve\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 11), sse, marker='o')\nplt.title('Elbow Method for Optimal k')\nplt.xlabel('Number of clusters')\nplt.ylabel('Sum of squared distances')\nplt.xticks(range(1, 11))\nplt.show()\n\n\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfeatures = merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']]\n\n# Applying K-means clustering with 5 clusters\nkmeans = KMeans(n_clusters=5)\nclusters = kmeans.fit_predict(features)\n\n# Adding the cluster information to the original dataframe\nmerge_df['Cluster'] = clusters\n\n# Analyzing the clusters\nfor i in range(5):  # Looping through 5 clusters\n    cluster = merge_df[merge_df['Cluster'] == i]\n    print(f\"Cluster {i}:\")\n    print(f\"Weather Conditions: {cluster['WEATHER'].mode()[0]}\")\n    print(f\"Light Conditions: {cluster['LGT_COND'].mode()[0]}\")\n    print(f\"Road Surface Conditions: {cluster['VSURCOND'].mode()[0]}\")\n    print(f\"Average Fatalities: {cluster['FATALS'].mean()}\")\n    print()\n\n# Visualizing the clusters\nsns.pairplot(merge_df, vars=['WEATHER', 'LGT_COND', 'VSURCOND'], hue='Cluster', palette='bright')\nplt.title('K-Means Clustering with 5 Clusters')\nplt.show()\n\n\nSilhouette\nfrom sklearn.metrics import silhouette_score\n\n# Assuming 'features' is your data prepared for clustering\nfeatures = merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']]\n\nsilhouette_scores = []\nrange_n_clusters = list(range(2, 10))  # Example range from 2 to 9 clusters\n\nfor n_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(features)\n    silhouette_avg = silhouette_score(features, clusters)\n    silhouette_scores.append(silhouette_avg)\n    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")\n\n# Plotting the silhouette scores\nplt.figure(figsize=(10, 6))\nplt.plot(range_n_clusters, silhouette_scores, marker='o')\nplt.title('Silhouette Method For Optimal k')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Average Silhouette Score')\nplt.xticks(range_n_clusters)\nplt.show()\n\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Applying K-means clustering with 2 clusters generated by silhouette method\nkmeans = KMeans(n_clusters=9)\nclusters = kmeans.fit_predict(features)\n\n# Adding the cluster information to the original dataframe\nmerge_df['Cluster'] = clusters\n\n# Analyzing the clusters\nfor i in range(9):  # Looping through 2 clusters\n    cluster = merge_df[merge_df['Cluster'] == i]\n    print(f\"Cluster {i}:\")\n    print(f\"Weather Conditions: {cluster['WEATHER'].mode()[0]}\")\n    print(f\"Light Conditions: {cluster['LGT_COND'].mode()[0]}\")\n    print(f\"Road Surface Conditions: {cluster['VSURCOND'].mode()[0]}\")\n    print(f\"Average Fatalities: {cluster['FATALS'].mean()}\")\n    print()\n\n# Visualizing the clusters\nsns.pairplot(merge_df, vars=['WEATHER', 'LGT_COND', 'VSURCOND'], hue='Cluster', palette='bright')\nplt.title('K-Means Clustering with 2 Clusters')\nplt.show()"
  },
  {
    "objectID": "Data Cleaning Python.html#dbscan",
    "href": "Data Cleaning Python.html#dbscan",
    "title": "Data cleaning in text data",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nfeatures_standardized = scaler.fit_transform(features)\n\ndbscan = DBSCAN(eps=0.5, min_samples=500)\nclusters = dbscan.fit_predict(features_standardized)\n\n# Add the cluster labels to your dataframe\nmerge_df['Cluster'] = clusters\n\n# Check the number of points in each cluster\ncluster_counts = merge_df['Cluster'].value_counts()\nprint(cluster_counts)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Reduce the data to two dimensions using PCA for visualization purposes\npca = PCA(n_components=2)\nfeatures_pca = pca.fit_transform(features_standardized)\n\n# Create a scatter plot of the PCA-reduced data color-coded by cluster label\nplt.figure(figsize=(10, 10))\nunique_labels = np.unique(clusters)\n# Generate a color map similar to plt.cm.Spectral but without black\ncolors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Skip noise points, they won't be plotted\n        continue\n\n    class_member_mask = (clusters == k)\n\n    xy = features_pca[class_member_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)\n\nplt.title('DBSCAN clustering (without noise)')\nplt.xlabel('PCA Feature 1')\nplt.ylabel('PCA Feature 2')\nplt.show()\n\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standardize the features before applying DBSCAN\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']])\n\n# Apply DBSCAN to the standardized features\n# Note: You'll need to tune these parameters\ndbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\nclusters = dbscan.fit_predict(features_scaled)\n\n# Add cluster info to the original DataFrame\nmerge_df['Cluster'] = clusters\n\n# Reduce dimensions for visualization\npca = PCA(n_components=2)\nfeatures_pca = pca.fit_transform(features_scaled)\n\n# Visualize the clusters\nunique_labels = set(clusters)\nfor label in unique_labels:\n    plt.scatter(features_pca[clusters == label, 0], features_pca[clusters == label, 1], label=label)\n\nplt.title('DBSCAN Clustering')\nplt.xlabel('PCA Feature 1')\nplt.ylabel('PCA Feature 2')\nplt.show()"
  },
  {
    "objectID": "Data Cleaning Python.html#hierarchical-clustering",
    "href": "Data Cleaning Python.html#hierarchical-clustering",
    "title": "Data cleaning in text data",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.decomposition import PCA\nimport sys\n\nsys.setrecursionlimit(10000)\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']])\n\n# Performing hierarchical clustering\nZ = linkage(features_scaled, method='ward')\n\n# Plotting the dendrogram\nplt.figure(figsize=(12, 8))\ndendrogram(Z, truncate_mode='lastp', p=50, show_leaf_counts=True)\n\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Index')\nplt.ylabel('Ward\\'s Distance')\nplt.show()\n\n# To cut the dendrogram and get the cluster labels, use the following:\nfrom scipy.cluster.hierarchy import fcluster\nk = 9  # for example, if we decide on 9 clusters\ncluster_labels = fcluster(Z, k, criterion='maxclust')\n\n# Adding the cluster information to the original dataframe\nmerge_df['Cluster'] = cluster_labels\n\n# Optionally reduce dimensions for visualization\npca = PCA(n_components=2)\nfeatures_pca = pca.fit_transform(features_scaled)\n\n# Visualize the clusters\nplt.figure(figsize=(8, 6))\nplt.scatter(features_pca[:, 0], features_pca[:, 1], c=merge_df['Cluster'], cmap='viridis')\nplt.title('Hierarchical Clustering Results')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()"
  },
  {
    "objectID": "Dimensionality Reduction.html",
    "href": "Dimensionality Reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "This project aims to find out which factors could have a strong influence on fatal crashes. Does it have connections with drinking and drugs? Is it positively related to sex or age?\nOn this page, we will discover two ways of doing dimensionality reduction. One is PCA, Principal Component Analysis, the other is t-Distributed Stochastic Neighbor Embedding (t-SNE). These techniques are applied to a dataset to understand their effectiveness in data visualization and pattern recognition.\nThe data I used is the fatal accidents that happened in America in 2020. It contains detailed information about every accident police recorded, and we will this time focus on the people side, to see which factors have the most impact on driver behavior and lead to fatal crashes.\nPackages used in this project: 1. Scikit-Learn: For implementing PCA and t-SNE algorithms. This library offers efficient and easy-to-use tools for data analysis and machine learning. 2. Matplotlib: For data visualization, creating plots to visually compare the results obtained from PCA and t-SNE. 3. NumPy: For numerical computations, which might be necessary for data preprocessing or additional analysis. 4. Pandas: For data manipulation and extraction, especially useful if the dataset is in a tabular format.\n\n\nBefore we proceed, we first rough clean the data, remove any non-human factors.\nmerge_df = pd.merge(accident_df, person_df, on = \"ST_CASE\", how = 'inner')\n\ncolumns_to_keep = [\n    # Driver-Specific Information\n    'AGE', 'SEX', 'DRINKING', 'DRUGS', 'ALC_DET', 'ALC_STATUS', \n    'DRUG_DET', 'DSTATUS', 'INJ_SEV', 'PER_TYP', 'DAY_WEEK', 'HOUR_x', 'WEATHER', 'LGT_COND','STATE_x'\n]\n\n# Keeping only the selected columns and dropping the rest\nmerge_df = merge_df[columns_to_keep]\nNow let’s proceed.\nUsing PCA, we are able to get a plot which is the cumulative explained variance by PCA Components. \nSo we select the first two components, and let’s see if we can get some information from these two\n\n\n\nPCA components\n\n\nThere is no clear clustering of points, which suggests that there may not be distinct groups within the data based on these two principal components alone.\nLet’s try biplot to see what kinds of result will we get\n\n\n\nPCA output\n\n\nAlright! Based on this plot, Variables like Drinking, drugs, and drug detection, are pointing in a similar way, this suggests that they are positively correlated with each other and contribute similarly to these components. Based on their direction, these factors have more influence on PC 1 than it has on PC 2.\nSex and age are similar to drugs and drinking but with less influence than those components.\nALC_STATUS has a strong loading on PC2 and some level of loading on PC1, indicating that this variable significantly differentiates the data along PC 2. Injury severity has a notable contribution to both PC1 and PC2, but more on PC2. This might be interpreted as injury severity varying along with alcohol status but also related to the other factors that align with PC1.\nThe variables associated with substance use (DRINKING, DRUGS, DRUG_DET, ALC_STATUS) are significant in explaining the variance in the data. This suggests that substance use is a major factor in the nature and severity of crashes. So, no drugs, no drinking if you are going to drive.\nThe positioning of INJ_SEV could indicate that crashes involving substance use or certain demographics (SEX) may result in different injury severities. Other variables like WEATHER, LGT_COND (light conditions), and HOUR have smaller angles with each other, suggesting they are somewhat correlated and may jointly influence the conditions under which accidents occur."
  },
  {
    "objectID": "Dimensionality Reduction.html#pca",
    "href": "Dimensionality Reduction.html#pca",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Before we proceed, we first rough clean the data, remove any non-human factors.\nmerge_df = pd.merge(accident_df, person_df, on = \"ST_CASE\", how = 'inner')\n\ncolumns_to_keep = [\n    # Driver-Specific Information\n    'AGE', 'SEX', 'DRINKING', 'DRUGS', 'ALC_DET', 'ALC_STATUS', \n    'DRUG_DET', 'DSTATUS', 'INJ_SEV', 'PER_TYP', 'DAY_WEEK', 'HOUR_x', 'WEATHER', 'LGT_COND','STATE_x'\n]\n\n# Keeping only the selected columns and dropping the rest\nmerge_df = merge_df[columns_to_keep]\nNow let’s proceed.\nUsing PCA, we are able to get a plot which is the cumulative explained variance by PCA Components. \nSo we select the first two components, and let’s see if we can get some information from these two\n\n\n\nPCA components\n\n\nThere is no clear clustering of points, which suggests that there may not be distinct groups within the data based on these two principal components alone.\nLet’s try biplot to see what kinds of result will we get\n\n\n\nPCA output\n\n\nAlright! Based on this plot, Variables like Drinking, drugs, and drug detection, are pointing in a similar way, this suggests that they are positively correlated with each other and contribute similarly to these components. Based on their direction, these factors have more influence on PC 1 than it has on PC 2.\nSex and age are similar to drugs and drinking but with less influence than those components.\nALC_STATUS has a strong loading on PC2 and some level of loading on PC1, indicating that this variable significantly differentiates the data along PC 2. Injury severity has a notable contribution to both PC1 and PC2, but more on PC2. This might be interpreted as injury severity varying along with alcohol status but also related to the other factors that align with PC1.\nThe variables associated with substance use (DRINKING, DRUGS, DRUG_DET, ALC_STATUS) are significant in explaining the variance in the data. This suggests that substance use is a major factor in the nature and severity of crashes. So, no drugs, no drinking if you are going to drive.\nThe positioning of INJ_SEV could indicate that crashes involving substance use or certain demographics (SEX) may result in different injury severities. Other variables like WEATHER, LGT_COND (light conditions), and HOUR have smaller angles with each other, suggesting they are somewhat correlated and may jointly influence the conditions under which accidents occur."
  },
  {
    "objectID": "Data Gathering.html",
    "href": "Data Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "The recod data I collected are mainly from NHTSA, the National Highway Traffic Safety Administration. The NHTSA have publications that contains many data that is quite useful for this research. However, some data are available in differnt kinds besides csv. So it will take quite a time to clean\n\n\nNow the raw data looks like this,  It’s quite messy on the top. So, I will need to clean the column names to make it tidy.\nAfter cleaning process, the data now looks like this, \nNote this is not enough, I still need to further tidy the column names to make it better\n\n\n\nThe data available about young driver information is mainly from NHTSA, and it’s in PDF format, so I will need to first download the data, scrape them into csv file, and then do more analysis.\n\n\n\n\nThere are also some text data available that I use API to pull them from the webiste. I will further clean them and also put them onto the page. \nSo right now I will need to sort the text out, like find out the publisher of the paper, the title, the description, etc."
  },
  {
    "objectID": "Data Gathering.html#record-data",
    "href": "Data Gathering.html#record-data",
    "title": "Data Gathering",
    "section": "",
    "text": "The recod data I collected are mainly from NHTSA, the National Highway Traffic Safety Administration. The NHTSA have publications that contains many data that is quite useful for this research. However, some data are available in differnt kinds besides csv. So it will take quite a time to clean\n\n\nNow the raw data looks like this,  It’s quite messy on the top. So, I will need to clean the column names to make it tidy.\nAfter cleaning process, the data now looks like this, \nNote this is not enough, I still need to further tidy the column names to make it better\n\n\n\nThe data available about young driver information is mainly from NHTSA, and it’s in PDF format, so I will need to first download the data, scrape them into csv file, and then do more analysis."
  },
  {
    "objectID": "Data Gathering.html#text-data",
    "href": "Data Gathering.html#text-data",
    "title": "Data Gathering",
    "section": "",
    "text": "There are also some text data available that I use API to pull them from the webiste. I will further clean them and also put them onto the page. \nSo right now I will need to sort the text out, like find out the publisher of the paper, the title, the description, etc."
  },
  {
    "objectID": "PCA python code.html",
    "href": "PCA python code.html",
    "title": "Rough Clean Data",
    "section": "",
    "text": "import pandas as pd\n\naccident_df = pd.read_csv('../../FARS2020NationalCSV/accident.csv', encoding = 'ISO-8859-1')\nperson_df = pd.read_csv('../../FARS2020NationalCSV/person.csv', encoding = 'ISO-8859-1')\n\nmerge_df = pd.merge(accident_df, person_df, on = \"ST_CASE\", how = 'inner')\n\n\ncolumns_to_keep = [\n    # Driver-Specific Information\n    'AGE', 'SEX', 'DRINKING', 'DRUGS', 'ALC_DET', 'ALC_STATUS', \n    'DRUG_DET', 'DSTATUS', 'INJ_SEV', 'PER_TYP', 'DAY_WEEK', 'HOUR_x', 'WEATHER', 'LGT_COND','STATE_x'\n]\n\n# Keeping only the selected columns and dropping the rest\nmerge_df = merge_df[columns_to_keep]\n\n\noutput_file_path = 'Data/PCA_data.csv'\nmerge_df.to_csv(output_file_path)\noutput_file_path\n\n'Data/PCA_data.csv'\n\n\n\nPCA\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\ndata = merge_df\nscaler = StandardScaler()\nscaled_pca_data = scaler.fit_transform(data)\n\n# Re-applying PCA\npca = PCA()\npca_transformed_data = pca.fit_transform(scaled_pca_data)\n\n# Explained variance ratio for each principal component\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Cumulative explained variance\ncumulative_explained_variance = np.cumsum(explained_variance_ratio)\n\n# Displaying the first few principal components' explained variance\nexplained_variance_ratio, cumulative_explained_variance\nplt.plot(cumulative_explained_variance, marker='o')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance by PCA Components')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\npc1 = pca_transformed_data[:, 0]  \npc2 = pca_transformed_data[:, 1]  \n\n\nplt.figure(figsize=(12, 10))\nplt.scatter(pc1, pc2, alpha=0.5)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA - First Two Principal Components')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef biplot(score, coeff, labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0/(xs.max() - xs.min())\n    scaley = 1.0/(ys.max() - ys.min())\n    \n    plt.scatter(xs * scalex, ys * scaley, s=5)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color='r', alpha=0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color='black', ha='center', va='center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color='black', ha='center', va='center')\n\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n\n# PCA scores (the transformed data points)\npca_scores = pca_transformed_data\n\n# PCA loadings (the contributions of the original variables to the components)\npca_loadings = pca.components_\n\nplt.figure(figsize=(12,12))\nbiplot(pca_scores, pca_loadings.transpose(), labels=data.columns)\nplt.show()\n\n\n\n\n\n\nt-SNE\n\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nsubset_data = data[['AGE', 'SEX', 'DRINKING', 'DRUGS']]\n\nimputer = SimpleImputer(strategy='median')\ndf_imputed = imputer.fit_transform(subset_data)\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_imputed)\nperplexities = [5, 20, 40]\n\nplt.figure(figsize=(12,12))  # Set the figure size for the subplot layout\n\nfor i, perp in enumerate(perplexities, 1):\n    tsne = TSNE(n_components=2, perplexity=perp, n_iter=300, random_state=42)\n    tsne_results = tsne.fit_transform(df_scaled)\n    \n    plt.subplot(1, len(perplexities), i)\n    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5, c= subset_data['SEX'], cmap='viridis')\n    plt.title(f't-SNE with Perplexity = {perp}')\n    plt.xlabel('t-SNE Component 1')\n    plt.ylabel('t-SNE Component 2')\n    plt.colorbar(scatter, label='SEX')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/slides.html#germany-vehicles",
    "href": "slides/slides.html#germany-vehicles",
    "title": "Intro to Cars",
    "section": "Germany Vehicles",
    "text": "Germany Vehicles\n\nAudi\nBMW\nMercedes-Benz\nPorsche"
  },
  {
    "objectID": "slides/slides.html#america-vehicles",
    "href": "slides/slides.html#america-vehicles",
    "title": "Intro to Cars",
    "section": "America Vehicles",
    "text": "America Vehicles\n\nFord\nLincoln\nJeep\nCorvette\nDodge"
  },
  {
    "objectID": "slides/slides.html#asia-vehicles",
    "href": "slides/slides.html#asia-vehicles",
    "title": "Intro to Cars",
    "section": "Asia Vehicles",
    "text": "Asia Vehicles\n\nHonda\nToyota\nXpeng\nNIO"
  },
  {
    "objectID": "slides/slides.html#europe-cars---audi-a-series",
    "href": "slides/slides.html#europe-cars---audi-a-series",
    "title": "Intro to Cars",
    "section": "Europe Cars - Audi A Series",
    "text": "Europe Cars - Audi A Series\n\n\nA1\nA3\nA4\nA5\nA6\nA7\nA8"
  },
  {
    "objectID": "slides/slides.html#audi-a4",
    "href": "slides/slides.html#audi-a4",
    "title": "Intro to Cars",
    "section": "Audi A4",
    "text": "Audi A4\n\nAudi A4"
  },
  {
    "objectID": "slides/slides.html#audi-a8",
    "href": "slides/slides.html#audi-a8",
    "title": "Intro to Cars",
    "section": "AUdi A8",
    "text": "AUdi A8\n\nAudi A8"
  },
  {
    "objectID": "slides/slides.html#a-citation",
    "href": "slides/slides.html#a-citation",
    "title": "Intro to Cars",
    "section": "A Citation",
    "text": "A Citation\nThis approach enables you to use the same bibliography with different citation styles without having to change anything about your document or the bibliography itself apart from the bibliography style when your paper is finally compiled for print. (Fenn 2006)"
  },
  {
    "objectID": "slides/slides.html#a-plot",
    "href": "slides/slides.html#a-plot",
    "title": "Intro to Cars",
    "section": "A Plot",
    "text": "A Plot\n\n\n\n\n\n\n\nFenn, Jürgen. 2006. “Managing Citations and Your Bibliography with BibTEX.” The PracTEX Journal. http://svn.tug.org/pracjourn/2006-4/fenn/fenn.pdf."
  },
  {
    "objectID": "DataMain.html",
    "href": "DataMain.html",
    "title": "Everything About Data",
    "section": "",
    "text": "Data\nData is the foundation of every data analysis, with more data, the more information we will get. On those sub-tab, you will find different steps of data analysis. It includes how I collected the data, how to clean those data, and in what ways I can explore them and elaborate based on those data."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Written by Xingrui Huo Froome\nGU ID: xh231\n\n\nAs I moved to a new place for grad school, my car moved with me and so does the car insurance. However, as I requested for the new quote, I found out that the price went higher than expected. So I changed my insurance company and during the conversation with the new insurance company. I found out that there is one discount that can be added to my policy if I agree to let the insurance company to send me a device and put it in my vehicle to record my daily drive data.\nI found that very interesting because the insurance staff told me that as I driving, the device will record data and send it back to the insurance company. As to every end of policy, I will receive discount based on my driving records.\nI start to become interested about this little device. How does it record the data? What kind of data does it record? What kind of result can it get based on the data it records? I feel like that I can do the same thing, collect the same data, and do some similar analysis on myself. So I can do driver behavior analysis on myself. Of course, the most important thing in here is, how to do driver behavior analysis?\nBelow are two academic sources and some questions to start with:\n\n\n\n(Kaplan et al. 2015)\nThis is a link to source\n\n\nDriver drowsiness and distraction are two main reasons for traffic accidents and the related financial losses. Therefore, researchers have been working for more than a decade on designing driver inattention monitoring systems. As a result, several detection techniques for the detection of both drowsiness and distraction have been proposed in the literature. Some of these techniques were successfully adopted and implemented by the leading car companies. This paper discusses and provides a comprehensive insight into the well-established techniques for driver inattention monitoring and introduces the use of most recent and futuristic solutions exploiting mobile technologies such as smartphones and wearable devices. Then, a proposal is made for the active of such systems into car-to-car communication to support vehicular ad hoc network’s (VANET’s) primary aim of safe driving. We call this approach the dissemination of driver behavior via C2C communication. Throughout this paper, the most remarkable studies of the last five years were examined thoroughly in order to reveal the recent driver monitoring techniques and demonstrate the basic pros and cons. In addition, the studies were categorized into two groups: driver drowsiness and distraction. Then, research on the driver drowsiness was further divided into two main subgroups based on the exploitation of either visual features or nonvisual features. A comprehensive compilation, including used features, classification methods, accuracy rates, system parameters, and environmental details, was represented as tables to highlight the (dis)advantages and/or limitations of the aforementioned categories. A similar approach was also taken for the methods used for the detection of driver distraction.\n\n\n\nthe literature review highlighted the need for combining techniques, the importance of alerting mechanisms, and the potential of C2C communication for improving driver safety. Despite limitations, ongoing advancements indicate a promising future for driver monitoring and assistance systems.\n\n\n\n\n(Chan et al. 2019)\nThis is a link to source\n\n\nHuman factors are the primary catalyst for traffic accidents. Among different factors, fatigue, distraction, drunkenness, and/or recklessness are the most common types of abnormal driving behavior that leads to an accident. With technological advances, modern smartphones have the capabilities for driving behavior analysis. There has not yet been a comprehensive review on methodologies utilizing only a smartphone for drowsiness detection and abnormal driver behavior detection. In this paper, different methodologies proposed by different authors are discussed. It includes the sensing schemes, detection algorithms, and their corresponding accuracy and limitations. Challenges and possible solutions such as integration of the smartphone behavior classification system with the concept of context-aware, mobile crowdsensing, and active steering control are analyzed. The issue of model training and updating on the smartphone and cloud environment is also included.\n\n\n\nThe paper discusses the use of smartphone data as a valuable resource for analyzing driver behavior. It reviews various methodologies proposed by different authors for detecting abnormal driving patterns. While smartphone-based systems offer advantages over telematics boxes, there are significant challenges to consider for accurate driver behavior classification.\nKey challenges include the impact of varying light conditions on vision-based methodologies and the need to eliminate noise and external factors in sensor-based approaches, such as road conditions and vehicle components. Future research should address these issues. Additionally, eliminating the requirement for mounting the smartphone in a fixed position would enhance flexibility and convenience for drivers.\nThe paper also explores potential solutions, such as integrating smartphone-based driver behavior analysis with context-awareness, crowdsensing, and steering control, as well as mobile or cloud-based algorithm training and updates. While these solutions may not be perfect, they have the potential to address some of the challenges, suggesting that there is room for further improvement in smartphone-based behavior analysis systems.\n\n\n\n\n\n\n\nHow can we accurately measure and quantify aggressive driving behavior using telematics data?\nWhat are the most effective strategies for detecting and reducing instances of distracted driving?\nCan machine learning algorithms predict the likelihood of a driver engaging in risky behaviors based on historical data?\nHow does driver behavior vary across different age groups, genders, and experience levels, and what implications does this have for road safety?\nHow can data analytics be used to design targeted driver training programs that address specific behavioral issues?\n\n\n\n\n\nHow do external factors like road conditions and traffic congestion influence driver behavior, and how can this information be used to improve road safety?\nHow do weather conditions, such as rain, snow, and fog, affect road safety, and what strategies can be employed to mitigate their impact?\nHow can data science be used to analyze the effectiveness of road safety policies, such as seat belt laws and speed limits, in reducing accidents?\nWhat role does human psychology play in road safety, and how can behavioral insights be used to design safer road systems?\n\n\n\n\n\nHow does driver behavior impact fuel efficiency, and how can eco-driving habits be promoted to reduce emissions and fuel consumption?\nHow can driver behavior analysis be used to optimize traffic flow and reduce congestion in urban areas?\nHow can driver behavior data be anonymized and protected to ensure privacy while still extracting valuable insights for road safety?"
  },
  {
    "objectID": "Introduction.html#research-background",
    "href": "Introduction.html#research-background",
    "title": "Introduction",
    "section": "",
    "text": "As I moved to a new place for grad school, my car moved with me and so does the car insurance. However, as I requested for the new quote, I found out that the price went higher than expected. So I changed my insurance company and during the conversation with the new insurance company. I found out that there is one discount that can be added to my policy if I agree to let the insurance company to send me a device and put it in my vehicle to record my daily drive data.\nI found that very interesting because the insurance staff told me that as I driving, the device will record data and send it back to the insurance company. As to every end of policy, I will receive discount based on my driving records.\nI start to become interested about this little device. How does it record the data? What kind of data does it record? What kind of result can it get based on the data it records? I feel like that I can do the same thing, collect the same data, and do some similar analysis on myself. So I can do driver behavior analysis on myself. Of course, the most important thing in here is, how to do driver behavior analysis?\nBelow are two academic sources and some questions to start with:"
  },
  {
    "objectID": "Introduction.html#driver-behavior-analysis-for-safe-driving-a-survey",
    "href": "Introduction.html#driver-behavior-analysis-for-safe-driving-a-survey",
    "title": "Introduction",
    "section": "",
    "text": "(Kaplan et al. 2015)\nThis is a link to source\n\n\nDriver drowsiness and distraction are two main reasons for traffic accidents and the related financial losses. Therefore, researchers have been working for more than a decade on designing driver inattention monitoring systems. As a result, several detection techniques for the detection of both drowsiness and distraction have been proposed in the literature. Some of these techniques were successfully adopted and implemented by the leading car companies. This paper discusses and provides a comprehensive insight into the well-established techniques for driver inattention monitoring and introduces the use of most recent and futuristic solutions exploiting mobile technologies such as smartphones and wearable devices. Then, a proposal is made for the active of such systems into car-to-car communication to support vehicular ad hoc network’s (VANET’s) primary aim of safe driving. We call this approach the dissemination of driver behavior via C2C communication. Throughout this paper, the most remarkable studies of the last five years were examined thoroughly in order to reveal the recent driver monitoring techniques and demonstrate the basic pros and cons. In addition, the studies were categorized into two groups: driver drowsiness and distraction. Then, research on the driver drowsiness was further divided into two main subgroups based on the exploitation of either visual features or nonvisual features. A comprehensive compilation, including used features, classification methods, accuracy rates, system parameters, and environmental details, was represented as tables to highlight the (dis)advantages and/or limitations of the aforementioned categories. A similar approach was also taken for the methods used for the detection of driver distraction.\n\n\n\nthe literature review highlighted the need for combining techniques, the importance of alerting mechanisms, and the potential of C2C communication for improving driver safety. Despite limitations, ongoing advancements indicate a promising future for driver monitoring and assistance systems."
  },
  {
    "objectID": "Introduction.html#a-comprehensive-review-of-driver-behavior-analysis-utilizing-smartphones",
    "href": "Introduction.html#a-comprehensive-review-of-driver-behavior-analysis-utilizing-smartphones",
    "title": "Introduction",
    "section": "",
    "text": "(Chan et al. 2019)\nThis is a link to source\n\n\nHuman factors are the primary catalyst for traffic accidents. Among different factors, fatigue, distraction, drunkenness, and/or recklessness are the most common types of abnormal driving behavior that leads to an accident. With technological advances, modern smartphones have the capabilities for driving behavior analysis. There has not yet been a comprehensive review on methodologies utilizing only a smartphone for drowsiness detection and abnormal driver behavior detection. In this paper, different methodologies proposed by different authors are discussed. It includes the sensing schemes, detection algorithms, and their corresponding accuracy and limitations. Challenges and possible solutions such as integration of the smartphone behavior classification system with the concept of context-aware, mobile crowdsensing, and active steering control are analyzed. The issue of model training and updating on the smartphone and cloud environment is also included.\n\n\n\nThe paper discusses the use of smartphone data as a valuable resource for analyzing driver behavior. It reviews various methodologies proposed by different authors for detecting abnormal driving patterns. While smartphone-based systems offer advantages over telematics boxes, there are significant challenges to consider for accurate driver behavior classification.\nKey challenges include the impact of varying light conditions on vision-based methodologies and the need to eliminate noise and external factors in sensor-based approaches, such as road conditions and vehicle components. Future research should address these issues. Additionally, eliminating the requirement for mounting the smartphone in a fixed position would enhance flexibility and convenience for drivers.\nThe paper also explores potential solutions, such as integrating smartphone-based driver behavior analysis with context-awareness, crowdsensing, and steering control, as well as mobile or cloud-based algorithm training and updates. While these solutions may not be perfect, they have the potential to address some of the challenges, suggesting that there is room for further improvement in smartphone-based behavior analysis systems."
  },
  {
    "objectID": "Introduction.html#questions",
    "href": "Introduction.html#questions",
    "title": "Introduction",
    "section": "",
    "text": "How can we accurately measure and quantify aggressive driving behavior using telematics data?\nWhat are the most effective strategies for detecting and reducing instances of distracted driving?\nCan machine learning algorithms predict the likelihood of a driver engaging in risky behaviors based on historical data?\nHow does driver behavior vary across different age groups, genders, and experience levels, and what implications does this have for road safety?\nHow can data analytics be used to design targeted driver training programs that address specific behavioral issues?\n\n\n\n\n\nHow do external factors like road conditions and traffic congestion influence driver behavior, and how can this information be used to improve road safety?\nHow do weather conditions, such as rain, snow, and fog, affect road safety, and what strategies can be employed to mitigate their impact?\nHow can data science be used to analyze the effectiveness of road safety policies, such as seat belt laws and speed limits, in reducing accidents?\nWhat role does human psychology play in road safety, and how can behavioral insights be used to design safer road systems?\n\n\n\n\n\nHow does driver behavior impact fuel efficiency, and how can eco-driving habits be promoted to reduce emissions and fuel consumption?\nHow can driver behavior analysis be used to optimize traffic flow and reduce congestion in urban areas?\nHow can driver behavior data be anonymized and protected to ensure privacy while still extracting valuable insights for road safety?"
  },
  {
    "objectID": "ts.html",
    "href": "ts.html",
    "title": "Grouping by State",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\naccident = pd.read_csv('Data/FARS2021NationalCSV/accident.csv', encoding='ISO-8859-1')\nevent = pd.read_csv('Data/FARS2021NationalCSV/cevent.csv', encoding='ISO-8859-1')\naccident.columns = accident.columns.str.strip()\nevent.columns = event.columns.str.strip()\naccident_columns_to_drop = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n                   36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n                   64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n\n# Drop the specified columns by index\naccident = accident.drop(accident.columns[accident_columns_to_drop], axis=1)\n\nevent_columns_to_drop = [9, 10, 11, 12]\nevent = event.drop(event.columns[event_columns_to_drop], axis = 1)\ndf = pd.merge(accident, event, on='ST_CASE', how='inner')\ndf\n\n\n\n\n\n\n\n\nSTATE_x\nSTATENAME_x\nST_CASE\nDAYNAME\nDAY_WEEKNAME\nHOUR\nMINUTE\nSTATE_y\nSTATENAME_y\nEVENTNUM\nVNUMBER1\nAOI1\nAOI1NAME\nSOE\nSOENAME\n\n\n\n\n0\n1\nAlabama\n10001\n12\nFriday\n22\n10\n1\nAlabama\n1\n1\n12\n12 Clock Point\n12\nMotor Vehicle In-Transport\n\n\n1\n1\nAlabama\n10002\n11\nThursday\n18\n0\n1\nAlabama\n1\n1\n55\nNon-Harmful Event\n64\nRan Off Roadway - Left\n\n\n2\n1\nAlabama\n10002\n11\nThursday\n18\n0\n1\nAlabama\n2\n1\n11\n11 Clock Point\n25\nConcrete Traffic Barrier\n\n\n3\n1\nAlabama\n10002\n11\nThursday\n18\n0\n1\nAlabama\n3\n1\n55\nNon-Harmful Event\n69\nRe-entering Roadway\n\n\n4\n1\nAlabama\n10002\n11\nThursday\n18\n0\n1\nAlabama\n4\n1\n55\nNon-Harmful Event\n63\nRan Off Roadway - Right\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n112720\n56\nWyoming\n560102\n15\nWednesday\n10\n34\n56\nWyoming\n1\n1\n55\nNon-Harmful Event\n63\nRan Off Roadway - Right\n\n\n112721\n56\nWyoming\n560102\n15\nWednesday\n10\n34\n56\nWyoming\n2\n1\n0\nNon-Collision\n1\nRollover/Overturn\n\n\n112722\n56\nWyoming\n560103\n19\nSunday\n17\n9\n56\nWyoming\n1\n1\n12\n12 Clock Point\n8\nPedestrian\n\n\n112723\n56\nWyoming\n560104\n20\nMonday\n6\n30\n56\nWyoming\n1\n1\n55\nNon-Harmful Event\n68\nCross Centerline\n\n\n112724\n56\nWyoming\n560104\n20\nMonday\n6\n30\n56\nWyoming\n2\n1\n12\n12 Clock Point\n12\nMotor Vehicle In-Transport\n\n\n\n\n112725 rows × 15 columns\nprint(df.shape)\n\n(112725, 15)\ndf = df.drop(columns=['STATE_y', 'STATENAME_y'])\nnumerical_vars = df.select_dtypes(include=[np.number])\nnumerical_summary = numerical_vars.describe()\n\n# Calculate variance for numerical variables (since it's not included in the describe method by default)\nvariance = numerical_vars.var()\n\n# Add variance to the summary statistics\nnumerical_summary.loc['variance'] = variance\n\nnumerical_summary\n\n\n\n\n\n\n\n\nSTATE_x\nST_CASE\nDAYNAME\nHOUR\nMINUTE\nEVENTNUM\nVNUMBER1\nAOI1\nSOE\n\n\n\n\ncount\n112725.000000\n1.127250e+05\n112725.000000\n112725.000000\n112725.000000\n112725.000000\n112725.000000\n112725.000000\n112725.000000\n\n\nmean\n27.475697\n2.756416e+05\n15.631076\n13.311040\n29.082990\n2.706480\n1.277764\n34.825301\n35.383633\n\n\nstd\n16.452688\n1.644243e+05\n8.871515\n10.491582\n18.481930\n3.216187\n2.629577\n30.664740\n25.523239\n\n\nmin\n1.000000\n1.000100e+04\n1.000000\n0.000000\n0.000000\n1.000000\n1.000000\n0.000000\n1.000000\n\n\n25%\n12.000000\n1.222640e+05\n8.000000\n7.000000\n14.000000\n1.000000\n1.000000\n12.000000\n12.000000\n\n\n50%\n27.000000\n2.704430e+05\n16.000000\n14.000000\n30.000000\n2.000000\n1.000000\n12.000000\n34.000000\n\n\n75%\n42.000000\n4.207750e+05\n23.000000\n19.000000\n44.000000\n3.000000\n1.000000\n55.000000\n63.000000\n\n\nmax\n56.000000\n5.601040e+05\n31.000000\n99.000000\n99.000000\n134.000000\n130.000000\n99.000000\n99.000000\n\n\nvariance\n270.690950\n2.703535e+10\n78.703777\n110.073297\n341.581745\n10.343860\n6.914676\n940.326272\n651.435726\nsns.set_style(\"whitegrid\")\n\n# Function to create bar plots for categorical variables\ndef plot_categorical_distribution(data, column_name, plot_size=(10, 6), rotation_angle=90):\n    plt.figure(figsize=plot_size)\n    ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette=\"husl\")\n    ax.set_title(f'Crash Distribution Summary of {column_name}', fontsize=15)\n    ax.set_ylabel(column_name, fontsize=12)\n    ax.set_xlabel('Count', fontsize=12)\n    plt.xticks(rotation=rotation_angle)\n    plt.show()\n\n# Plot the distribution of STATENAME_x\nplot_categorical_distribution(df, 'STATENAME_x')\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/2693093590.py:6: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette=\"husl\")\nplot_categorical_distribution(df, 'DAY_WEEKNAME', rotation_angle=0)\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/2693093590.py:6: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette=\"husl\")\nplot_categorical_distribution(df, 'AOI1NAME', plot_size=(10, 8))\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/2693093590.py:6: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette=\"husl\")\n# Calculate the correlation matrix for Step 4\ncorrelation_matrix = numerical_vars.corr()\n\ncorrelation_matrix\nplt.figure(figsize=(10, 8))\n\n# Create a heatmap to visualize the correlation matrix\nax = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\nax.set_title('Correlation Matrix', fontsize=15)\nplt.show()\nplt.figure(figsize=(10, 8))\nax = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\nax.set_title('Correlation Matrix', fontsize=15)\nplt.show()\n# handle the 99 (unknown) values\ndf['HOUR'] = df['HOUR'].astype(str).replace('99', 'Unknown')\n\n# Create a pivot table to count the number of DAY_WEEKNAME and HOUR\nhour_weekday_pivot = pd.pivot_table(df, index='DAY_WEEKNAME', columns='HOUR', aggfunc='size', fill_value=0)\n\n# Order the days of the week\ndays_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nhour_weekday_pivot = hour_weekday_pivot.reindex(days_order)\n\nplt.figure(figsize=(15, 7))\nax = sns.heatmap(hour_weekday_pivot, cmap=\"YlGnBu\", linewidths=.5)\nax.set_title('Number of Entries by Day of the Week and Hour of the Day', fontsize=15)\nax.set_xlabel('Hour of the Day', fontsize=12)\nax.set_ylabel('Day of the Week', fontsize=12)\nplt.show()\nHypothesis Generation Based on the exploratory data analysis we’ve conducted so far, here are some potential hypotheses and questions:\nTime of Day and Incidents:\nHypothesis: There are more incidents in the afternoon and evening compared to other times of the day. Potential Analysis: Investigate if certain types of incidents are more likely to occur during these times. Weekend Driving Behavior:\nHypothesis: Driving behavior during the weekends, especially late at night and in the early morning hours, leads to more incidents. Potential Analysis: Examine the types of incidents that occur during these times and if they are different from weekday incidents. State-wise Distribution:\nQuestion: Why do some states have significantly more incidents recorded in the dataset? Is it due to population, traffic volume, or data collection methods? Potential Analysis: Normalize the data by population or traffic volume to better understand the state-wise distribution. Impact Areas:\nHypothesis: Certain areas of impact, such as the “Non-Harmful Event” and “12 Clock Point”, are more common. Potential Analysis: Investigate the circumstances that lead to these common impact areas. Sequence of Events:\nHypothesis: The “Motor Vehicle In-Transport” event is the most common sequence of events leading to incidents. Potential Analysis: Explore what specific situations or factors contribute to this sequence of events.\nCalifornia and Texas have the highest number of entries, with 11,952 and 11,787 incidents respectively. States like Alaska, District of Columbia, and Rhode Island have the lowest number of entries, all below 150 incidents.\n# Group the data by state and \n# calculate the total number of entries for each state\nstate_group = df.groupby('STATENAME_x').size().sort_values(ascending=False)\n\nstate_group\n\nplt.figure(figsize=(12, 8))\n\n# Create a bar plot for the number of entries by state\nax = sns.barplot(x=state_group.index, y=state_group.values, palette=\"husl\")\n\nax.set_title('Number of Entries by State', fontsize=15)\nax.set_xlabel('State', fontsize=12)\nax.set_ylabel('Number of Entries', fontsize=12)\nplt.xticks(rotation=90)\nplt.show()\n\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/29044026.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.barplot(x=state_group.index, y=state_group.values, palette=\"husl\")"
  },
  {
    "objectID": "ts.html#grouping-by-time-of-the-day",
    "href": "ts.html#grouping-by-time-of-the-day",
    "title": "Grouping by State",
    "section": "Grouping by Time of the Day",
    "text": "Grouping by Time of the Day\n\n# Convert HOUR back to numeric, treating \"Unknown\" as a missing value\ndf['HOUR'] = pd.to_numeric(df['HOUR'], errors='coerce')\n\n# Define a function to categorize the time of day\ndef categorize_time_of_day(hour):\n    if pd.isna(hour):\n        return \"Unknown\"\n    elif 6 &lt;= hour &lt; 12:\n        return \"Morning\"\n    elif 12 &lt;= hour &lt; 18:\n        return \"Afternoon\"\n    elif 18 &lt;= hour &lt; 24:\n        return \"Evening\"\n    else:\n        return \"Night\"\n\n# Apply the function to create a new variable \"TIME_OF_DAY\"\ndf['TIME_OF_DAY'] = df['HOUR'].apply(categorize_time_of_day)\n\n# Group the data by \"TIME_OF_DAY\" and calculate the total number of entries for each time segment\ntime_of_day_group = df.groupby('TIME_OF_DAY').size().sort_index()\n\ntime_of_day_group\n\n# Set the size of the plot\nplt.figure(figsize=(10, 6))\n\n# Create a bar plot for the number of entries by time of day\nax = sns.barplot(x=time_of_day_group.index, y=time_of_day_group.values, palette=\"husl\")\n\n# Set the title and labels\nax.set_title('Number of Entries by Time of Day', fontsize=15)\nax.set_xlabel('Time of Day', fontsize=12)\nax.set_ylabel('Number of Entries', fontsize=12)\n\n# Show the plot\nplt.show()\n\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3951407133.py:29: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.barplot(x=time_of_day_group.index, y=time_of_day_group.values, palette=\"husl\")\n\n\n\n\n\nStep 7\n\n\nplt.figure(figsize=(15, 10))\nfor i, column in enumerate(numerical_vars.columns, 1):\n    plt.subplot(3, 3, i)\n    sns.boxplot(x=df[column])\n    plt.title(column)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nEVENTNUM, VNUMBER1, AOI1, SOE: These variables have a significant number of high values that could be considered outliers. However, without more context on what these numbers represent, it’s challenging to definitively label them as outliers. To properly handle the potential outliers, we would need additional context on the data and the variables, especially for the ones with coded values (EVENTNUM, VNUMBER1, AOI1, SOE)."
  },
  {
    "objectID": "ts.html#data-cleaning-for-naïve-bayes",
    "href": "ts.html#data-cleaning-for-naïve-bayes",
    "title": "Grouping by State",
    "section": "Data Cleaning for Naïve Bayes",
    "text": "Data Cleaning for Naïve Bayes\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n\n\n\ndata = pd.read_csv('Data/FARS2021NationalCSV/accident.csv', encoding='ISO-8859-1')\n\nNext is to create a new column that calculate the time gap between the notification time and the arrival time\n\n# Define a function to convert hours and minutes into minutes since the start of the day\ndef convert_to_minutes(hour_col, min_col):\n    return hour_col * 60 + min_col\n\n# Convert notification time and arrival time into minutes\ndata['NOT_MINUTES'] = convert_to_minutes(data['NOT_HOUR'], data['NOT_MIN'])\ndata['ARR_MINUTES'] = convert_to_minutes(data['ARR_HOUR'], data['ARR_MIN'])\n\n# Calculate the time gap\ndata['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']\n\n# Handling cases where the time difference is negative due to crossing midnight\n# Assuming that EMS response times will be within a 24-hour period\ndata['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x &lt; 0 else 0))\n\n# Create the binary target variable\ndata['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] &gt; 10).astype(int)\n\n# Display the new columns\ndata[['NOT_HOUR', 'NOT_MIN', 'ARR_HOUR', 'ARR_MIN', 'TIME_DIFF', 'EMS_MORE_THAN_10_MIN']].head()\n\n\n\n\n\n\n\n\nNOT_HOUR\nNOT_MIN\nARR_HOUR\nARR_MIN\nTIME_DIFF\nEMS_MORE_THAN_10_MIN\n\n\n\n\n0\n22\n13\n22\n25\n12\n1\n\n\n1\n99\n99\n19\n9\n-3450\n0\n\n\n2\n9\n29\n9\n40\n11\n1\n\n\n3\n16\n20\n16\n28\n8\n0\n\n\n4\n22\n20\n22\n30\n10\n0\n\n\n\n\n\n\n\nHere we have some rows that notification time is 99, which indicates invalid time, we will remove it first\n\n# Remove records with placeholder values for hours or minutes (assuming '99' is the placeholder value)\nvalid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]\n\nHere, for feature selection, there are many possible factors that affect the time EMS took on the road. In addition, there are many dynamic factors that could possibly affect the time as well, and as of current time, there is no way to record every single one of them. What we can do right now is to record those data that might affect the traffic flow, and applied those as features.\n\n# Redefine the feature selection without the specified columns\nrevised_features = [\n    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', \n    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'\n]\n\n\nselected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')\n\n# Define the target variable\ny_revised = valid_data['EMS_MORE_THAN_10_MIN']\n\n# Split the revised data into training and testing sets\nX_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(\n    selected_features_revised, y_revised, test_size=0.2, random_state=0)\n\nmodel_revised = GaussianNB()\n\n# Train the revised model\nmodel_revised.fit(X_train_revised, y_train_revised)\n\n# Predict on revised test data\ny_pred_revised = model_revised.predict(X_test_revised)\n\ncm_revised = confusion_matrix(y_test_revised, y_pred_revised)\n\naccuracy_revised = accuracy_score(y_test_revised, y_pred_revised)\nreport_revised = classification_report(y_test_revised, y_pred_revised)\n\nprint(accuracy_revised)\nprint(report_revised)\n\n# Plotting the confusion matrix for the revised model\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_revised, annot=True, fmt='d', cmap='Blues', xticklabels=['&lt;=10 min', '&gt;10 min'], yticklabels=['&lt;=10 min', '&gt;10 min'])\nplt.title('Confusion Matrix for Revised EMS Arrival Time Prediction')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\n0.47060544018718925\n              precision    recall  f1-score   support\n\n           0       0.79      0.31      0.45      2354\n           1       0.35      0.82      0.49      1065\n\n    accuracy                           0.47      3419\n   macro avg       0.57      0.56      0.47      3419\nweighted avg       0.65      0.47      0.46      3419\n\n\n\nText(72.72222222222221, 0.5, 'True Label')\n\n\n\n\n\nSo Based on the result, we can tell that\nPrecision for class 0 (EMS took 10 minutes or less): 79%\nRecall for class 0: 31%\nF1-score for class 0: 45%\nPrecision for class 1 (EMS took more than 10 minutes): 35%\nRecall for class 1: 82%\nF1-score for class 1: 49%\nFrom the confusion matrix, The high number of false positives (802) relative to true negatives (346) indicates that the model is overly pessimistic about the EMS response time, often predicting delays where there are none. The model has a better true positive rate, with 893 correctly predicted delays, but this comes at the cost of a high false-positive rate. The false-negative count (196) is lower than the false positives, which suggests that when the model predicts a quick response, it is somewhat more likely to be correct. However, in emergency response situations, even a small number of false negatives can be critical.\nour model is trying to predict whether an emergency medical service (EMS) will take more than 10 minutes to arrive at the scene of an accident.\nThis accuracy tells us what portion of the total predictions made by the model were correct. Our model has an accuracy of approximately 47.06%, which means that about 47 out of every 100 predictions it makes about EMS arrival times are correct. It’s not very high, so the model is not very reliable in its current state.\nPrecision tells us how often the model is correct when it predicts a certain event. For instance, when our model predicts that the EMS will take more than 10 minutes to arrive, it is correct 35% of the time. Conversely, when it predicts that EMS will take 10 minutes or less, it is correct 79% of the time. High precision for a category means that when the model predicts that category, it’s usually right.\nThe F1-score is 49%, and for predictions of 10 minutes or less, it’s 45%. This suggests that the model is slightly better at predicting longer arrival times than shorter ones, but it still isn’t highly accurate in either case.\nGiven the accuracy and the confusion matrix, we see that the model has an accuracy of approximately 47.06% on the test set, which is not very high and is close to random guessing. This could indicate that the model is underfitting. It is too simplistic and not capturing the underlying patterns in the data well enough to make accurate predictions on either the training or the test set.\n\n\n# Load the dataset\npdf_path = ('Data/FARS2021NationalCSV/accident.csv')\ndata = pd.read_csv(pdf_path, encoding='ISO-8859-1')\n\n# Preprocess the data as before\ndata['NOT_MINUTES'] = data['NOT_HOUR'] * 60 + data['NOT_MIN']\ndata['ARR_MINUTES'] = data['ARR_HOUR'] * 60 + data['ARR_MIN']\ndata['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']\ndata['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x &lt; 0 else 0))\ndata['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] &gt; 10).astype(int)\n\n# Remove invalid records (where NOT_HOUR or ARR_HOUR is 99)\nvalid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]\n\n# Redefine the feature selection without the specified columns\nrevised_features = [\n    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', \n    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'\n]\n\n# Prepare the feature matrix and target vector\nselected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')\ny_revised = valid_data['EMS_MORE_THAN_10_MIN']\n\n# Encode categorical variables\nlabel_encoders = {}\nfor column in selected_features_revised.select_dtypes(include=['object']).columns:\n    label_encoders[column] = LabelEncoder()\n    selected_features_revised[column] = label_encoders[column].fit_transform(selected_features_revised[column])\n\n# Split the dataset into train and test sets\nX_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(\n    selected_features_revised, y_revised, test_size=0.2, random_state=0)\n\n# Initialize and train the Gaussian Naive Bayes model\nmodel_revised = GaussianNB()\nmodel_revised.fit(X_train_revised, y_train_revised)\n\n# Predict probabilities for the test set\ny_scores_revised = model_revised.predict_proba(X_test_revised)[:, 1]\n\n# Compute precision-recall pairs for different probability thresholds\nprecision_revised, recall_revised, thresholds_revised = precision_recall_curve(y_test_revised, y_scores_revised)\n\n# Plot the Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall_revised, precision_revised, marker='.', label='Revised Naive Bayes')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Revised Model')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nfpr, tpr, roc_thresholds = roc_curve(y_test_revised, y_scores_revised)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "ts.html#naïve-bayes-for-labeled-text-data",
    "href": "ts.html#naïve-bayes-for-labeled-text-data",
    "title": "Grouping by State",
    "section": "Naïve Bayes for labeled text data",
    "text": "Naïve Bayes for labeled text data\n\nimport requests\nimport json\nimport re\nimport os\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=1\nverbose=True\n\nAPI_KEY='2133663c4ec54af8a9839f0c500203de'\nTOPIC1 = 'Motor vehicle crash'\n\nURLpost1 = {'apiKey': API_KEY,\n            'q': '+'+TOPIC1,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nprint(baseURL)\n\nresponse1 = requests.get(baseURL, URLpost1) \n\nresponse1 = response1.json() \n\nprint(json.dumps(response1, indent=2))\n\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\noutput_file_path = os.path.join(\"data\", f'{timestamp}-topic1-newapi-raw-data-driver-profilling.json')\n\nwith open(output_file_path, 'w') as outfile:\n    json.dump(response1, outfile, indent=4)\n\nwe need to preprocess the data to make sure there’s no noise for feature extraction and feature selection.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nfile_path = (\"Data/labeled_articles_sentiment.csv\")\ndata = pd.read_csv(file_path)\n\ndata['text'] = data['title'] + ' ' + data['description']\n\n# Encode the sentiment column to numerical values\nlabel_encoder = LabelEncoder()\ndata['sentiment_encoded'] = label_encoder.fit_transform(data['sentiment'])\n\n# Split the data into features and target\nX = data['text']\ny = data['sentiment_encoded']\n\n# Perform a train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n\n# Fit and transform the vectorizer on the training data and transform the testing data\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Show the shape of the resulting TF-IDF matrices\nX_train_tfidf.shape, X_test_tfidf.shape\n\n((80, 969), (20, 969))\n\n\nThe TF-IDF vectorization has been applied to both the training and testing text data, resulting in a feature space of 969 terms after limiting to a maximum of 5000 features. There are 80 articles in the training set and 20 in the testing set.\nThe next step is to perform feature selection to find the most relevant features for the Naive Bayes model. However, since the number of features is already quite manageable (969 features), and Naive Bayes handles high-dimensional data well, we might not need to reduce the feature space further. Instead, we’ll proceed with these features and train the Naive Bayes model.\nLet’s train a Multinomial Naive Bayes classifier, which is often used for text classification with features representing the frequency of words. After training the model, we’ll use it to make predictions on the test set and then evaluate the model’s performance.​\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Initialize the Multinomial Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the classifier\nnb_classifier.fit(X_train_tfidf, y_train)\n\n# Predict the labels for the test set\ny_pred = nb_classifier.predict(X_test_tfidf)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\n# Create a confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\naccuracy, report\n\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/huoxingrui/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n(0.5,\n '              precision    recall  f1-score   support\\n\\n    negative       1.00      0.20      0.33         5\\n     neutral       0.00      0.00      0.00         6\\n    positive       0.47      1.00      0.64         9\\n\\n    accuracy                           0.50        20\\n   macro avg       0.49      0.40      0.33        20\\nweighted avg       0.46      0.50      0.37        20\\n')\n\n\nThe Multinomial Naive Bayes classifier has an overall accuracy of 50% on the test set. The classification report and confusion matrix give us a more detailed insight into the performance for each sentiment class:\nNegative Sentiment: The model has a high precision of 100% but a very low recall of 20%, indicating that while the predictions made as negative are all correct, the model fails to identify most of the negative instances. Neutral Sentiment: The model fails to correctly identify any neutral sentiments, as indicated by both precision and recall being 0%. Positive Sentiment: The model has a precision of 47% with a recall of 100%, suggesting that while it identifies all positive instances, it also incorrectly labels some non-positive instances as positive. The confusion matrix visualization shows the distribution of predictions across the actual sentiments. We can see that all negative and neutral sentiments are predominantly classified as positive, which is a sign of bias towards the positive class in the model’s predictions.\nEvaluation Metrics The accuracy metric alone is not sufficient to assess the performance of the Naive Bayes classifier. Precision, recall, and F1-score provide a more comprehensive evaluation. The precision tells us the accuracy of the positive predictions made, recall gives us a measure of the model’s ability to find all the positive instances, and the F1-score is a harmonic mean of precision and recall.\nOverfitting and Underfitting The model does not appear to be overfitting, as overfitting would typically present as a high accuracy on the training set but poor performance on the test set. However, the model might be underfitting since it is overly generalized, leading to poor performance across all metrics.\nModel Performance The model’s performance is not ideal, with a low F1-score for negative and neutral classes and a moderate F1-score for the positive class. This suggests that while the model can predict positive sentiments relatively well, it struggles to distinguish between negative and neutral sentiments.\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import average_precision_score\n\n# Binarize the output labels for the multi-class case\ny_test_binarized = label_binarize(y_test, classes=range(len(label_encoder.classes_)))\n\n# Initialize a dictionary to hold the precision-recall curves for each class\nprecision_recall_curve_dict = {}\n\n# Calculate the precision-recall curve and average precision for each class\nfor i, class_label in enumerate(label_encoder.classes_):\n    class_precisions, class_recalls, class_thresholds = precision_recall_curve(y_test_binarized[:, i], y_scores[:, i])\n    precision_recall_curve_dict[class_label] = (class_precisions, class_recalls)\n    avg_precision = average_precision_score(y_test_binarized[:, i], y_scores[:, i])\n    print(f\"Average precision-recall score for class '{class_label}': {avg_precision:.2f}\")\n\n# Plot the precision-recall curve for each class\nplt.figure(figsize=(10, 6))\n\nfor class_label, (class_precisions, class_recalls) in precision_recall_curve_dict.items():\n    plt.plot(class_recalls, class_precisions, lw=2, label=f'Precision-Recall curve of class {class_label}')\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall curve per class\")\nplt.legend(loc=\"best\")\nplt.show()\n\nAverage precision-recall score for class 'negative': 0.56\nAverage precision-recall score for class 'neutral': 0.45\nAverage precision-recall score for class 'positive': 0.48\n\n\n\n\n\n\n# Let's check the actual counts for each sentiment in y_train and y_pred\ntrain_counts = pd.Series(y_train).value_counts().sort_index()\npred_counts = pd.Series(y_pred).value_counts().sort_index()\n\n# Now we plot the actual counts to verify the distribution\nfig, ax = plt.subplots(1, 2, figsize=(14, 7), sharey=True)\n\nsns.barplot(x=train_counts.index, y=train_counts.values, ax=ax[0], palette=\"viridis\")\nax[0].set_title('Distribution of Actual Sentiments (Training Set)')\nax[0].set_xlabel('Sentiment')\nax[0].set_ylabel('Count')\nax[0].set_xticklabels(label_encoder.inverse_transform(train_counts.index))\n\nsns.barplot(x=pred_counts.index, y=pred_counts.values, ax=ax[1], palette=\"viridis\")\nax[1].set_title('Distribution of Predicted Sentiments (Test Set)')\nax[1].set_xlabel('Sentiment')\nax[1].set_xticklabels(label_encoder.inverse_transform(pred_counts.index))\n\nplt.tight_layout()\nplt.show()\n\n# Show actual counts for verification\ntrain_counts, pred_counts\n\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3963520661.py:8: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=train_counts.index, y=train_counts.values, ax=ax[0], palette=\"viridis\")\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3963520661.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[0].set_xticklabels(label_encoder.inverse_transform(train_counts.index))\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3963520661.py:14: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=pred_counts.index, y=pred_counts.values, ax=ax[1], palette=\"viridis\")\n/var/folders/h3/qs6jt3s124g0xq78192txdnr0000gn/T/ipykernel_5584/3963520661.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[1].set_xticklabels(label_encoder.inverse_transform(pred_counts.index))\n\n\n\n\n\n(sentiment_encoded\n 0    20\n 1    12\n 2    48\n Name: count, dtype: int64,\n 0     1\n 2    19\n Name: count, dtype: int64)\n\n\nThe visualizations confirm the earlier discussions about the model’s performance and suggest potential avenues for improvement, such as addressing the class imbalance or exploring more sophisticated models and features.​"
  },
  {
    "objectID": "Decision Trees.html",
    "href": "Decision Trees.html",
    "title": "Decision Tree",
    "section": "",
    "text": "What is Decision Tree?\nA decision tree is a flowchart-like structure used in decision making and data mining. It’s a popular tool in machine learning, statistics, and information theory for modeling decisions and their possible consequences.\n\nIf you happened to watch Formula 1…\n Source\nIn this plot, each question helps you make a decision, and you follow the path that matches the answers you get. It’s like a flowchart of choices that leads you to a final decision. Decision trees work similarly in various fields, helping to make choices based on a series of yes-or-no questions and criteria.\n Source\n\n\n\nBaseline Decision Tree Analysis\nThe baseline model returned as follows: Classification Report:\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n1\n0.91\n0.98\n0.94\n\n\n2\n0.08\n0.02\n0.03\n\n\n3\n0.03\n0.01\n0.02\n\n\n4\n0.00\n0.00\n0.00\n\n\n5\n0.00\n0.00\n0.00\n\n\n6\n0.00\n0.00\n0.00\n\n\naccuracy\n\n\n0.89\n\n\nmacro avg\n0.17\n0.17\n0.16\n\n\nweighted avg\n0.83\n0.89\n0.86\n\n\n\nConfusion Matrix:"
  }
]