---
jupyter: python3
---

# Data cleaning in text data
First I use NewsAPI to get text data from NewsAPI on different key words in terms of driver behavior analysis

The key words I used are: driving behavior, distracted driving, driver risk assessment, driver profilling

```{.python .hide}
import requests
import json
import re
import os
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

baseURL = "https://newsapi.org/v2/everything?"
total_requests=1
verbose=True

API_KEY='2133663c4ec54af8a9839f0c500203de'
TOPIC1 = 'driver profiling'

URLpost1 = {'apiKey': API_KEY,
            'q': '+'+TOPIC1,
            'sortBy': 'relevancy',
            'totalRequests': 1}

print(baseURL)

response1 = requests.get(baseURL, URLpost1) 

response1 = response1.json() 

print(json.dumps(response1, indent=2))

from datetime import datetime
timestamp = datetime.now().strftime("%Y-%m-%d-H%H-M%M-S%S")

output_file_path = os.path.join("data", f'{timestamp}-topic1-newapi-raw-data-driver-profilling.json')

with open(output_file_path, 'w') as outfile:
    json.dump(response1, outfile, indent=4)
```


# Data Cleaning for EDA

```{.python .hide}

import csv
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

accident = pd.read_csv('Data/FARS2021NationalCSV/accident.csv', encoding='ISO-8859-1')
event = pd.read_csv('Data/FARS2021NationalCSV/cevent.csv', encoding='ISO-8859-1')
accident.columns = accident.columns.str.strip()
event.columns = event.columns.str.strip()

accident_columns_to_drop = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
                   36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,
                   64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]

# Drop the specified columns by index
accident = accident.drop(accident.columns[accident_columns_to_drop], axis=1)

event_columns_to_drop = [9, 10, 11, 12]
event = event.drop(event.columns[event_columns_to_drop], axis = 1)

df = pd.merge(accident, event, on='ST_CASE', how='inner')
df = df.drop(columns=['STATE_y', 'STATENAME_y'])


numerical_vars = df.select_dtypes(include=[np.number])
numerical_summary = numerical_vars.describe()

# Calculate variance for numerical variables (since it's not included in the describe method by default)
variance = numerical_vars.var()

# Add variance to the summary statistics
numerical_summary.loc['variance'] = variance

numerical_summary

sns.set_style("whitegrid")

# Function to create bar plots for categorical variables
def plot_categorical_distribution(data, column_name, plot_size=(10, 6), rotation_angle=90):
    plt.figure(figsize=plot_size)
    ax = sns.countplot(data=data, y=column_name, order=data[column_name].value_counts().index,  palette="husl")
    ax.set_title(f'Crash Distribution Summary of {column_name}', fontsize=15)
    ax.set_ylabel(column_name, fontsize=12)
    ax.set_xlabel('Count', fontsize=12)
    plt.xticks(rotation=rotation_angle)
    plt.show()

# Plot the distribution of STATENAME_x
plot_categorical_distribution(df, 'STATENAME_x')

plot_categorical_distribution(df, 'DAY_WEEKNAME', rotation_angle=0)


plot_categorical_distribution(df, 'AOI1NAME', plot_size=(10, 8))


# Calculate the correlation matrix for the numerical variables Step 4
correlation_matrix = numerical_vars.corr()

correlation_matrix

# Set the size of the plot
plt.figure(figsize=(10, 8))

# Create a heatmap to visualize the correlation matrix
ax = sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)

# Set the title and show the plot
ax.set_title('Correlation Matrix', fontsize=15)
plt.show()


# Convert HOUR from numeric to categorical to better handle the 99 (unknown) values
df['HOUR'] = df['HOUR'].astype(str).replace('99', 'Unknown')

# Create a pivot table to count the number of entries for each combination of DAY_WEEKNAME and HOUR
hour_weekday_pivot = pd.pivot_table(df, index='DAY_WEEKNAME', columns='HOUR', aggfunc='size', fill_value=0)

# Order the days of the week
days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
hour_weekday_pivot = hour_weekday_pivot.reindex(days_order)

# Create the heatmap
plt.figure(figsize=(15, 7))
ax = sns.heatmap(hour_weekday_pivot, cmap="YlGnBu", linewidths=.5)

# Set the title and labels
ax.set_title('Number of Entries by Day of the Week and Hour of the Day', fontsize=15)
ax.set_xlabel('Hour of the Day', fontsize=12)
ax.set_ylabel('Day of the Week', fontsize=12)

plt.show()

# Group the data by state and calculate the total number of entries for each state
state_group = df.groupby('STATENAME_x').size().sort_values(ascending=False)

state_group

# Set the size of the plot
plt.figure(figsize=(12, 8))

# Create a bar plot for the number of entries by state
ax = sns.barplot(x=state_group.index, y=state_group.values, palette="husl")

# Set the title and labels
ax.set_title('Number of Entries by State', fontsize=15)
ax.set_xlabel('State', fontsize=12)
ax.set_ylabel('Number of Entries', fontsize=12)
plt.xticks(rotation=90)

# Show the plot
plt.show()

# Convert HOUR back to numeric, treating "Unknown" as a missing value
df['HOUR'] = pd.to_numeric(df['HOUR'], errors='coerce')

# Define a function to categorize the time of day
def categorize_time_of_day(hour):
    if pd.isna(hour):
        return "Unknown"
    elif 6 <= hour < 12:
        return "Morning"
    elif 12 <= hour < 18:
        return "Afternoon"
    elif 18 <= hour < 24:
        return "Evening"
    else:
        return "Night"

# Apply the function to create a new variable "TIME_OF_DAY"
df['TIME_OF_DAY'] = df['HOUR'].apply(categorize_time_of_day)

# Group the data by "TIME_OF_DAY" and calculate the total number of entries for each time segment
time_of_day_group = df.groupby('TIME_OF_DAY').size().sort_index()

time_of_day_group

# Set the size of the plot
plt.figure(figsize=(10, 6))

# Create a bar plot for the number of entries by time of day
ax = sns.barplot(x=time_of_day_group.index, y=time_of_day_group.values, palette="husl")

# Set the title and labels
ax.set_title('Number of Entries by Time of Day', fontsize=15)
ax.set_xlabel('Time of Day', fontsize=12)
ax.set_ylabel('Number of Entries', fontsize=12)

# Show the plot
plt.show()



# Set the size of the plots
plt.figure(figsize=(15, 10))

# Create box plots for the numerical variables
for i, column in enumerate(numerical_vars.columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=df[column])
    plt.title(column)

plt.tight_layout()
plt.show()

```

# Data Cleaning in NB Progress

## Record Data

```{.python .hide}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc



data = pd.read_csv('Data/FARS2021NationalCSV/accident.csv', encoding='ISO-8859-1')

# Define a function to convert hours and minutes into minutes since the start of the day
def convert_to_minutes(hour_col, min_col):
    return hour_col * 60 + min_col

# Convert notification time and arrival time into minutes
data['NOT_MINUTES'] = convert_to_minutes(data['NOT_HOUR'], data['NOT_MIN'])
data['ARR_MINUTES'] = convert_to_minutes(data['ARR_HOUR'], data['ARR_MIN'])

# Calculate the time gap
data['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']

# Handling cases where the time difference is negative due to crossing midnight
# Assuming that EMS response times will be within a 24-hour period
data['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x < 0 else 0))

# Create the binary target variable
data['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] > 10).astype(int)

# Display the new columns
data[['NOT_HOUR', 'NOT_MIN', 'ARR_HOUR', 'ARR_MIN', 'TIME_DIFF', 'EMS_MORE_THAN_10_MIN']].head()

valid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]

# Redefine the feature selection without the specified columns
revised_features = [
    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', 
    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'
]


selected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')

# Define the target variable
y_revised = valid_data['EMS_MORE_THAN_10_MIN']

# Split the revised data into training and testing sets
X_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(
    selected_features_revised, y_revised, test_size=0.2, random_state=0)

model_revised = GaussianNB()

# Train the revised model
model_revised.fit(X_train_revised, y_train_revised)

# Predict on revised test data
y_pred_revised = model_revised.predict(X_test_revised)

cm_revised = confusion_matrix(y_test_revised, y_pred_revised)

accuracy_revised = accuracy_score(y_test_revised, y_pred_revised)
report_revised = classification_report(y_test_revised, y_pred_revised)

print(accuracy_revised)
print(report_revised)

# Plotting the confusion matrix for the revised model
plt.figure(figsize=(8, 6))
sns.heatmap(cm_revised, annot=True, fmt='d', cmap='Blues', xticklabels=['<=10 min', '>10 min'], yticklabels=['<=10 min', '>10 min'])
plt.title('Confusion Matrix for Revised EMS Arrival Time Prediction')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')


# Load the dataset
pdf_path = ('Data/FARS2021NationalCSV/accident.csv')
data = pd.read_csv(pdf_path, encoding='ISO-8859-1')

# Preprocess the data as before
data['NOT_MINUTES'] = data['NOT_HOUR'] * 60 + data['NOT_MIN']
data['ARR_MINUTES'] = data['ARR_HOUR'] * 60 + data['ARR_MIN']
data['TIME_DIFF'] = data['ARR_MINUTES'] - data['NOT_MINUTES']
data['TIME_DIFF'] = data['TIME_DIFF'].apply(lambda x: x + (1440 if x < 0 else 0))
data['EMS_MORE_THAN_10_MIN'] = (data['TIME_DIFF'] > 10).astype(int)

# Remove invalid records (where NOT_HOUR or ARR_HOUR is 99)
valid_data = data[(data['NOT_HOUR'] != 99) & (data['ARR_HOUR'] != 99)]

# Redefine the feature selection without the specified columns
revised_features = [
    'MONTH', 'DAY_WEEK', 'HOUR', 'MINUTE', 'ROUTE', 'TYP_INT', 
    'WRK_ZONE', 'REL_ROAD', 'LGT_COND', 'WEATHER'
]

# Prepare the feature matrix and target vector
selected_features_revised = valid_data[revised_features].dropna(axis=1, how='any')
y_revised = valid_data['EMS_MORE_THAN_10_MIN']

# Encode categorical variables
label_encoders = {}
for column in selected_features_revised.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    selected_features_revised[column] = label_encoders[column].fit_transform(selected_features_revised[column])

# Split the dataset into train and test sets
X_train_revised, X_test_revised, y_train_revised, y_test_revised = train_test_split(
    selected_features_revised, y_revised, test_size=0.2, random_state=0)

# Initialize and train the Gaussian Naive Bayes model
model_revised = GaussianNB()
model_revised.fit(X_train_revised, y_train_revised)

# Predict probabilities for the test set
y_scores_revised = model_revised.predict_proba(X_test_revised)[:, 1]

# Compute precision-recall pairs for different probability thresholds
precision_revised, recall_revised, thresholds_revised = precision_recall_curve(y_test_revised, y_scores_revised)

# Plot the Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall_revised, precision_revised, marker='.', label='Revised Naive Bayes')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Revised Model')
plt.legend()
plt.grid(True)
plt.show()


fpr, tpr, roc_thresholds = roc_curve(y_test_revised, y_scores_revised)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

```

## Text Data

```{.python .hide}

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

file_path = ("Data/labeled_articles_sentiment.csv")
data = pd.read_csv(file_path)

data['text'] = data['title'] + ' ' + data['description']

# Encode the sentiment column to numerical values
label_encoder = LabelEncoder()
data['sentiment_encoded'] = label_encoder.fit_transform(data['sentiment'])

# Split the data into features and target
X = data['text']
y = data['sentiment_encoded']

# Perform a train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=74)

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)

# Fit and transform the vectorizer on the training data and transform the testing data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Show the shape of the resulting TF-IDF matrices
X_train_tfidf.shape, X_test_tfidf.shape



from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize the Multinomial Naive Bayes classifier
nb_classifier = MultinomialNB()

# Train the classifier
nb_classifier.fit(X_train_tfidf, y_train)

# Predict the labels for the test set
y_pred = nb_classifier.predict(X_test_tfidf)

# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)

# Generate a classification report
report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)

# Create a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

accuracy, report

from sklearn.metrics import precision_recall_curve
from sklearn.preprocessing import label_binarize
from sklearn.metrics import average_precision_score

# Binarize the output labels for the multi-class case
y_test_binarized = label_binarize(y_test, classes=range(len(label_encoder.classes_)))

# Initialize a dictionary to hold the precision-recall curves for each class
precision_recall_curve_dict = {}

# Calculate the precision-recall curve and average precision for each class
for i, class_label in enumerate(label_encoder.classes_):
    class_precisions, class_recalls, class_thresholds = precision_recall_curve(y_test_binarized[:, i], y_scores[:, i])
    precision_recall_curve_dict[class_label] = (class_precisions, class_recalls)
    avg_precision = average_precision_score(y_test_binarized[:, i], y_scores[:, i])
    print(f"Average precision-recall score for class '{class_label}': {avg_precision:.2f}")

# Plot the precision-recall curve for each class
plt.figure(figsize=(10, 6))

for class_label, (class_precisions, class_recalls) in precision_recall_curve_dict.items():
    plt.plot(class_recalls, class_precisions, lw=2, label=f'Precision-Recall curve of class {class_label}')

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall curve per class")
plt.legend(loc="best")
plt.show()


# Let's check the actual counts for each sentiment in y_train and y_pred
train_counts = pd.Series(y_train).value_counts().sort_index()
pred_counts = pd.Series(y_pred).value_counts().sort_index()

# Now we plot the actual counts to verify the distribution
fig, ax = plt.subplots(1, 2, figsize=(14, 7), sharey=True)

sns.barplot(x=train_counts.index, y=train_counts.values, ax=ax[0], palette="viridis")
ax[0].set_title('Distribution of Actual Sentiments (Training Set)')
ax[0].set_xlabel('Sentiment')
ax[0].set_ylabel('Count')
ax[0].set_xticklabels(label_encoder.inverse_transform(train_counts.index))

sns.barplot(x=pred_counts.index, y=pred_counts.values, ax=ax[1], palette="viridis")
ax[1].set_title('Distribution of Predicted Sentiments (Test Set)')
ax[1].set_xlabel('Sentiment')
ax[1].set_xticklabels(label_encoder.inverse_transform(pred_counts.index))

plt.tight_layout()
plt.show()

# Show actual counts for verification
train_counts, pred_counts

```


# Data Cleaning in Dimensionality Reduction

## PCA

```{.python .hide}
import pandas as pd

accident_df = pd.read_csv('../../FARS2020NationalCSV/accident.csv', encoding = 'ISO-8859-1')
person_df = pd.read_csv('../../FARS2020NationalCSV/person.csv', encoding = 'ISO-8859-1')

merge_df = pd.merge(accident_df, person_df, on = "ST_CASE", how = 'inner')

columns_to_keep = [
    # Driver-Specific Information
    'AGE', 'SEX', 'DRINKING', 'DRUGS', 'ALC_DET', 'ALC_STATUS', 
    'DRUG_DET', 'DSTATUS', 'INJ_SEV', 'PER_TYP', 'DAY_WEEK', 'HOUR_x', 'WEATHER', 'LGT_COND','STATE_x'
]

# Keeping only the selected columns and dropping the rest
merge_df = merge_df[columns_to_keep]


from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt
data = merge_df
scaler = StandardScaler()
scaled_pca_data = scaler.fit_transform(data)

# Re-applying PCA
pca = PCA()
pca_transformed_data = pca.fit_transform(scaled_pca_data)

# Explained variance ratio for each principal component
explained_variance_ratio = pca.explained_variance_ratio_

# Cumulative explained variance
cumulative_explained_variance = np.cumsum(explained_variance_ratio)

# Displaying the first few principal components' explained variance
explained_variance_ratio, cumulative_explained_variance
plt.plot(cumulative_explained_variance, marker='o')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance by PCA Components')
plt.grid(True)
plt.show()

pc1 = pca_transformed_data[:, 0]  
pc2 = pca_transformed_data[:, 1]  


plt.figure(figsize=(12, 10))
plt.scatter(pc1, pc2, alpha=0.5)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA - First Two Principal Components')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

def biplot(score, coeff, labels=None):
    xs = score[:,0]
    ys = score[:,1]
    n = coeff.shape[0]
    scalex = 1.0/(xs.max() - xs.min())
    scaley = 1.0/(ys.max() - ys.min())
    
    plt.scatter(xs * scalex, ys * scaley, s=5)
    for i in range(n):
        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color='r', alpha=0.5)
        if labels is None:
            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, "Var"+str(i+1), color='black', ha='center', va='center')
        else:
            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color='black', ha='center', va='center')

    plt.xlabel("PC{}".format(1))
    plt.ylabel("PC{}".format(2))
    plt.grid()

# PCA scores (the transformed data points)
pca_scores = pca_transformed_data

# PCA loadings (the contributions of the original variables to the components)
pca_loadings = pca.components_

plt.figure(figsize=(12,12))
biplot(pca_scores, pca_loadings.transpose(), labels=data.columns)
plt.show()
```


## t-SNE

```{.python .hide}
import pandas as pd
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

subset_data = data[['AGE', 'SEX', 'DRINKING', 'DRUGS']]

imputer = SimpleImputer(strategy='median')
df_imputed = imputer.fit_transform(subset_data)

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_imputed)
perplexities = [5, 20, 40]

plt.figure(figsize=(12,12))  # Set the figure size for the subplot layout

for i, perp in enumerate(perplexities, 1):
    tsne = TSNE(n_components=2, perplexity=perp, n_iter=300, random_state=74)
    tsne_results = tsne.fit_transform(df_scaled)
    
    plt.subplot(1, len(perplexities), i)
    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5, c= subset_data['SEX'], cmap='viridis')
    plt.title(f't-SNE with Perplexity = {perp}')
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.colorbar(scatter, label='SEX')

plt.tight_layout()
plt.show()
```


# Clustering Code

## Rough Clean

```{.python .hide}
import pandas as pd

accident_df = pd.read_csv('../../FARS2020NationalCSV/accident.csv', encoding = 'ISO-8859-1')
vehicle_df = pd.read_csv('../../FARS2020NationalCSV/vehicle.csv', encoding = 'ISO-8859-1')
accident_df = accident_df[['WEATHER', 'ST_CASE','STATE', 'LGT_COND', 'FATALS']]
vehicle_df = vehicle_df[['ST_CASE','DR_DRINK', 'DR_ZIP', 'HIT_RUN', 'L_STATE', 'MAKENAME', 'MAK_MODNAME', 'MOD_YEAR', 'VSURCOND', 'VPICMAKENAME', 'VPICMODELNAME']]
merge_df = pd.merge(accident_df, vehicle_df, on = "ST_CASE", how = 'inner')
merge_df = merge_df[(merge_df['WEATHER'] <= 90) & (merge_df['VSURCOND'] <= 90) & (merge_df['MOD_YEAR'] <= 3000)]
output_file_path = 'Data/clustering_data.csv'
merge_df.to_csv(output_file_path)
output_file_path
```

## K-Means Clustering

### Elbow Method

```{.python .hide}
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# Preparing data for K-means clustering
# Selecting only numerical features relevant for environmental analysis
kmeans_data = merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']]

# Using the Elbow Method to find the optimal number of clusters
sse = []  # Sum of squared distances
for k in range(1, 11):  # Testing for 1 to 10 clusters
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(kmeans_data)
    sse.append(kmeans.inertia_)  # Inertia: Sum of squared distances of samples to their closest cluster center

# Plotting the Elbow Curve
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), sse, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of squared distances')
plt.xticks(range(1, 11))
plt.show()


import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns


features = merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']]

# Applying K-means clustering with 5 clusters
kmeans = KMeans(n_clusters=5)
clusters = kmeans.fit_predict(features)

# Adding the cluster information to the original dataframe
merge_df['Cluster'] = clusters

# Analyzing the clusters
for i in range(5):  # Looping through 5 clusters
    cluster = merge_df[merge_df['Cluster'] == i]
    print(f"Cluster {i}:")
    print(f"Weather Conditions: {cluster['WEATHER'].mode()[0]}")
    print(f"Light Conditions: {cluster['LGT_COND'].mode()[0]}")
    print(f"Road Surface Conditions: {cluster['VSURCOND'].mode()[0]}")
    print(f"Average Fatalities: {cluster['FATALS'].mean()}")
    print()

# Visualizing the clusters
sns.pairplot(merge_df, vars=['WEATHER', 'LGT_COND', 'VSURCOND'], hue='Cluster', palette='bright')
plt.title('K-Means Clustering with 5 Clusters')
plt.show()
```
### Silhouette

```{.python .hide}
from sklearn.metrics import silhouette_score

# Assuming 'features' is your data prepared for clustering
features = merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']]

silhouette_scores = []
range_n_clusters = list(range(2, 10))  # Example range from 2 to 9 clusters

for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=74)
    clusters = kmeans.fit_predict(features)
    silhouette_avg = silhouette_score(features, clusters)
    silhouette_scores.append(silhouette_avg)
    print(f"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}")

# Plotting the silhouette scores
plt.figure(figsize=(10, 6))
plt.plot(range_n_clusters, silhouette_scores, marker='o')
plt.title('Silhouette Method For Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('Average Silhouette Score')
plt.xticks(range_n_clusters)
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Applying K-means clustering with 2 clusters generated by silhouette method
kmeans = KMeans(n_clusters=9)
clusters = kmeans.fit_predict(features)

# Adding the cluster information to the original dataframe
merge_df['Cluster'] = clusters

# Analyzing the clusters
for i in range(9):  # Looping through 2 clusters
    cluster = merge_df[merge_df['Cluster'] == i]
    print(f"Cluster {i}:")
    print(f"Weather Conditions: {cluster['WEATHER'].mode()[0]}")
    print(f"Light Conditions: {cluster['LGT_COND'].mode()[0]}")
    print(f"Road Surface Conditions: {cluster['VSURCOND'].mode()[0]}")
    print(f"Average Fatalities: {cluster['FATALS'].mean()}")
    print()

# Visualizing the clusters
sns.pairplot(merge_df, vars=['WEATHER', 'LGT_COND', 'VSURCOND'], hue='Cluster', palette='bright')
plt.title('K-Means Clustering with 2 Clusters')
plt.show()
```
## DBSCAN

```{.python .hide}

import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
features_standardized = scaler.fit_transform(features)

dbscan = DBSCAN(eps=0.5, min_samples=500)
clusters = dbscan.fit_predict(features_standardized)

# Add the cluster labels to your dataframe
merge_df['Cluster'] = clusters

# Check the number of points in each cluster
cluster_counts = merge_df['Cluster'].value_counts()
print(cluster_counts)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Reduce the data to two dimensions using PCA for visualization purposes
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features_standardized)

# Create a scatter plot of the PCA-reduced data color-coded by cluster label
plt.figure(figsize=(10, 10))
unique_labels = np.unique(clusters)
# Generate a color map similar to plt.cm.Spectral but without black
colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))

for k, col in zip(unique_labels, colors):
    if k == -1:
        # Skip noise points, they won't be plotted
        continue

    class_member_mask = (clusters == k)

    xy = features_pca[class_member_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)

plt.title('DBSCAN clustering (without noise)')
plt.xlabel('PCA Feature 1')
plt.ylabel('PCA Feature 2')
plt.show()


from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Standardize the features before applying DBSCAN
scaler = StandardScaler()
features_scaled = scaler.fit_transform(merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']])

# Apply DBSCAN to the standardized features
# Note: You'll need to tune these parameters
dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')
clusters = dbscan.fit_predict(features_scaled)

# Add cluster info to the original DataFrame
merge_df['Cluster'] = clusters

# Reduce dimensions for visualization
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features_scaled)

# Visualize the clusters
unique_labels = set(clusters)
for label in unique_labels:
    plt.scatter(features_pca[clusters == label, 0], features_pca[clusters == label, 1], label=label)

plt.title('DBSCAN Clustering')
plt.xlabel('PCA Feature 1')
plt.ylabel('PCA Feature 2')
plt.show()

```

## Hierarchical Clustering

```{.python .hide}
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.decomposition import PCA
import sys

sys.setrecursionlimit(10000)
scaler = StandardScaler()
features_scaled = scaler.fit_transform(merge_df[['WEATHER', 'LGT_COND', 'VSURCOND']])

# Performing hierarchical clustering
Z = linkage(features_scaled, method='ward')

# Plotting the dendrogram
plt.figure(figsize=(12, 8))
dendrogram(Z, truncate_mode='lastp', p=50, show_leaf_counts=True)

plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Index')
plt.ylabel('Ward\'s Distance')
plt.show()

# To cut the dendrogram and get the cluster labels, use the following:
from scipy.cluster.hierarchy import fcluster
k = 9  # for example, if we decide on 9 clusters
cluster_labels = fcluster(Z, k, criterion='maxclust')

# Adding the cluster information to the original dataframe
merge_df['Cluster'] = cluster_labels

# Optionally reduce dimensions for visualization
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features_scaled)

# Visualize the clusters
plt.figure(figsize=(8, 6))
plt.scatter(features_pca[:, 0], features_pca[:, 1], c=merge_df['Cluster'], cmap='viridis')
plt.title('Hierarchical Clustering Results')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
```

## Decision Tree
```{.python .hide}
import pandas as pd


data = pd.read_csv("Data/clustering_data.csv")

# Calculate the distribution of class labels
class_distribution = data['FATALS'].value_counts(normalize=True) * 100

print(class_distribution)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

data = pd.read_csv("Data/clustering_data.csv")

# Select the relevant features and target variable
features = data[['MAKENAME', 'MOD_YEAR', 'WEATHER', 'LGT_COND', 'DR_DRINK', 'HIT_RUN']]
target = data['FATALS']

# Encoding categorical variables
label_encoders = {}
for column in ['MAKENAME', 'WEATHER', 'LGT_COND']:
    le = LabelEncoder()
    features[column] = le.fit_transform(features[column])
    label_encoders[column] = le

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=74)

# Creating and training a baseline decision tree classifier
dt_classifier = DecisionTreeClassifier(random_state=74)
dt_classifier.fit(X_train, y_train)

# Predicting using the baseline decision tree
y_pred = dt_classifier.predict(X_test)

# Evaluating the baseline decision tree
class_report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print the classification report and confusion matrix
print("Classification Report:\n", class_report)
print("Confusion Matrix:\n", conf_matrix)


import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

# Assuming the decision tree model is stored in `dt_classifier`
plt.figure(figsize=(20,10))  # Adjust the size as needed
plot_tree(dt_classifier, 
          filled=True, 
          rounded=True, 
          class_names=True, 
          feature_names=['MAKENAME', 'MOD_YEAR', 'WEATHER', 'LGT_COND', 'DR_DRINK', 'HIT_RUN'],
          max_depth=5)  
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Creating and training a Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=74, n_estimators=100)  # You can adjust the number of trees with n_estimators
rf_classifier.fit(X_train, y_train)

# Predicting using the Random Forest
rf_predictions = rf_classifier.predict(X_test)

# Evaluating the Random Forest
rf_class_report = classification_report(y_test, rf_predictions)
rf_conf_matrix = confusion_matrix(y_test, rf_predictions)
rf_feature_importances = rf_classifier.feature_importances_

# Print the evaluation metrics and feature importances
print("Random Forest Classification Report:\n", rf_class_report)
print("Random Forest Confusion Matrix:\n", rf_conf_matrix)
print("Feature Importances:\n", rf_feature_importances)


import matplotlib.pyplot as plt
from sklearn.tree import plot_tree


tree_in_rf = rf_classifier.estimators_[0]

# Visualizing the selected tree
plt.figure(figsize=(20,10))  # Adjust the size as needed
plot_tree(tree_in_rf, 
          filled=True, 
          rounded=True, 
          feature_names=['MAKENAME', 'MOD_YEAR', 'WEATHER', 'LGT_COND', 'DR_DRINK', 'HIT_RUN'],
          max_depth=5,  
          class_names=True)
plt.show()


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint as sp_randint

# Load the data
data = pd.read_csv("Data/clustering_data.csv")

# Select the relevant features and target variable
features = data[['MAKENAME', 'MOD_YEAR', 'WEATHER', 'LGT_COND', 'DR_DRINK', 'HIT_RUN']]
target = data['FATALS']

# Encoding categorical variables
label_encoders = {}
for column in ['MAKENAME', 'WEATHER', 'LGT_COND']:
    le = LabelEncoder()
    features[column] = le.fit_transform(features[column])
    label_encoders[column] = le

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=74)

# Parameter grid for Random Search
param_dist = {
    "n_estimators": sp_randint(10, 200),
    "max_depth": sp_randint(3, 20),
    "min_samples_split": sp_randint(2, 11),
    "min_samples_leaf": sp_randint(1, 11),
    "max_features": sp_randint(1, X_train.shape[1]+1)
}

# Setting up RandomizedSearchCV
# Using 10 iterations and 3-fold cross-validation
random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=74),
    param_distributions=param_dist,
    n_iter=10,
    cv=3,
    random_state=74,
    n_jobs=-1  # Using all processors
)

# Running the Random Search
random_search.fit(X_train, y_train)

# Best parameters found
best_params = random_search.best_params_
print("Best Parameters:", best_params)

# Evaluate the best model on the test data
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Initialize the Random Forest Classifier with the best parameters
tuned_rf = RandomForestClassifier(
    n_estimators=112,
    max_depth=9,
    min_samples_split=6,
    min_samples_leaf=8,
    max_features=4,
    random_state=74
)

# Train the model
tuned_rf.fit(X_train, y_train)

# Predict on the test data
y_pred = tuned_rf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", class_report)
print("Confusion Matrix:\n", conf_matrix)


import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

# Assuming 'tuned_rf' is your trained Random Forest model
# Extract one tree from the Random Forest
chosen_tree = tuned_rf.estimators_[0]

# Set the figure size
plt.figure(figsize=(20,10))

# Plot the decision tree
plot_tree(chosen_tree, 
          feature_names=features.columns, 
          class_names=True, 
          filled=True, 
          rounded=True,
          max_depth = 3)

plt.show()
