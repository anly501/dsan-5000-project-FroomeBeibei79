---
title: "Decision Tree"
format: html
---


# What is Decision Tree?

## Introduction

A decision tree is a flowchart-like structure used in decision making and data mining. It's a popular tool in machine learning, statistics, and information theory for modeling decisions and their possible consequences.


If you happened to watch Formula 1...

![Formula 1 meme](images/f1%20decision%20tree.jpeg)
[Source](https://www.reddit.com/r/formuladank/comments/12qhedw/made_this_for_my_biology_homework/)

In this plot, each question helps you make a decision, and you follow the path that matches the answers you get. It's like a flowchart of choices that leads you to a final decision. Decision trees work similarly in various fields, helping to make choices based on a series of yes-or-no questions and criteria.

![Decision Tree](images/decision%20tree.jpeg)
[Source](https://www.kaggle.com/code/ahmetburabua/decision-tree)


# Baseline Decision Tree Analysis

## Dataset and What I want to Achieve

In this analysis, we delve deeper into understanding the factors contributing to fatal crashes by examining specific conditions such as Lighting Condition, Weather Condition, Car Make, and Year. By leveraging the power of machine learning, particularly classification algorithms like Decision Trees and Random Forest, we aim to unearth patterns and interactions among these variables that significantly increase the risk of fatal outcomes in crashes.

The dataset, previously used for clustering, provides a rich source of information for this supervised learning task. Each record in the dataset represents a unique crash event, detailing various attributes like the make and model year of the car involved, the lighting condition at the time of the crash (e.g., daylight, dusk, or darkness), and the prevailing weather conditions (such as clear, rainy, or foggy). By analyzing these factors, we can build a predictive model that not only identifies high-risk scenarios but also helps in recommending preventive measures. For instance, if certain car models or specific weather conditions are found to be more frequently associated with fatal crashes, this information can be invaluable for road safety authorities and car manufacturers to improve safety standards and for drivers to exercise caution under identified high-risk conditions.

Furthermore, this approach also enables us to evaluate the relative importance of each factor in contributing to fatal crashes. For example, it may be revealed that certain lighting conditions, when combined with specific weather patterns, significantly heighten the risk of a fatal outcome, regardless of the car make or year. Such insights are crucial for targeted safety campaigns and policy-making, emphasizing conditions where drivers should be extra cautious. By dissecting the dataset in this manner, we not only aim to contribute to the broader field of traffic safety research but also provide actionable insights that could potentially save lives on the road.



## Baseline Model Result

The baseline model returned as follows:
Classification Report:

|precision |   recall  | f1-score | support |
|----------|-----------|----------|---------|
|  1       |0.91  |    0.98      |0.94      |8865
|  2       |0.08  |    0.02      |0.03       |739
|  3       |0.03  |    0.01      |0.02       |103
|  4       |0.00  |    0.00      |0.00        |26
|  5       |0.00  |    0.00      |0.00        |11
|  6       |0.00  |    0.00      |0.00         |1
|accuracy   |            |     |0.89      |9745
   macro avg|       0.17      |0.17      |0.16      |9745
weighted avg|       0.83      |0.89      |0.86      |9745

Confusion Matrix:
![cm](images/confusion%20matrix.jpg)




## Baseline Tree

![baseline](images/baseline%20tree.png)

## Classification Report Analysisï¼š

1. Class '1' (Most Likely Single Fatality)
    High precision (0.91) and recall (0.98), indicating good model performance for this class.
    The model correctly predicts most of the single fatality cases.
2. Class '2' and Beyond (Multiple Fatalities)
    Significantly lower precision and recall, particularly for classes 3 to 6.
    Indicates poor performance in predicting instances with more than one fatality.
3. Overall Accuracy
    The accuracy is 0.89, which seems high. However, this is largely due to the model's performance on the most common class (class 1).

## Confusion Matrix Analysis
1. Class '1'
    The model correctly predicted 8664 out of 8866 cases.
    Most errors involve class '1' being predicted when it's actually class '2'.
2. Classes '2' to '6'
    High misclassification rates, with the majority being incorrectly predicted as class '1'.
    Very few correct predictions for these classes.

The analysis shows that while the model performs well for the most common scenario, it struggles with less common, potentially more complex scenarios. Let's see if it would be better if we use random forest method.


# Random Forest Analysis

The random forests returned as follows:

Random Forest Classification Report:

|precision |   recall  | f1-score | support |
|----------|-----------|----------|---------|
|  1       |0.91  |    0.99      |0.95      |8865
|  2       |0.08  |    0.01      |0.02       |739
|  3       |0.08  |    0.01      |0.02       |103
|  4       |0.00  |    0.00      |0.00        |26
|  5       |0.00  |    0.00      |0.00        |11
|  6       |0.00  |    0.00      |0.00         |1
|accuracy   |            |     |0.90      |9745
   macro avg|       0.18      |0.17      |0.16      |9745
weighted avg|       0.83      |0.90      |0.86      |9745

![rf](images/random%20forest%20result.jpg)

The tree looks almost the same...
![rft](images/random%20forest%20tree.png)

## Classification Report Analysis Based on Random Forest

1. Class '1' (Most Likely Single Fatality):
    High precision (0.91) and recall (0.99), indicating the model is very effective in predicting this class.
2. Class '2' and Beyond (Multiple Fatalities):
    Low precision and recall for these classes, similar to the baseline decision tree.
    Indicates a struggle to correctly predict cases with multiple fatalities.
3. Overall Accuracy:
    The accuracy is 0.90, which is slightly better than the baseline decision tree. However, this is still largely due to the model's performance on the most common class (class 1).

## Confusion Matrix Analysis
1. Class '1':
    High number of correct predictions (8734 out of 8865).
    Relatively few misclassifications into other classes.
2. Classes '2' to '6':
    Majority are incorrectly predicted as class '1'.
    Very few correct predictions for these classes, indicating that the model struggles with minority classes.

## Feature Importances Analysis

1. MAKENAME and MOD_YEAR:
    These features have the highest importance scores (0.453 and 0.374 respectively), suggesting they are the most influential in the model's decisions.
2. Other Features:
    WEATHER, LGT_COND, DR_DRINK, and HIT_RUN have lower importance scores, indicating less influence on the model's predictions.


The Random Forest model provides a slight improvement over the baseline decision tree, especially in overall accuracy, but still faces challenges with the less common classes. Addressing the data imbalance and exploring alternative models or features could lead to further improvements.


# Hyper-Parameter Tunning

Both results are not very ideal, let's try to find the optimal parameters for this model.

For this model, we are using 10 iterrations and 3-fold cross-validation to find the optimal parameters.

The results are as follows:

Best Parameters: {'max_depth': 9, 'max_features': 4, 'min_samples_leaf': 8, 'min_samples_split': 6, 'n_estimators': 112}

# The Final Result

Using the optimal parameters, we run the model and the result is quite good on majority class compared to the previous one, but still, very poor on minority class.

Accuracy: 0.909697280656747

Classification Report:

|precision |   recall  | f1-score | support |
|----------|-----------|----------|---------|
|  1       |0.91  |    1.00      |0.95      |8865
|  2       |0.00  |    0.00      |0.00       |739
|  3       |0.00  |    0.00      |0.00      |103
|  4       |0.00  |    0.00      |0.00        |26
|  5       |0.00  |    0.00      |0.00        |11
|  6       |0.00  |    0.00      |0.00         |1
|accuracy   |            |     |0.91      |9745
   macro avg|       0.15      |0.17      |0.16      |9745
weighted avg|       0.83      |0.91      |0.87      |9745

![optimal parameter confusion matrix](images/best%20parameters%20cm.jpg)


## Optimized Tree
![optimized tree](images/optimized%20tree.png)
This plot is hard to interpret, and it's probably overfitting, so let's set the layer limit into 3 to make it easy to interpret.

![4 layer](images/4%20layer.png)


## Conclusion

The model performs very well in predicting the majority class (labeled as "1"), with a precision of 0.91 and a recall of 1.00. This is indicated by the 8865 correct predictions in the confusion matrix for this class.

The model fails to correctly predict any instances of the minority classes (labeled as "2", "3", "4", "5", and "6"). This could be due to imbalance datasets as fatal numbers of 1 is the most common and anything bigger than 1 is rarely seen in accidents. The model also tends to be overwhelmingly favor the majority class.

This decision tree can only provide us with limited information, clearly it's not enough to deliver solid conclusion. However, this is a good start as the last tree already have some pretty good result and it also make sense in real life.







